[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "欢迎来到 RSSPtho，这里有\n\n读书笔记\n工作经验\n流程搭建\nshiny软件\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "一名拥有2年工作经验的生物信息工程师。我对R语言充满热情，致力于使用R语言解决生命科学中的数据。\n\n\n\n中科新生命 - 生物信息工程师 (2024.02 - 至今)\n上海云序生物科技有限公司 - 生物信息工程师 (2023.07 - 2024.01)\n\n\n\n硕士 - 生物信息学\n\n中科院分子细胞科学卓越创新中心 & 中国科学院大学 (2020 - 2023)\n\n本科 - 生物科学\n\n山东大学 (2014 - 2020)\n\n\n\n\n\n\nR\nnextflow\npython\ndocker\ngit"
  },
  {
    "objectID": "about.html#工作经历",
    "href": "about.html#工作经历",
    "title": "About me",
    "section": "",
    "text": "中科新生命 - 生物信息工程师 (2024.02 - 至今)\n上海云序生物科技有限公司 - 生物信息工程师 (2023.07 - 2024.01)"
  },
  {
    "objectID": "about.html#教育经历",
    "href": "about.html#教育经历",
    "title": "About me",
    "section": "",
    "text": "硕士 - 生物信息学\n\n中科院分子细胞科学卓越创新中心 & 中国科学院大学 (2020 - 2023)\n\n本科 - 生物科学\n\n山东大学 (2014 - 2020)"
  },
  {
    "objectID": "about.html#技能",
    "href": "about.html#技能",
    "title": "About me",
    "section": "",
    "text": "R\nnextflow\npython\ndocker\ngit"
  },
  {
    "objectID": "Books/ggplot2/index.html",
    "href": "Books/ggplot2/index.html",
    "title": "index",
    "section": "",
    "text": "test\n\n\n\n Back to top",
    "crumbs": [
      "index"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html",
    "title": "5 Spending our Data",
    "section": "",
    "text": "创建一个有用的模型有几个步骤，包括参数估计、模型选择与调优以及性能评估。在一个新项目开始时，通常会有一个初始的有限数据池可用于所有这些任务，我们可以将其视为可用的数据预算。应该如何将数据应用于不同的步骤或任务呢？数据分配的理念是建模时首先要考虑的重要问题，尤其是在涉及经验验证时。当数据被重复用于多项任务，而非从有限的数据预算中谨慎“使用”时，某些风险会增加，例如加剧偏见的风险或因方法误差产生的复合效应的风险。\n当有大量数据可用时，一个明智的策略是为不同的任务分配特定的数据子集，而不是将尽可能多（甚至全部）的数据仅分配给模型参数估计。例如，一种可能的策略（当数据和预测变量都很充足时）是，在考虑参数估计之前，先使用特定的数据子集来确定哪些预测变量是有信息价值的。如果可用的初始数据量不是很大，那么在数据的“使用”或分配方式和时间上就会存在一些重叠，因此，一套可靠的数据使用方法就显得很重要。\n本章将演示为不同目的对我们初始的样本池进行拆分（即制定数据预算）的基本方法。",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html#common-methods-for-splitting-data",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html#common-methods-for-splitting-data",
    "title": "5 Spending our Data",
    "section": "Common Methods for Splitting Data",
    "text": "Common Methods for Splitting Data\n经验模型验证的主要方法是将现有的数据池分成两个不同的集合，即训练集和测试集。训练集通常占数据的大部分，用于开发和优化模型；这些数据是模型构建的试验场，在这里可以拟合不同的模型、研究特征工程策略等等。作为建模从业者，我们在建模过程的绝大部分时间里，都以训练集为基础来开发模型。测试集会被保留起来，直到选出一两个最有可能成功的模型方法之后，就会被用作最终的评判标准，来确定模型的有效性；至关重要的是，测试集只能查看一次；否则，它就会变成建模过程的一部分。我们应该如何进行这种数据拆分呢？答案取决于具体情境。\n假设我们将80%的数据分配给训练集，剩下的20%用于测试。最常用的方法是使用简单随机抽样。rsample包提供了用于进行此类数据拆分的工具；initial_split()函数就是为此目的而创建的。它将数据框以及要放入训练集的比例作为参数。我们对Ames数据集进行拆分：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\n\n# Set the random number stream using `set.seed()` so that the results can be\n# reproduced later.\nset.seed(501)\n\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\n# Save the split information for an 80/20 split of the data\names_split &lt;- initial_split(ames, prop = 0.80)\names_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;2344/586/2930&gt;\n\n打印的信息表明了训练集中的数据量（n=2,344）、测试集中的数据量（n=586）以及原始样本池的大小（n=2,930）。对象ames_split 是一个rsplit对象，仅包含分区信息；要获取生成的数据集，我们还需应用两个函数：training()和testing()。生成的结果是数据框，具有与原始数据相同的列，但每个集合只有相应的行。\n\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\ndim(ames_train)\n#&gt; [1] 2344   74\n\n简单随机抽样在很多情况下是适用的，但也存在例外。在分类问题中存在显著的类别不平衡时，即一个类别的出现频率远低于另一个类别，使用简单随机抽样可能会随意地将这些不常见的样本不成比例地分配到训练集或测试集中。为避免这种情况，可以采用分层抽样。训练集/测试集的划分在每个类别内部单独进行，然后将这些子样本组合成整体的训练集和测试集。对于回归问题，可以将结果数据人为地分箱为四分位数，然后分别进行四次分层抽样。这是一种有效的方法，能使训练集和测试集中结果的分布保持相似。Ames房产数据的销售价格结果分布如 Figure 1 所示。\n\nsale_dens &lt;-\n  density(ames$Sale_Price, n = 2^10) %&gt;%\n  tidy()\nquartiles &lt;- quantile(ames$Sale_Price, probs = c(1:3) / 4)\nquartiles &lt;- tibble(prob = (1:3 / 4), value = unname(quartiles))\nquartiles$y &lt;- approx(sale_dens$x, sale_dens$y, xout = quartiles$value)$y\n\nquart_plot &lt;-\n  ggplot(ames, aes(x = Sale_Price)) +\n  geom_line(stat = \"density\") +\n  geom_segment(\n    data = quartiles,\n    aes(x = value, xend = value, y = 0, yend = y),\n    lty = 2\n  ) +\n  labs(x = \"Sale Price (log-10 USD)\", y = NULL)\nquart_plot\n\n\n\n\n\n\nFigure 1: The distribution of the sale price (in log units) for the Ames housing data. The vertical lines indicate the quartiles of the data\n\n\n\n\n如第4章所讨论的，销售价格分布呈右偏态（尾巴在右侧），在分布中心的两侧，低价房屋的数量比例多于高价房屋。简单分割在这里存在一个问题，即高价房屋在训练集中可能无法得到充分体现；这会增加我们的模型在预测此类房产价格时效果不佳的风险。 Figure 1 中的虚线垂直线标示了这些数据的四个四分位数。分层随机抽样会在每个数据子集中进行80/20的分割，然后将结果合并。在rsample中，这可以通过strata参数（只能使用单个列）来实现：\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\ndim(ames_train)\n#&gt; [1] 2342   74\n\n使用分层抽样的弊端很少，是否存在随机抽样并非最佳选择的情况呢？一种情况是当数据具有显著的时间成分时，例如时间序列数据。在这种情况下，更常见的做法是使用最新的数据作为测试集。rsample包包含一个名为initial_time_split()的函数，它与initial_split()非常相似。不同于随机抽样，prop参数表示数据的前一部分中应有多大比例用作训练集；该函数假设数据已经按适当的顺序预先排序好了。\n应该分配用于拆分的数据比例在很大程度上取决于当前问题的背景。训练集中的数据过少会妨碍模型找到合适的参数估计值。相反，测试集中的数据过少会降低性能估计的质量。统计学界的部分人士普遍不赞成使用测试集，因为他们认为所有数据都应用于参数估计。尽管这个论点有其可取之处，但拥有一组无偏的观测数据作为模型质量的最终评判标准是一种良好的建模实践。只有当数据少到离谱时，才应避免使用测试集。",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html#what-about-a-validation-set",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html#what-about-a-validation-set",
    "title": "5 Spending our Data",
    "section": "What About a Validation Set?",
    "text": "What About a Validation Set?\n在描述数据拆分的目标时，我们特别指出测试集是用来正确评估最终模型性能的数据。这就引出了一个问题：“如果我们直到测试集才衡量性能，那我们怎么知道什么是最好的呢？”经常会听到有人用验证集来回答这个问题，尤其是在神经网络和深度学习的文献中。在神经网络发展的早期，研究人员意识到，通过重新预测训练集样本來衡量性能，会得到过于乐观的结果（明显不切实际）——模型过拟合，也就是说，模型在训练集上表现很好，但在测试集上表现很差。为了解决这个问题，人们会留出一小部分验证集数据，在网络训练过程中用它来衡量性能。一旦验证集的错误率开始上升，训练就会停止。换句话说，验证集是在使用测试集之前，大致了解模型表现的一种手段。\n验证集是训练集的一个子集，还是数据初始划分中的第三部分，这在很大程度上只是语义上的问题。\n验证集将在第10.2.2节中作为训练集上使用的重采样方法的一种特殊情况进行更详细的讨论。如果您打算使用验证集，可以从一个不同的拆分函数开始：\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;1758/586/586/2930&gt;\n\n现在打印分割结果会显示训练集（1,758）、验证集（586）和测试集（586）的大小。要获取训练、验证和测试数据，使用的语法相同：\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)\n\n第10.2.2节将演示如何使用ames_val_split对象进行重采样和模型优化。",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html#multilevel-data",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html#multilevel-data",
    "title": "5 Spending our Data",
    "section": "Multilevel Data",
    "text": "Multilevel Data\n在Ames房产数据中，一处房产被视为独立实验单元。从统计学角度来讲，可以合理假设一处房产的数据与其他房产的数据是相互独立的。但在其他应用场景中，情况并非总是如此：\n\n对于纵向数据，例如，同一个独立实验单位可以在多个时间点被测量。一个例子是医学试验中的人类受试者。\n一批制成品也可能被视为独立的实验单元。在重复测量设计中，会在多个时间点收集来自同一批次的重复数据点。\nJohnson et al.（2018）报告了一项实验，在该实验中，研究人员对树干顶部和底部的不同树木进行了采样。在此，树木是实验单位，数据层次结构为树木内的树干位置内的样本。\n\n第9章包含了其他例子。\n在这些情况下，每个实验单元在数据集中会有多行数据。简单地对行进行重采样会导致一个实验单元内的部分数据进入训练集，而其他数据进入测试集。数据拆分应在数据的独立实验单元层面进行。例如，要对Ames房产数据集进行80/20的拆分，应将80%的房产分配到训练集。",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html#other-considerations-for-a-data-budget",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html#other-considerations-for-a-data-budget",
    "title": "5 Spending our Data",
    "section": "Other Considerations for a Data Budget",
    "text": "Other Considerations for a Data Budget\n在决定如何使用你手头的数据时，还要记住几件事：\n\n首先，将测试集与任何模型构建活动隔离开来至关重要。在阅读本书时，要留意在特定时间哪些数据会暴露给模型。当训练集之外的数据被用于建模过程时，就会出现信息泄露问题。例如，在机器学习竞赛中，测试集数据可能会在不提供真实结果值的情况下给出，这样就能对模型进行评分和排名。一种可能提高分数的方法是，使用训练集中与测试集值最相似的数据点来拟合模型。虽然测试集并未直接用于拟合模型，但它仍然有着很大的影响。一般来说，这种技术存在很大问题，因为它会降低模型的泛化误差，以优化在特定数据集上的性能。在训练过程中，还有更隐蔽的方式会用到测试集数据。将训练数据和测试集放在不同的数据框中，这是一个小小的检查措施，可确保不会意外发生信息泄露。\n其次，对训练集进行子采样的技术可以缓解特定问题（例如类别不平衡）。这是一种有效且常见的技术，它会有意使训练集数据偏离其抽取自的总体。至关重要的是，测试集要继续反映模型在实际应用中会遇到的情况。换句话说，测试集应该始终与将要输入给模型的新数据相似。\n接下来，在本章开头，我们就警告过不要将相同的数据用于不同的任务。第10章将讨论可靠的、数据驱动的数据使用方法，这些方法将降低与偏差、过拟合和其他问题相关的风险。其中许多方法都应用了本章介绍的数据拆分工具。\n最后，本章中的考量适用于开发和选择可靠的模型，这也是本书的主要主题。在为生产环境训练最终选定的模型时，在确定其在新数据上的预期性能后，从业者通常会使用所有可用数据来获得更优的参数估计。",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/5 Spending our Data.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/5 Spending our Data.html#chapter-summary",
    "title": "5 Spending our Data",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n数据拆分是模型实证验证的基本策略。即便在数据收集不受限制的时代，一个典型的建模项目所拥有的合适数据量也是有限的，因此有必要明智地使用项目数据。在本章中，我们讨论了将数据划分为不同组以进行建模和评估的几种策略。\n在这一节点，用于数据准备和拆分的重要代码片段如下：\nlibrary(tidymodels)\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)",
    "crumbs": [
      "5 Spending our Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "",
    "text": "在介绍如何使用tidymodels将整洁数据原则应用于用R构建模型之前，让我们先回顾一下在核心R语言（通常称为“base R”）中模型是如何创建、训练和使用的。本章简要介绍了base R中的一些约定，即使你根本不会使用base R来构建模型，了解这些约定也很重要。本章内容并非详尽无遗，但它为读者（尤其是那些刚接触R的读者）提供了最基本、最常用的模式。\nR所基于的S语言，自Chambers和Hastie的著作（通常被称为《白皮书》）出版以来，就拥有了丰富的数据分析环境。这个版本的S语言引入了如今R用户所熟悉的标准基础设施组件，例如符号模型公式、模型矩阵和数据框，以及用于数据分析的标准面向对象编程方法。从那以后，这些用户界面并未发生实质性的变化。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#an-example",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#an-example",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "An Example",
    "text": "An Example\n为了演示在base R中建模的一些基本原理，让我们使用McDonald的实验数据，该数据由Mangiafico提供，研究的是环境温度与蟋蟀每分钟鸣叫次数之间的关系。收集的数据涉及两个物种：O.exclamationis和O.niveus。这些数据包含在一个名为crickets的数据框中，共有31个数据点。图3.1展示了这些数据，使用的是以下ggplot2代码：\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.1.0     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(crickets, package = \"modeldata\")\nnames(crickets)\n#&gt; [1] \"species\" \"temp\"    \"rate\"\n\n# Plot the temperature on the x-axis, the chirp rate on the y-axis. The plot\n# elements will be colored differently for each species:\nggplot(\n  crickets,\n  aes(x = temp, y = rate, color = species, pch = species, lty = species)\n) +\n  # Plot points for each data point and color by species\n  geom_point(size = 2) +\n  # Show a simple linear model fit created separately for each species:\n  geom_smooth(method = lm, se = FALSE, alpha = 0.5) +\n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"Temperature (C)\", y = \"Chirp Rate (per minute)\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\nRelationship between chirp rate and temperature for two different species of crickets\n\n\n\n这些数据对每个物种都呈现出相当线性的趋势。在给定温度下，O.exclamationis每分钟的鸣叫次数似乎比O.niveus更多。对于推断模型，研究人员在查看数据之前可能已经指定了以下零假设：\n\n温度对鸣叫率没有影响。\n不同物种的鸣叫率之间没有差异。\n\n预测鸣叫率可能具有一定的科学或实际价值，但在本示例中，我们将重点关注推断。\nLiner Regression\n要在R中拟合普通线性模型，通常会使用lm()函数。该函数的重要参数包括模型公式和包含数据的数据框。这个公式是符号化的。例如，简单的公式：rate ~ temp。指定鸣叫率是结果（因为它位于波浪号~的左侧），而温度值是预测因子。假设数据中包含一个名为time的列，其中记录了获取测量值的一天中的时间，则公式可以是：rate ~ temp + time。这个公式象征性地表示，温度和时间应作为单独的主效应（主效应是包含单个预测变量的模型项）添加到模型中，而不是将时间和温度值相加。这些数据中没有时间测量值，但可以用同样的方式将物种添加到模型中：rate ~ temp + species。\n物种不是一个数值型变量；在数据框中，它被表示为一个因子列，其水平为”O.exclamationis”和”O.niveus”。绝大多数模型函数无法处理非数值数据。对于物种，模型需要将物种数据编码为数值格式。最常见的方法是使用指示变量（也称为虚拟变量）来替代原始的定性值。在这种情况下，由于物种有两个可能的值，模型公式会通过添加一个新列将该列自动编码为数值，当物种是”O.exclamationis”时，该新列的值为0；当数据对应于”O.niveus”时，该新列的值为1。底层的公式机制会自动转换用于创建模型的数据集以及任何新数据点（例如，当模型用于预测时）的这些值。\n假设存在五个物种而非两个。在这种情况下，模型公式会创建四个二进制列，作为其中四个物种的二进制指示器。该因子的参考水平（即第一个水平）总是会被排除在预测变量集之外。其原理是，如果你知道这四个指示变量的值，就能确定物种的取值。我们将在8.4.1节中更详细地讨论二进制指示变量。\n模型公式rate ~ temp + species构建了一个针对每个物种具有相同斜率但不同y轴截距的模型。考虑到回归线的斜率也可能因物种而异，可以在模型中添加交互项。所谓交互项，是指某个变量对响应变量的影响，依赖于另一个或多个变量的取值（在图上直观地显示为斜率不同）。交互项可以通过几种不同的方式来指定（下面三种都是等价的），其中最基本的方式是使用冒号：\nrate ~ temp + species + temp:species\n\n# A shortcut can be used to expand all interactions containing\n# interactions with two variables:\nrate ~ (temp + species)^2\n\n# Another shortcut to expand factors to include all possible\n# interactions (equivalent for this example):\nrate ~ temp * species\n除了能便捷地自动创建指示变量外，该公式还具备其他一些优点：\n\n内嵌函数可用于公式中。例如，要使用温度的自然对数，我们可以创建公式rate ~ log(temp)。由于公式默认是符号性(symbol)的，也可以使用恒等函数I()对预测变量应用实际数学运算。要使用华氏度单位，可通过公式rate ~ I( (temp * 9/5) + 32 )从摄氏度进行转换。\nR有许多在公式中很有用的函数。例如，poly(x, 3)会为模型添加x的线性项、二次项和三次项作为主效应。splines包也有几个函数可用于在公式中创建非线性样条项。\n对于存在许多预测变量的数据集，可以使用句点快捷方式。句点代表了所有不在波浪号左侧的列的主效应。使用~ (.)^3会将主效应以及所有双变量和三变量交互项添加到模型中。\n\n回到我们的鸣虫（蟋蟀）问题，让我们使用双向交互模型。在本书中，我们对经过拟合的R模型对象使用后缀_fit。\n\ninteraction_fit &lt;- lm(rate ~ (temp + species)^2, data = crickets)\n\n# To print a short summary of the model:\ninteraction_fit\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = rate ~ (temp + species)^2, data = crickets)\n#&gt; \n#&gt; Coefficients:\n#&gt;           (Intercept)                   temp       speciesO. niveus  \n#&gt;               -11.041                  3.751                 -4.348  \n#&gt; temp:speciesO. niveus  \n#&gt;                -0.234\n\n这个输出有点难以阅读。对于物种指示变量，R将变量名（species）和因子水平（O.niveus）合并在一起，没有分隔符。\n在深入探讨该模型的任何推断结果之前，应使用诊断图来评估拟合情况。我们可以对lm对象使用plot()方法。该方法会为对象生成一组四个图，每个图都展示了拟合的不同方面，如图3.2所示：\nplot(interaction_fit)\n\n\n\n\n\nResiduals-Fitted\n\n\n\n\n\nQ-Q Residuals\n\n\n\n\n\n\n\nScale-Location\n\n\n\n\n\nResiduals-Leverage\n\n\n\n\n\nResidual diagnostic plots for the linear model with interactions, which appear reasonable enough to conduct inferential analysis\n\n\n\n\nResiduals-Fitted：描述残差与拟合值的关系，理想情况下，残差应该随机分布，没有明显的模式或趋势。如果出现明显的曲线或其他模式，可能表明模型未能捕捉到数据中的某些结构，或者存在非线性关系。图中还显示了几个标记点，如“21”、“13”等，这些可能是异常值或具有高影响力的观测点。\nQ-Q Residuals：描述标准化残差值与正态分布理论值的关系，检查残差的分布是否接近正态分布。如果残差是正态分布的，那么它们应该大致沿着图中的直线排列。图中这种情况意味着残差的分布有轻微的偏斜或重尾。\nScale-Location：描述标准化残差的绝对值开方法后与拟合值的关系，检查残差的方差是否随拟合值的变化而变化，即方差齐性。如果存在异方差性，图中可能会显示出某种趋势或模式。\nResiduals-Leverage：描述标准残差值与观测点leverage（或权重）之间的关系，同时添加Cook距离线，检查某个观测点是否对模型影响很大。如果某个观测点位于Cook距离线附近，则这些点对模型影响较大，例如“13”。\n\n在评估表达式的技术细节方面，R语言是惰性的（而非急切的）。这意味着模型拟合函数通常会在最后一刻才计算尽可能少的量。例如，如果你想获取每个模型项的系数表，它不会随模型自动计算，而是通过summary()方法来计算。\nAnova\n我们接下来关于蟋蟀的工作是评估是否有必要纳入交互项。对于这个模型，最合适的方法是重新计算不含交互项的模型，并使用anova()方法。\n\n# Fit a reduced model:\nmain_effect_fit &lt;- lm(rate ~ temp + species, data = crickets)\n\n# Compare the two:\nanova(main_effect_fit, interaction_fit)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: rate ~ temp + species\n#&gt; Model 2: rate ~ (temp + species)^2\n#&gt;   Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n#&gt; 1     28 89.350                          \n#&gt; 2     27 85.074  1    4.2758 1.357 0.2542\n\n该统计检验生成的p值为0.25。这意味着缺乏证据反驳模型不需要交互项这一零假设。因此，我们将对不含交互项的模型进行进一步分析。\n应该重新评估残差图，以确保我们的理论假设足够有效，从而可以信任模型产生的p值（此处未展示这些图，但先透露一下：这些假设是有效的）。\n我们可以使用summary()方法来查看每个模型项的系数、标准误差和p值：\n\nsummary(main_effect_fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = rate ~ temp + species, data = crickets)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.0128 -1.1296 -0.3912  0.9650  3.7800 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -7.21091    2.55094  -2.827  0.00858 ** \n#&gt; temp               3.60275    0.09729  37.032  &lt; 2e-16 ***\n#&gt; speciesO. niveus -10.06529    0.73526 -13.689 6.27e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.786 on 28 degrees of freedom\n#&gt; Multiple R-squared:  0.9896, Adjusted R-squared:  0.9888 \n#&gt; F-statistic:  1331 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n每个物种的鸣叫率会随着温度每升高1度而增加3.6次鸣叫。p值表明，这个项具有很强的统计显著性。物种项的值为-10.07。这表明，在所有温度值下，O.niveus的鸣叫率大约比O.exclamationis每分钟少10次。与温度项类似，物种效应也与一个非常小的p值相关。\n本分析中唯一的问题是截距值。它表明在0°C时，两个物种的每分钟鸣叫次数都是负数。但数据中温度最低仅为17.2°C，在0°C时对模型进行解释属于外推法，这不是个好主意，结论应仅限于观测到的温度范围。\n如果我们需要在实验中未观测到的温度下估计鸣叫率，我们可以使用predict()方法。该方法需要模型对象和一个包含用于预测的新值的数据框。例如，模型对15°C至20°C之间温度下的O. exclamationis的鸣叫率估计可以通过以下方式计算：\n\nnew_values &lt;- data.frame(species = \"O. exclamationis\", temp = 15:20)\npredict(main_effect_fit, new_values)\n#&gt;        1        2        3        4        5        6 \n#&gt; 46.83039 50.43314 54.03589 57.63865 61.24140 64.84415\n\n请注意，传递给predict()方法的是species的非数值型值，而非数值型的二元指示变量。\nSummary\n虽然本分析显然并未全面展示R的建模能力，但它确实强调了对本书其余部分而言很重要的一些主要功能：\n\n这种语言具有富有表现力的语法，可用于为简单模型和相当复杂的模型指定模型项。\nR语言的公式方法在建模方面有许多便利之处，这些便利之处在生成预测时也会应用于新数据。\n有许多辅助函数（例如anova()、summary()和predict()），你可以在创建拟合模型后使用它们来进行特定的计算。\n\n最后，如前所述，该框架于1992年首次发表。其中的大部分理念和方法都是在那个时期发展出来的，但直到今天仍然具有显著的相似性。这凸显出S语言以及由此衍生的R语言从一开始就专为数据分析而设计。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#what-does-the-r-formula-do",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#what-does-the-r-formula-do",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "What Does the R Formula Do?",
    "text": "What Does the R Formula Do?\nR模型公式被许多建模包所使用。它通常有多种用途：\n\n公式定义了模型所使用的数据框中的列。\n标准的R机制利用该公式将列编码为适当的格式。\n数据框中列的角色由该公式定义。\n\n在很大程度上，从业者对公式功能的理解主要受最后一个用途的影响。我们在输入公式时，注意力往往集中在声明应如何使用这些列上。例如，我们之前讨论的规范就以特定方式设置了要使用的预测变量：(temp + species)^2。看到这个公式时，我们的关注点在于，存在两个预测变量，模型应当包含它们的主效应和双向交互项。然而，这个公式还意味着，由于species是一个因子，它还应该为这个预测变量创建指示变量列（参见第8.4.1节），并将这些列与temp列相乘来创建交互项。这种转换体现了我们关于编码的第二点；公式还定义了每一列的编码方式，并且可以创建原始数据中不存在的额外列。\n这一点很重要，在本书中会多次出现，尤其是当我们在第8章及以后章节讨论更复杂的特征工程时。R语言中的公式存在一些局限性，而我们克服这些局限性的方法涉及到所有三个方面。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#why-tidiness-is-important-for-modeling",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#why-tidiness-is-important-for-modeling",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "Why Tidiness Is Important for Modeling",
    "text": "Why Tidiness Is Important for Modeling\nR’s diversity\nR的优势之一在于，它鼓励开发者创建符合自身需求的用户界面。举个例子，以下是在名为plot_data的数据框中创建两个数值变量的散点图的三种常见方法：\nplot(plot_data$x, plot_data$y)\n\nlibrary(lattice)\nxyplot(y ~ x, data = plot_data)\n\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = y)) + geom_point()\n在这三种情况下，不同的开发人员团队为同一任务设计了三种截然不同的界面。每种界面都各有优缺点。相比之下，《Python开发者指南》在解决问题时信奉这样一种理念：“There should be one – and preferably only one – obvious way to do it.” 在这方面，R与Python有很大不同。R的接口多样性的一个优势在于，它可以随着时间的推移不断发展，满足不同用户的不同需求。不幸的是，部分语法多样性源于对开发代码者需求的关注，而非对使用代码者需求的关注。各软件包之间的不一致性可能会成为R用户的绊脚石。\n假设你的建模项目有一个包含两个类别的结果。你可以从多种统计模型和机器学习模型中进行选择。为了为每个样本生成类别概率估计，模型函数通常会有一个对应的predict()方法。然而，这些方法用于生成类别概率预测的参数值存在显著差异，这种差异即使对于有经验的用户来说也可能难以应对。不同模型的这些参数值示例如表3.1所示。\n\nTable 3.1: Heterogeneous argument names for different modeling functions.\n\n\n\n\n\n\nFunction\nPackage\nCode\n\n\n\nlda()\nMASS\npredict(object)\n\n\nglm()\nstats\npredict(object, type = “response”)\n\n\ngbm()\ngbm\npredict(object, type = “response”, n.trees)\n\n\nmda()\nmda\npredict(object, type = “posterior”)\n\n\nrpart()\nrpart\npredict(object, type = “prob”)\n\n\nvarious\nRWeka\npredict(object, type = “probability”)\n\n\nlogitboost()\nLogitBoost\npredict(object, type = “raw”, nIter)\n\n\npamr.train()\npamr\npamr.predict(object, type = “posterior”)\n\n\n\n请注意，最后一个示例使用了一个自定义函数来进行预测，而不是使用更常见的predict()接口（即predict()的泛函方法）。这种不一致性是日常使用R语言进行建模的一个障碍。\nMissing value\n作为不可预测性的另一个例子，R语言中有关缺失数据的约定处理方式并不一致。一般规则是缺失数据会产生更多缺失数据；例如，一组包含缺失数据点的值的平均值本身也是缺失的，依此类推。当模型进行预测时，绝大多数模型都要求所有预测变量具有完整的值。在这一点上，R语言内置了几种通过通用函数na.action()实现的选项。这一函数设定了函数在遇到缺失值时的处理策略。两种最常见的策略是na.fail()和na.omit()。前者在存在缺失数据时会产生错误，而后者在计算前通过个案删除的方式移除缺失数据。从我们之前的例子来看：\n\n# Add a missing value to the prediction set\nnew_values$temp[1] &lt;- NA\n\n# The predict method for `lm` defaults to `na.pass`:\npredict(main_effect_fit, new_values)\n#&gt;        1        2        3        4        5        6 \n#&gt;       NA 50.43314 54.03589 57.63865 61.24140 64.84415\n\n# Alternatively\npredict(main_effect_fit, new_values, na.action = na.fail)\n#&gt; Error in na.fail.default(structure(list(temp = c(NA, 16L, 17L, 18L, 19L, : missing values in object\n\npredict(main_effect_fit, new_values, na.action = na.omit)\n#&gt;        2        3        4        5        6 \n#&gt; 50.43314 54.03589 57.63865 61.24140 64.84415\n\n从用户的角度来看，na.omit()可能会带来问题。在我们的示例中，new_values有6个，但使用na.omit()只会返回5个。为了对此进行调整，如果要将预测结果合并到new_values中，用户就必须确定哪一行存在缺失值，并在适当的位置插入一个缺失值。虽然预测函数将na.omit()用作其缺失数据处理策略的情况很少见，但这种情况确实会发生。那些确定这是其代码中错误原因的用户会对此印象非常深刻。\n为了解决此处描述的使用问题，tidymodels包设定了一系列设计目标。tidymodels的大多数设计目标都属于tidyverse中已有的“为人类而设计”这一范畴（Wickham et al. 2019），但这些目标在建模代码方面有特定的应用。此外，tidymodels还有一些额外的设计目标，作为对tidyverse设计目标的补充。以下是一些例子：\n\nR具有出色的面向对象编程能力，我们利用这一点来替代创建新的函数名称（例如一个假设的新predict_samples()函数）。\n合理的默认值非常重要。此外，当更适合迫使用户做出选择时，函数的参数不应有默认值（例如，read_csv()的文件名参数）。\n同样，那些默认值可以从数据中推导出来的参数值也应该如此。例如，对于glm()函数，family参数可以检查结果中数据的类型，并且如果没有提供family参数，就可以在内部确定一个默认值。\n函数应该接受用户已有的数据结构，而非开发者想要的数据结构。例如，模型函数的唯一接口不应局限于矩阵。用户往往会有非数值型的预测变量，比如因子。\n\n这些理念中有许多在tidymodels的模型实现指南中都有阐述。在后续章节中，我们将举例说明一些现存的问题及其解决方案。\nbroom::tidy()\n一些现有的R包提供了统一的接口来协调这些异构的建模API，例如caret和mlr。tidymodels框架与它们类似，采用了统一的函数接口，并确保函数名称和返回值的一致性。但它在其固执己见的设计目标和建模实现方面有所不同，本书将对此进行详细讨论。\n我们在本书中一直使用的broom::tidy()函数是另一个用于标准化R对象结构的工具。它能以更易用的格式返回多种类型的R对象。例如，假设正根据预测变量与结果列的相关性对其进行筛选。使用purrr::map()函数，cor.test()函数的结果可以以列表形式为每个预测变量返回：\n\ncorr_res &lt;- map(mtcars %&gt;% select(-mpg), cor.test, y = mtcars$mpg)\n\n# The first of ten results in the vector:\ncorr_res[[1]]\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  .x[[i]] and mtcars$mpg\n#&gt; t = -8.9197, df = 30, p-value = 6.113e-10\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.9257694 -0.7163171\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; -0.852162\n\n如果我们想在图表中使用这些结果，假设检验结果的标准格式并不是很有用。tidy()方法可以将其以tibble的形式返回，并带有标准化的名称：\n\nlibrary(broom)\n\ntidy(corr_res[[1]])\n#&gt; # A tibble: 1 × 8\n#&gt;   estimate statistic  p.value parameter conf.low conf.high method            \n#&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             \n#&gt; 1   -0.852     -8.92 6.11e-10        30   -0.926    -0.716 Pearson's product…\n#&gt; # ℹ 1 more variable: alternative &lt;chr&gt;\n\n这些结果可以“堆叠”起来并添加到ggplot()中，如图3.3所示。\n\ncorr_res %&gt;%\n  # Convert each to a tidy format; `map_dfr()` stacks the data frames\n  map_dfr(tidy, .id = \"predictor\") %&gt;%\n  ggplot(aes(x = fct_reorder(predictor, estimate))) +\n  geom_point(aes(y = estimate)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +\n  labs(x = NULL, y = \"Correlation with mpg\")\n\n\n\nCorrelations (and 95% confidence intervals) between predictors and the outcome in the mtcars data set\n\n\n\n使用base R中的函数也可以创建这样的图表，但自动重新格式化结果能使代码更简洁，且出错的可能性更小。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#combining-base-r-models-and-the-tidyverse",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#combining-base-r-models-and-the-tidyverse",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "Combining Base R Models and the Tidyverse",
    "text": "Combining Base R Models and the Tidyverse\nbase R或其他R包中的建模函数可以与tidyverse结合使用，尤其是与dplyr、purrr和tidyr包。例如，如果我们想为每个蟋蟀物种拟合单独的模型，我们可以首先使用dplyr::group_nest()按该列拆分蟋蟀数据：\n\nsplit_by_species &lt;-\n  crickets %&gt;%\n  group_nest(species)\nsplit_by_species\n#&gt; # A tibble: 2 × 2\n#&gt;   species                        data\n#&gt;   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt;\n#&gt; 1 O. exclamationis           [14 × 2]\n#&gt; 2 O. niveus                  [17 × 2]\n\ndata列包含来自crickets的rate列和temp列，并以列表列的形式呈现。基于此，purrr::map()函数可以为每个物种创建单独的模型：\n\nmodel_by_species &lt;-\n  split_by_species %&gt;%\n  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))\nmodel_by_species\n#&gt; # A tibble: 2 × 3\n#&gt;   species                        data model \n#&gt;   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt; &lt;list&gt;\n#&gt; 1 O. exclamationis           [14 × 2] &lt;lm&gt;  \n#&gt; 2 O. niveus                  [17 × 2] &lt;lm&gt;\n\n要收集这些模型中每个模型的系数，请使用broom::tidy()将它们转换为一致的数据框格式，以便可以对其进行拆嵌：\n\nmodel_by_species %&gt;%\n  mutate(coef = map(model, tidy)) %&gt;%\n  select(species, coef) %&gt;%\n  unnest(cols = c(coef))\n#&gt; # A tibble: 4 × 6\n#&gt;   species          term        estimate std.error statistic  p.value\n#&gt;   &lt;fct&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 O. exclamationis (Intercept)   -11.0      4.77      -2.32 3.90e- 2\n#&gt; 2 O. exclamationis temp            3.75     0.184     20.4  1.10e-10\n#&gt; 3 O. niveus        (Intercept)   -15.4      2.35      -6.56 9.07e- 6\n#&gt; 4 O. niveus        temp            3.52     0.105     33.6  1.57e-15\n\n列表列(list-column)在建模项目中可能非常强大。列表列可为任何类型的R对象提供容器，从拟合模型本身到重要的数据框结构均可容纳。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#the-tidymodels-metapackage",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#the-tidymodels-metapackage",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "The tidymodels Metapackage",
    "text": "The tidymodels Metapackage\ntidyverse（第2章）是作为一组模块化的R包设计的，每个包的范围都相当狭窄。tidymodels框架采用了类似的设计。例如，rsample包专注于数据拆分和重采样。尽管重采样方法对建模的其他活动（如衡量性能）至关重要，但它们存在于单个包中，而性能指标则包含在另一个独立的包yardstick中。采用这种模块化包的理念有很多好处，从更简洁的模型部署到更顺畅的包维护。\n这种理念的缺点是tidymodels框架中有很多包。为了弥补这一点，tidymodels包（你可以把它看作是一个像tidyverse包一样的元包）会加载一组核心的tidymodels和tidyverse包。加载该包会显示哪些包被附加了：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ dials        1.4.2     ✔ tailor       0.1.0\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ recipes      1.3.1     ✔ yardstick    1.3.2\n#&gt; ✔ rsample      1.3.1\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ scales::discard() masks purrr::discard()\n#&gt; ✖ dplyr::filter()   masks stats::filter()\n#&gt; ✖ recipes::fixed()  masks stringr::fixed()\n#&gt; ✖ dplyr::lag()      masks stats::lag()\n#&gt; ✖ yardstick::spec() masks readr::spec()\n#&gt; ✖ recipes::step()   masks stats::step()\n\n如果你用过tidyverse，你会注意到一些熟悉的名字，因为有几个tidyverse包，比如dplyr和ggplot2，会和tidymodels包一起加载。我们已经说过，tidymodels框架将tidyverse的原则应用到建模中，而实际上，tidymodels框架也建立在一些最基础的tidyverse包之上，比如这些包。\n加载这个元包还会显示是否存在与之前加载的包的函数命名冲突。作为命名冲突的一个例子，在加载tidymodels之前，调用filter()函数会执行stats包中的该函数。加载tidymodels之后，它会执行同名的dplyr函数。\n有几种处理命名冲突的方法。可以使用函数的命名空间来调用该函数（例如，stats::filter()）。这并非不好的做法，但确实会降低代码的可读性。\n另一个选择是使用conflicted包。我们可以设置一条规则，该规则在R会话结束前一直有效，以确保如果代码中没有指定命名空间，某个特定的函数将始终运行。例如，如果我们更倾向于使用前面提到的函数的dplyr版本：\nlibrary(conflicted)\nconflict_prefer(\"filter\", winner = \"dplyr\")\n为方便起见，tidymodels包含一个函数，该函数可捕获我们可能遇到的大多数常见命名冲突：\n\ntidymodels_prefer(quiet = FALSE)\n#&gt; [conflicted] Will prefer agua::refit over any other package.\n#&gt; [conflicted] Will prefer DALEX::explain over any other package.\n#&gt; [conflicted] Will prefer dials::Laplace over any other package.\n#&gt; [conflicted] Will prefer dials::max_rules over any other package.\n#&gt; [conflicted] Will prefer dials::neighbors over any other package.\n#&gt; [conflicted] Will prefer dials::prune over any other package.\n#&gt; [conflicted] Will prefer dials::smoothness over any other package.\n#&gt; [conflicted] Will prefer dplyr::collapse over any other package.\n#&gt; [conflicted] Will prefer dplyr::combine over any other package.\n#&gt; [conflicted] Will prefer dplyr::filter over any other package.\n#&gt; [conflicted] Will prefer dplyr::rename over any other package.\n#&gt; [conflicted] Will prefer dplyr::select over any other package.\n#&gt; [conflicted] Will prefer dplyr::slice over any other package.\n#&gt; [conflicted] Will prefer ggplot2::`%+%` over any other package.\n#&gt; [conflicted] Will prefer ggplot2::margin over any other package.\n#&gt; [conflicted] Will prefer parsnip::bart over any other package.\n#&gt; [conflicted] Will prefer parsnip::fit over any other package.\n#&gt; [conflicted] Will prefer parsnip::mars over any other package.\n#&gt; [conflicted] Will prefer parsnip::pls over any other package.\n#&gt; [conflicted] Will prefer purrr::cross over any other package.\n#&gt; [conflicted] Will prefer purrr::invoke over any other package.\n#&gt; [conflicted] Will prefer purrr::map over any other package.\n#&gt; [conflicted] Will prefer recipes::discretize over any other package.\n#&gt; [conflicted] Will prefer recipes::step over any other package.\n#&gt; [conflicted] Will prefer recipes::update over any other package.\n#&gt; [conflicted] Will prefer rsample::populate over any other package.\n#&gt; [conflicted] Will prefer scales::rescale over any other package.\n#&gt; [conflicted] Will prefer themis::step_downsample over any other package.\n#&gt; [conflicted] Will prefer themis::step_upsample over any other package.\n#&gt; [conflicted] Will prefer tidyr::expand over any other package.\n#&gt; [conflicted] Will prefer tidyr::extract over any other package.\n#&gt; [conflicted] Will prefer tidyr::pack over any other package.\n#&gt; [conflicted] Will prefer tidyr::unpack over any other package.\n#&gt; [conflicted] Will prefer tune::parameters over any other package.\n#&gt; [conflicted] Will prefer tune::tune over any other package.\n#&gt; [conflicted] Will prefer yardstick::get_weights over any other package.\n#&gt; [conflicted] Will prefer yardstick::precision over any other package.\n#&gt; [conflicted] Will prefer yardstick::recall over any other package.\n#&gt; [conflicted] Will prefer yardstick::spec over any other package.\n#&gt; [conflicted] Removing existing preference.\n#&gt; [conflicted] Will prefer recipes::update over Matrix::update.\n#&gt; ── Conflicts ───────────────────────────────────────── tidymodels_prefer() ──\n\n请注意，使用此函数意味着您同意对所有命名空间冲突使用conflicted::conflict_prefer()，这会将每个冲突都视为错误，并强制您选择要使用的函数。tidymodels::tidymodels_prefer()函数会处理tidymodels函数中最常见的冲突，但您需要自行处理R会话中的其他冲突。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/3 A Review of R Modeling Fundamentals.html#chapter-summary",
    "title": "3 A Review of R Modeling Fundamentals",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章回顾了创建和使用模型的核心R语言约定，这些约定是本书其余部分的重要基础。公式运算符是在R中拟合模型时一个富有表现力且重要的方面，并且在非tidymodels函数中通常有多种用途。传统的R建模方法存在一些局限性，特别是在流畅地处理和可视化模型输出方面。tidymodels元包将tidyverse设计理念应用于建模包。",
    "crumbs": [
      "3 A Review of R Modeling Fundamentals"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html",
    "title": "1 Software for modeling",
    "section": "",
    "text": "模型是一种能描述系统及捕获系统内数据关系的数学工具。模型可用于各种目的，包括预测未来事件、确定几个群组之间是否存在差异、基于地图的辅助可视化、在数据中发现新的模式以供进一步研究等。模型的实用性取决于其还原能力，或者将复杂关系简化为简单项的能力。数据中的主要影响可以用数学方法有效地捕捉，例如可以用方程表示的关系。\n自21世纪初以来，数学模型以明显和微妙的方式在我们的日常生活中变得无处不在。对许多人来说，典型的一天可能包括查看天气以确定什么时候是遛狗的好时机、从网站订购产品、给朋友发送短信并让其自动更正，以及查看电子邮件。在每种情况下，都很可能涉及某种类型的模型。在某些情况下，模型的贡献可能很容易被察觉 (“你可能也对购买产品 X 感兴趣”), 而在其他情况下，影响可能是某些东西的缺失 (例如，垃圾邮件)。模型被用来选择顾客可能喜欢的服装，识别一种应被评估为药物候选物的分子，甚至可能是一家邪恶公司用来避免发现过度污染汽车的机制。无论如何，模型都会存在。\n本书主要聚焦于软件。显然，软件能否生成正确的关系来表示数据至关重要。在大多数情况下，判断数学上的正确性是有可能的，但要可靠地创建合适的模型，还需要更多条件。在本章中，我们将概述构建或选择建模软件时需考虑的因素、模型的用途，以及建模在更广泛的数据分析过程中所处的位置。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#fundamentals-for-modeling-software",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#fundamentals-for-modeling-software",
    "title": "1 Software for modeling",
    "section": "Fundamentals for Modeling Software",
    "text": "Fundamentals for Modeling Software\n建模软件易于正确操作，这一点至关重要。用户界面的设计不应糟糕到让用户意识不到自己使用不当。例如，Baggerly和Coombes(2009)在一份备受关注的计算生物学出版物中，报告了数据分析方面存在的诸多问题。其中一个问题与要求用户添加模型输入名称的方式有关。该软件的用户界面很容易导致数据的列名与实际的数据列错位。这导致错误的基因被认定为对治疗癌症患者很重要，并最终致使多项临床试验终止(Carlson 2012)。\n如果我们需要高质量的模型，软件必须便于正确使用。Abrams(2003)描述了一个有趣的原则来指导我们：与需要通过诸多考验和意外才能抵达的顶峰、巅峰或穿越沙漠的旅程形成鲜明对比的是，我们希望客户只需使用我们的平台和框架，就能自然而然地采用成功的实践方法——成功之坑。数据分析和建模软件应该拥护这一理念。其次，建模软件应当倡导良好的科学方法论。随着我们的模型变得更加强大和复杂，犯下潜在错误也变得更加容易。在处理复杂的预测模型时，人们很容易在不知不觉中犯下与逻辑谬误或不当假设相关的错误。许多机器学习模型在发现模式方面极为擅长，它们能毫不费力地在数据中找到经验模式，但这些模式在之后却无法复现。有些方法论错误具有隐蔽性，其问题可能直到后来获得包含真实结果的新数据时才会被发现。同样的原则也适用于编程。只要有可能，软件就应该能够保护用户避免犯错。软件应当让用户易于做出正确的操作。\n模型开发的这两个方面——易于正确使用和良好的方法实践——至关重要。由于创建模型的工具易于获取，且模型能产生深远的影响，因此越来越多的人开始创建模型；但创建者的背景各不相同，他们使用的工具就需要能适应不同用户的经验水平——既要有足够的能力来创建高性能模型，又要易于正确使用。本书介绍了一套建模软件，其设计就考虑到了这些特点。\n该软件基于R编程语言(R核心团队，2014)。R是专为数据分析和建模设计的。它是S语言的一种实现(其词法作用域规则借鉴了Scheme和Lisp)，S语言创建于20世纪70年代，旨在“快速且忠实地将想法转化为软件”(Chambers 1998)。R，开源且免费，是一种功能强大的编程语言，可用于多种不同用途，但专长于数据分析、建模、可视化和机器学习。R 具有良好的可扩展性，它拥有庞大的软件包生态系统，其中大部分是用户贡献的模块，专注于特定主题，如建模、可视化等。\n有一组软件包集合被称为tidyverse(Wickham et al. 2019)。tidyverse是一组具有明确观点的R软件包集合，专为数据科学设计。所有软件包都共享相同的底层设计理念、语法和数据结构。其中一些设计理念直接源于本章所描述的建模软件的各个方面。如果你从未使用过tidyverse软件包，第2章包含了基本概念的回顾。在tidyverse中，专门专注于建模的那部分软件包被称为tidymodels软件包。本书是一本使用tidyverse和tidymodels软件包进行建模的实用指南。它展示了如何将一组各有特定用途的软件包结合起来，创建高质量的模型。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#types-of-models",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#types-of-models",
    "title": "1 Software for modeling",
    "section": "Types of Models",
    "text": "Types of Models\n在继续之前，让我们描述一种按用途分组的模型类型分类法。这种分类法既说明了模型的使用方式，也说明了模型创建或评估的诸多方面。虽然这份列表并非详尽无遗，但大多数模型至少属于以下类别之一：\nDescriptive models\n描述性模型的目的是描述或阐明某些数据的特征。这种分析的唯一目的可能就是直观地强调数据中的某种趋势或特征。\n例如，一段时间以来，利用微阵列对RNA进行大规模测量已成为可能。早期的实验室方法是将生物样本放置在小型微芯片上。芯片上非常小的位置能够根据特定RNA序列的丰度来测量信号。这种芯片会包含数千个(甚至更多)结果，每个结果都是与某一生物过程相关的RNA的量化数据。然而，芯片可能存在质量问题，进而导致结果不佳。例如，不小心在芯片的某一部分留下指纹，可能会在扫描时造成测量不准确。\n一种用于评估此类问题的早期方法是探针水平模型(probe-level models，简称PLMs)(Bolstad 2004)。该模型会考虑数据中已知的差异，比如芯片、RNA序列、RNA类型等。如果数据中存在其他未知因素，这些影响会体现在模型残差中。当按照残差在芯片上的位置绘图时，质量良好的芯片不会呈现出任何规律。而当出现问题时，某种空间模式就会变得可识别。通常，模式的类型会暗示潜在的问题(例如，指纹)以及可能的解决方案(擦拭芯片后重新扫描、重新制备样本等)。图1.1(a)展示了这种方法在两个微阵列上的应用，这些微阵列来自Gentleman等人(2005)的研究。图像呈现出两种不同的颜色值：颜色较深的区域表示信号强度大于模型预期值，而颜色较浅的区域则表示信号强度低于预期值。左侧面板显示出相当随机的模式，而右侧面板则在芯片中间出现了不理想的伪影。\n\n\nFigure 1.1\n\n描述性模型的另一个例子是局部估计散点图平滑模型(locally estimated scatterplot smoothing model)，更常见的名称是LOESS(Cleveland 1979 年)。在这里，会为数据集拟合一个平滑且灵活的回归模型，该数据集通常只有一个自变量，而拟合出的回归线被用于阐明数据中的某种趋势。这类平滑器用于发现模型中变量的潜在表示方法。图1.1(b)就展示了这一点，其中灵活的平滑器揭示了一种非线性趋势。从该图中可以清楚地看出，房屋售价与其纬度之间存在高度的非线性关系。\nInferential models\n推断模型的目标是为研究问题做出决策或探索特定假设，这与统计检验的使用方式类似。推断模型始于对总体预先设定的推测或想法，并得出统计结论，如区间估计或对某一假设的拒绝。\n例如，临床试验的目标可能是证实一种新疗法在延长生命方面比其他疗法(如现有疗法或完全不治疗)更有效。如果临床试验的终点与患者的生存相关，那么零假设可能是新疗法的中位生存时间相等或更短，而备择假设则是新疗法的中位生存时间更长。如果通过建模采用传统的零假设显著性检验来评估该试验，显著性检验会基于对数据的一系列假设，使用某种预先定义的方法计算出P值。模型结果中较小的P值表明有证据显示新疗法有助于患者延长寿命。模型结果中较大的P值则表明未能证明存在这种差异；这种证据的缺乏可能由多种原因导致，包括疗法无效。\n这类分析的重要方面是什么？推断建模技术通常会产生某种类型的概率输出，例如p值、置信区间或后验概率。一般来说，要计算这样的量，必须对数据以及生成数据的潜在过程做出正式的概率假设。统计建模结果的质量在很大程度上取决于这些预先定义的假设，以及观察到的数据与这些假设的吻合程度。这里最关键的因素是理论层面的：“如果我的数据是独立的，且残差服从分布X，那么检验统计量Y可用于生成p值。否则，所得p值可能不准确。”\n推断性分析的一个方面是，在理解数据与模型假设的匹配程度时，往往存在一个延迟的反馈循环。在我们的临床试验示例中，如果统计学(和临床)显著性表明新疗法应该可供患者使用，但可能还需要数年时间，该疗法才能在实际中应用，并且才能生成足够的数据，以便独立评估最初的统计分析是否得出了恰当的决策。\nPredictive models\n有时，数据会被建模，以尽可能对新数据做出最准确的预测。在这里，主要目标是让预测值与新数据的真实值尽可能地吻合。\n一个简单的例子是，图书采购商要预测下个月应该向自己的门店运送多少本某一特定书籍。预测过高会因为书籍过剩而浪费空间和资金。如果预测低于实际所需，就会造成机会损失和利润减少。\n对于这类模型，问题类型属于估计而非推断。例如，买家通常不会关心诸如“下个月我能卖出超过100本X书吗？”这样的问题，而是更关注“下个月顾客会购买多少本X书？”此外，根据具体情境，人们可能并不在意预测值为何是X。换句话说，相比评估与数据相关的正式假设，人们更关注预测值本身。预测还可以包含不确定性度量。就图书买家而言，提供预测误差可能有助于决定购买多少本书，它也可以作为衡量预测方法效果的一个指标。\n影响预测模型的最重要因素是什么？创建预测模型的方法有很多种，因此重要因素取决于模型的开发方式。\n可以利用基本原理推导出一个机械模型(mechanistic model)，从而得到一个依赖于假设的模型方程。例如，在预测某一时刻人体内的药物量时，需要对药物的给药、吸收、代谢和消除方式做出一些正式假设。基于此，可以使用一组微分方程推导出特定的模型方程。数据被用于估计该方程的未知参数，以便生成预测结果。与推理模型一样，机械预测模型在很大程度上依赖于定义其模型方程的假设。然而，与推理模型不同的是，根据模型对现有数据的预测效果，很容易做出基于数据的关于模型性能的表述。在这里，建模从业者的反馈循环比假设检验的反馈循环快得多。\n经验驱动模型(empirically driven models)的创建基于更模糊的假设。这类模型往往属于机器学习范畴。一个很好的例子是K近邻(KNN)模型。给定一组参考数据，通过使用参考集中K个最相似数据的值来预测新样本。例如，如果一个图书采购员需要对一本新书进行预测，可能会有现有图书的历史数据可用。5近邻模型会根据与这本新书最相似的五本书的销量(基于某种“相似性”定义)来估计这本新书的采购数量。该模型仅由预测结构(五本相似图书的平均值)来定义。对于销量数据或用于定义相似性的变量，没有做出任何理论上或概率上的假设。事实上，评估该模型适用性的主要方法是利用现有数据来评估其准确性。如果这类模型的结构选择得当，其预测结果将接近实际值。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#connections-between-types-of-models",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#connections-between-types-of-models",
    "title": "1 Software for modeling",
    "section": "Connections Between Types of Models",
    "text": "Connections Between Types of Models\n请注意，我们是根据模型的使用方式而非其数学特性来定义模型类型的。普通线性回归模型可能会属于这三类模型中的任何一类，这取决于它的使用方式：\n\n一种类似于LOESS的描述性平滑器，称为限制性平滑样条(Durrleman和Simon 1989年)，可以通过带有专门项的普通线性回归来描述数据中的趋势。\n方差分析(ANOVA)模型是生成用于推断的p值的常用方法。方差分析模型是线性回归的一种特例。\n如果简单线性回归模型能产生准确的预测，那么它就可以用作预测模型。\n\n有许多预测模型无法(或者至少不应该)用于推断的例子。即使对数据做出了概率假设，例如，K近邻模型的特性也使得推断所需的数学计算难以处理。\n模型类型之间还有另一种联系。虽然描述性模型和推断性模型的主要目的可能与预测无关，但不应忽视模型的预测能力。例如，逻辑回归是一种常用于结果为具有两个可能值的定性数据的模型。它可以模拟变量与结果概率之间的关系。当用于推断时，人们会高度关注模型的统计性质。例如，分析师往往会着重关注模型中包含的自变量的选择。可能会通过多次模型构建迭代来确定与结果变量具有“统计显著性”关系的最小自变量子集。通常，当所有自变量的p值都低于某个特定值(例如0.05)时，就达到了这一目标。在此基础上，分析师可能会着重对变量对结果的相对影响做出定性陈述(例如，“年龄与患心脏病的几率之间存在统计上的显著关系”)。\n然而，当将统计显著性作为衡量模型质量的唯一标准时，这种方法可能具有危险性。有可能这个经过统计优化的模型具有较差的模型准确性，或者在其他一些预测能力指标上表现不佳。尽管该模型可能不会用于预测，但对于一个p值显著却准确性极差的模型，其得出的推论又能有多少可信度呢？预测性能往往与模型的拟合值和观测数据的接近程度有关。如果一个模型与数据的保真度有限，那么该模型生成的推论就非常值得怀疑。换句话说，统计显著性可能不足以证明一个模型是合适的。这在直觉上可能显而易见，但在现实世界的数据分析中却常常被忽视。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#some-terminology",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#some-terminology",
    "title": "1 Software for modeling",
    "section": "Some Terminology",
    "text": "Some Terminology\n在继续之前，我们将概述与建模和数据相关的其他术语。这些描述旨在帮助你阅读本书，但并非详尽无遗。\n首先，许多模型可以分为有监督或无监督两类。无监督模型是指那些学习数据的模式、聚类或其他特征但缺乏结果(即因变量)的模型。主成分分析(PCA)、聚类和自编码器都是无监督模型的例子；它们用于理解变量之间或变量集之间的关系，而无需预测变量与结果之间存在明确的关系。有监督模型是那些具有结果变量的模型。线性回归、神经网络以及许多其他方法都属于这一类。\n在有监督模型中，有两个主要的子类别：\n\n回归用于预测数值型结果。\n分类用于预测属于有序或无序定性值集合的结果。\n\n这些定义并不完善，也没有涵盖所有可能的模型类型。在第6章中，我们将监督技术的这一特征称为模型模式。\n不同的变量可以有不同的作用，尤其是在监督建模分析中。结果,也称为标签(labels)、端点(endpoints)或因变量(dependent variables)是监督模型中预测的值。自变量(independent variables)是预测结果的基础，也被称为预测因子(predictors)、特征(features)或协变量(covariates)(取决于上下文)。本书中最常用的术语是结果和预测因子。\n就数据或变量本身而言，无论是用于有监督模型还是无监督模型，作为预测变量还是结果变量，主要分为两类：定量的和定性的。前者的例子包括像3.14159这样的实数以及像42这样的整数。定性值，也称为名义数据，指的是代表某种离散状态、无法自然地置于数值尺度上的值，例如“红色”“绿色”和“蓝色”。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#how-does-modeling-fit-into-the-data-analysis-process",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#how-does-modeling-fit-into-the-data-analysis-process",
    "title": "1 Software for modeling",
    "section": "How Does Modeling Fit into the Data Analysis Process?",
    "text": "How Does Modeling Fit into the Data Analysis Process?\n在哪些情况下会创建模型？在进行这样的工作之前有哪些步骤吗？模型创建是数据分析的第一步吗？在建模之前，总有几个关键的数据分析阶段。\n首先，存在一个长期被低估的过程，即数据清洗。无论在何种情况下，你都应该对数据进行调查，以确保它们适用于你的项目目标、准确且恰当。这些步骤通常比数据分析过程的其他部分花费更多时间(具体取决于实际情况)。\n数据清洗也可能与第二阶段——理解数据(通常称为探索性数据分析，即EDA)重叠。探索性数据分析会揭示不同变量之间的关系、它们的分布情况、典型范围以及其他属性。在这个阶段，一个值得问的问题是：“我是如何获得这些数据的？”这个问题能帮助你了解手头的数据是如何被抽样或筛选的，以及这些操作是否恰当。例如，在合并数据库表时，连接操作可能会出错，从而意外地排除一个或多个子群体。另一个好主意是询问这些数据是否相关。例如，要预测患者是否患有阿尔茨海默病，使用包含该病患者和从普通人群中随机抽取的健康成年人的数据集是不明智的。鉴于这种疾病的进行性，模型可能只会预测出哪些患者年龄最大。\n最后，在开始数据分析过程之前，应该对模型的目标以及如何评判其性能(和成功与否)有明确的预期。至少要确定一个具有可实现的现实目标的性能指标。第9章将更详细地讨论一些常见的统计指标，例如分类准确率、真阳性率和假阳性率、均方根误差等。应该权衡这些指标的相对优缺点。同样重要的是，所选指标要贴切；它与更广泛的数据分析目标保持一致至关重要。\n对数据进行调查的过程可能并不简单。Wickham和Grolemund(2016)对一般数据分析过程进行了精彩的阐述，其内容转载于图1.2中。数据获取以及数据清理/整理被列为初始步骤。当用于理解数据的分析步骤开始时，它们是一个启发式过程；我们无法预先确定这些步骤可能需要多长时间。转换、建模和可视化这一循环往往需要多次迭代。\n\n\nFigure 1.2: The data science process (from R for Data Science, used with permission)\n\n这种迭代过程在建模方面尤为如此。图1.3模拟了确定合适模型的典型路径。大致阶段如下：\n\n探索性数据分析(EDA)：起初，数值分析和数据可视化之间会有反复的互动(如图1.2所示)，在这个过程中，不同的发现会引发更多问题，进而促使人们开展数据分析的支线任务，以获得更深入的理解。\n特征工程：从探索性数据分析(EDA)中获得的理解会促使人们创建特定的模型项，这些模型项能让对观测数据进行准确建模变得更加容易。这可能包括复杂的方法(例如主成分分析(PCA))或更简单的特征(使用两个预测变量的比率)。第8章专门探讨这一重要步骤。\n模型调优与选择(带有交替片段的大圆圈)：会生成多种模型并对它们的性能进行比较。有些模型需要参数调优，其中一些结构参数必须被指定或优化。圆圈内的交替片段表示重采样过程中使用的重复数据拆分(参见第10章)。\n模型评估：在模型开发的这一阶段，我们会评估模型的性能指标，检查残差图，并进行其他类似探索性数据分析(EDA)的分析，以了解模型的运行效果。在某些情况下，正式的模型间比较(第11章)能帮助你了解模型之间的差异是否在实验误差范围内。\n\n\n\nFigure 1.3: A schematic for the typical modeling process\n\n在完成这些任务的初始序列后，人们会对哪些模型更优以及哪些数据子群体没有得到有效估计有更深入的理解。这会引发额外的探索性数据分析(EDA)和特征工程、另一轮建模，依此类推。一旦达成数据分析目标，通常最后的步骤就是对模型进行定稿、文档记录和交流。对于预测模型，最后通常会使用一组专门为此目的预留的额外数据来验证模型。\n例如，M.Kuhn和Johnson(2020)利用数据对芝加哥公共铁路系统的每日客流量进行建模，所使用的预测变量包括日期、以往的客流量结果、天气以及其他因素。表1.1大致呈现了这些作者在分析这些数据并最终选择一个性能足够的模型时，其假设的内心想法。\n\nHypothetical inner monologue of a model developer.\n\n\n\n\n\nThoughts\nActivity\n\n\n\n车站之间的每日客流量数值具有极强的相关性。\nEDA\n\n\n工作日和周末的客流量看起来大不相同。\nEDA\n\n\n2010年夏天的某一天，乘客数量异常庞大。\nEDA\n\n\n哪些车站的每日客流量最低？\nEDA\n\n\n日期至少应编码为星期几和年份。\nFeature Engineering\n\n\n或许可以对这些相关的预测变量使用主成分分析(PCA)，以便模型能更轻松地运用它们。\nFeature Engineering\n\n\n每小时的天气记录或许应该汇总为每日的测量数据。\nFeature Engineering\n\n\n让我们从简单线性回归、K近邻和梯度提升决策树开始。\nModel Fitting\n\n\n应该使用多少个邻居？\nModel Tuning\n\n\n我们应该进行大量的 boosting 迭代，还是只进行少量迭代呢？\nModel Tuning\n\n\n对于这些数据，多少个邻居似乎是最优的？\nModel Tuning\n\n\n哪些模型的均方根误差最低？\nModel Evaluation\n\n\n哪些日子的预测效果很差？\nEDA\n\n\n变量重要性得分表明天气信息不具有预测性。我们将在接下来的模型集中剔除它们。\nModel Evaluation\n\n\n看来我们应该专注于为该模型进行大量的提升迭代。\nModel Evaluation\n\n\n我们需要对节假日特征进行编码，以改进在这些日期(及前后)的预测。\nFeature Engineering\n\n\n让我们把KNN从模型列表中剔除。\nModel Evaluation",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/1 Software for modeling.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/1 Software for modeling.html#chapter-summary",
    "title": "1 Software for modeling",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章重点介绍了模型如何描述数据中的关系，以及不同类型的模型，如描述性模型、推断性模型和预测性模型。即使模型的主要目标不是预测，其预测能力也可用于对其进行评估。建模本身处于更广泛的数据分析过程中，而探索性数据分析是构建高质量模型的关键部分。",
    "crumbs": [
      "1 Software for modeling"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html",
    "href": "Books/R4DS2/27 A field guide to base R.html",
    "title": "27 A field guide to base R",
    "section": "",
    "text": "本章介绍一些base R中的重要函数：\n\n提取多个元素——[\n提取单个元素——[&$\napply家族\nfor循环\nPlot\n\n\nlibrary(tidyverse)",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#提取向量",
    "href": "Books/R4DS2/27 A field guide to base R.html#提取向量",
    "title": "27 A field guide to base R",
    "section": "提取向量",
    "text": "提取向量\n五种常见情景：\n\n正整数表示元素位置提取，重复提取生成重复元素的向量。\n\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#&gt; [1] \"three\" \"two\"   \"five\"\n\nx[c(1, 1, 5, 5, 5, 2)]\n#&gt; [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\n\n\n负整数表示删除对应位置的元素。\n\n\nx[c(-1, -3, -5)]\n#&gt; [1] \"two\"  \"four\"\n\n\n逻辑向量提取值为TRUE的元素；关于NA的处理与dplyr::filter()不同，前者保留，后者不保留。\n\n\nx &lt;- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n#&gt; [1] 10  3  5  8  1\n\n# All even (or missing!) values of x\nx[x %% 2 == 0]\n#&gt; [1] 10 NA  8 NA\n\n\n字符串向量提取有name属性的向量元素。\n\n\nx &lt;- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#&gt; xyz def \n#&gt;   5   2\n\n\nnothing–x[]返回完整的对象，在后面对data.frame提取时有用。",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#提取数据框",
    "href": "Books/R4DS2/27 A field guide to base R.html#提取数据框",
    "title": "27 A field guide to base R",
    "section": "提取数据框",
    "text": "提取数据框\n使用df[rows, cols]提取数据框中对应的行或列；其中rows和cols与上面的使用方法一致。\n\ndf &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"e\", \"f\"),\n  z = runif(3)\n)\n\n# Select first row and second column\ndf[1, 2]\n#&gt; # A tibble: 1 × 1\n#&gt;   y    \n#&gt;   &lt;chr&gt;\n#&gt; 1 a\n\n# Select all rows and columns x and y\ndf[, c(\"x\", \"y\")]\n#&gt; # A tibble: 3 × 2\n#&gt;       x y    \n#&gt;   &lt;int&gt; &lt;chr&gt;\n#&gt; 1     1 a    \n#&gt; 2     2 e    \n#&gt; 3     3 f\n\n# Select rows where `x` is greater than 1 and all columns\ndf[df$x &gt; 1, ]\n#&gt; # A tibble: 2 × 3\n#&gt;       x y         z\n#&gt;   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     2 e     0.834\n#&gt; 2     3 f     0.601\n\ndata.frame格式与tibble格式的数据框在使用[上的唯一区别是：当df[,cols]中的cols只有一个元素时，data.frame格式返回向量，而tibble格式仍返回tibble。\n\ndf1 &lt;- data.frame(x = 1:3)\ndf1[, \"x\"]\n#&gt; [1] 1 2 3\n\ndf2 &lt;- tibble(x = 1:3)\ndf2[, \"x\"]\n#&gt; # A tibble: 3 × 1\n#&gt;       x\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n\ndata.frame格式使用drop参数，可以避免降维。\n\ndf1[, \"x\", drop = FALSE]\n#&gt;   x\n#&gt; 1 1\n#&gt; 2 2\n#&gt; 3 3",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#dplyr-中的等价操作",
    "href": "Books/R4DS2/27 A field guide to base R.html#dplyr-中的等价操作",
    "title": "27 A field guide to base R",
    "section": "dplyr 中的等价操作",
    "text": "dplyr 中的等价操作\n在dplyr包中有几个verb等价于[的特例：\n\nfilter()：等价于按行使用逻辑向量提取，但对于NA的处理不同，filter()不保留NA，而[保留。\n\n\ndf &lt;- tibble(\n  x = c(2, 3, 1, 1, NA),\n  y = letters[1:5],\n  z = runif(5)\n)\ndf |&gt; filter(x &gt; 1)\n#&gt; # A tibble: 2 × 3\n#&gt;       x y           z\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1     2 a     0.157  \n#&gt; 2     3 b     0.00740\n\n# same as\ndf[!is.na(df$x) & df$x &gt; 1, ]\n#&gt; # A tibble: 2 × 3\n#&gt;       x y           z\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1     2 a     0.157  \n#&gt; 2     3 b     0.00740\n\ndf[which(df$x &gt; 1), ]\n#&gt; # A tibble: 2 × 3\n#&gt;       x y           z\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1     2 a     0.157  \n#&gt; 2     3 b     0.00740\n\n\narrange()：等价于按行使用正整数向量提取，向量通常由order()生成。\n\n\ndf |&gt; arrange(x, y)\n#&gt; # A tibble: 5 × 3\n#&gt;       x y           z\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1     1 c     0.466  \n#&gt; 2     1 d     0.498  \n#&gt; 3     2 a     0.157  \n#&gt; 4     3 b     0.00740\n#&gt; 5    NA e     0.290\n\n# same as\ndf[order(df$x, df$y), ]\n#&gt; # A tibble: 5 × 3\n#&gt;       x y           z\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1     1 c     0.466  \n#&gt; 2     1 d     0.498  \n#&gt; 3     2 a     0.157  \n#&gt; 4     3 b     0.00740\n#&gt; 5    NA e     0.290\n\n\nselect() & relocate()：等价于按列使用字符向量提取。\n\n\ndf |&gt; select(x, z)\n#&gt; # A tibble: 5 × 2\n#&gt;       x       z\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     2 0.157  \n#&gt; 2     3 0.00740\n#&gt; 3     1 0.466  \n#&gt; 4     1 0.498  \n#&gt; 5    NA 0.290\n\n# same as\ndf[, c(\"x\", \"z\")]\n#&gt; # A tibble: 5 × 2\n#&gt;       x       z\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     2 0.157  \n#&gt; 2     3 0.00740\n#&gt; 3     1 0.466  \n#&gt; 4     1 0.498  \n#&gt; 5    NA 0.290",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#data-frames",
    "href": "Books/R4DS2/27 A field guide to base R.html#data-frames",
    "title": "27 A field guide to base R",
    "section": "Data Frames",
    "text": "Data Frames\n[[和$用来提取数据框中的某列；[[可以通过位置或name属性提取，而$只能通过name属性提取。\n\ntb &lt;- tibble(\n  x = 1:4,\n  y = c(10, 4, 1, 21)\n)\n\n# by position\ntb[[1]]\n#&gt; [1] 1 2 3 4\n\n# by name\ntb[[\"x\"]]\n#&gt; [1] 1 2 3 4\ntb$x\n#&gt; [1] 1 2 3 4\n\ndplyr包提取了pull()函数，它等价于[[和$。\n\ntb |&gt; pull(x)\n#&gt; [1] 1 2 3 4",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#tibbles",
    "href": "Books/R4DS2/27 A field guide to base R.html#tibbles",
    "title": "27 A field guide to base R",
    "section": "Tibbles",
    "text": "Tibbles\ndata.frame与tibble在使用$时有着显著的不同；前者遵循部分匹配原则，后者使用精确匹配原则。\n\ndf &lt;- data.frame(x1 = 1)\ndf$x\n#&gt; [1] 1\ndf$z\n#&gt; NULL\n\n\ntb &lt;- tibble(x1 = 1)\n\ntb$x1\n#&gt; [1] 1\ntb$z\n#&gt; NULL",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#dplyrmutate的等价操作",
    "href": "Books/R4DS2/27 A field guide to base R.html#dplyrmutate的等价操作",
    "title": "27 A field guide to base R",
    "section": "dplyr::mutate的等价操作",
    "text": "dplyr::mutate的等价操作\n下面是使用with(),within()和transform()进行等价操作的例子。\n\n\ndata(diamonds, package = \"ggplot2\")\n\n# Most straightforward\ndiamonds$ppc &lt;- diamonds$price / diamonds$carat\n\n# Avoid repeating diamonds\ndiamonds$ppc &lt;- with(diamonds, price / carat)\n\n# The inspiration for dplyr's mutate\ndiamonds &lt;- transform(diamonds, ppc = price / carat)\ndiamonds &lt;- diamonds |&gt; transform(ppc = price / carat)\n\n# Similar to transform(), but uses assignment rather argument matching\n# (can also use = here, since = is equivalent to &lt;- outside of a function call)\ndiamonds &lt;- within(diamonds, {\n  ppc &lt;- price / carat\n})\ndiamonds &lt;- diamonds |&gt; within({\n  ppc &lt;- price / carat\n})\n\n# Protect against partial matching\ndiamonds$ppc &lt;- diamonds[[\"price\"]] / diamonds[[\"carat\"]]\ndiamonds$ppc &lt;- diamonds[, \"price\"] / diamonds[, \"carat\"]\n\n# FORBIDDEN\nattach(diamonds)\ndiamonds$ppc &lt;- price / carat",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/R4DS2/27 A field guide to base R.html#lists",
    "href": "Books/R4DS2/27 A field guide to base R.html#lists",
    "title": "27 A field guide to base R",
    "section": "lists",
    "text": "lists\n[，[[和$都可以提取list中的元素，但[保留原list层级，而[[和$不保留。\n\nl &lt;- list(\n  a = 1:3,\n  b = \"a string\",\n  c = pi,\n  d = list(-1, -5)\n)\n\nstr(l[1:2])\n#&gt; List of 2\n#&gt;  $ a: int [1:3] 1 2 3\n#&gt;  $ b: chr \"a string\"\n\nstr(l[1])\n#&gt; List of 1\n#&gt;  $ a: int [1:3] 1 2 3\nstr(l[[1]])\n#&gt;  int [1:3] 1 2 3\n\nstr(l[4])\n#&gt; List of 1\n#&gt;  $ d:List of 2\n#&gt;   ..$ : num -1\n#&gt;   ..$ : num -5\nstr(l[[4]])\n#&gt; List of 2\n#&gt;  $ : num -1\n#&gt;  $ : num -5\n\n两者的差异如下图所示：",
    "crumbs": [
      "27 A field guide to base R"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/index.html",
    "href": "Books/Advanced R(2e)/index.html",
    "title": "index",
    "section": "",
    "text": "本篇为书籍Advanced R的学习笔记。\n原文见：Advanced R\n本书主要包含两方面：\n\nR 语言底层逻辑\n高级编程技巧\n\n下面是全文概览：\n\n第1章：简介。\n第2-8章：R语言基础概念。\n第9-11章：函数编程。\n第12~16章：面向对象编程。\n第17~21章：元编程。\n第22-25章：高级编程技巧。\n\n本书适合在不同阶段，根本不同需求阅读不同章节。\n本书中的练习题解决方案：Advanced R Solutions\n\n\n\n Back to top",
    "crumbs": [
      "index"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Object-oriented Programming.html",
    "href": "Books/Advanced R(2e)/Object-oriented Programming.html",
    "title": "Introduction",
    "section": "",
    "text": "与其他语言相比，R 中的面向对象编程（object-oriented programming(OOP)）更有挑战性。\n在R语言中，泛函编程要比面向对象编程更重要，因为在解决复杂问题时，我们通常会使用多个函数而非多个对象进行解决。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Object-oriented Programming.html#outline",
    "href": "Books/Advanced R(2e)/Object-oriented Programming.html#outline",
    "title": "Introduction",
    "section": "Outline",
    "text": "Outline\n\n12章：介绍所有OOP系统依赖的底层概念——base types。\n13章：介绍S3系统。\n14章：介绍R6系统。\n15章：介绍S4系统。\n16章：对比三种OOP系统。\n\n本书侧重于OOP的“机制”，而非其“高效使用”。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Object-oriented Programming.html#oop-systems",
    "href": "Books/Advanced R(2e)/Object-oriented Programming.html#oop-systems",
    "title": "Introduction",
    "section": "OOP systems",
    "text": "OOP systems\n使用OOP系统的主要原因是——polymorphism或encapsulation。\n\npolymorphism：函数接受不同类型的输入，根据输入类型执行不同的操作，返回对应的结果。\nencapsulation：封装函数，隐藏数据，只提供接口，接口的调用者不需要知道数据如何实现。\n\npolymorphism在R中的直接感受就是——summary()函数可以根据输入类型的不同，返回不同结果。\n\ndiamonds &lt;- ggplot2::diamonds\n\nsummary(diamonds$carat)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2000  0.4000  0.7000  0.7979  1.0400  5.0100\n\nsummary(diamonds$cut)\n#&gt;      Fair      Good Very Good   Premium     Ideal \n#&gt;      1610      4906     12082     13791     21551\n\nOOP系统通过对象的class属性为对象分配其可用的方法method。class属性同时也定义了域field。class之间是层级结构，具有继承inherit关系，当对象所属的class没有定义method时，会向上查找父类中的method，这个过程叫做method dispatch。\n在R中主要有两种类型的OOP系统：\n\nencapsulated OOP：对象包含了数据（field）和方法（method），调用方法类似object.method(arg1, arg2)。\nfunctional OOP：方法属于泛型函数，使用方法与常规函数无异：generic(object, arg2, arg3)。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Object-oriented Programming.html#oop-in-r",
    "href": "Books/Advanced R(2e)/Object-oriented Programming.html#oop-in-r",
    "title": "Introduction",
    "section": "OOP in R",
    "text": "OOP in R\nbase R 提供了三种OOP系统：S3，S4，RC。\n\nS3系统会使函数返回丰富的结果，具有用户友好的显示和程序员友好的内部功能。S3系统贯穿于整个base R，因此如果你想扩展base R中的函数以处理新的输入类型，掌握S3系统非常重要。\nS4系统由method包支持，是一个严格的系统，会迫使你仔细思考程序设计。它特别适合构建随时间推移不断演变并且接受其他程序员贡献的的大型系统。它也是Bioconductor项目的主要框架。\nRC系统是一种特殊类型的S4系统，它们也可以被原地修改，但不使用R的“复制后修改”语义，实现了封装（encapsulated OOP）。\n\n其他R包提供的OOP系统：\n\nR6包系统提供了一种标准化的方式来规避R中的的“复制后修改”语义。这意味着在使用R6系统时，对象的属性不会被复制，而是直接引用原始数据；将对象传递给其他函数或程序，不需要担心它们被意外地修改。\nR.oo包提供了修改S3对象的机制。\nproto包提供了一种基于prototype思想的OOP系统，是ggplot2包使用的OOP。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Object-oriented Programming.html#sloop",
    "href": "Books/Advanced R(2e)/Object-oriented Programming.html#sloop",
    "title": "Introduction",
    "section": "sloop",
    "text": "sloop\n后续我们会经常使用到sloop(sail the seas of OOP)包，这个包提供了一些工具来查看R对象。例如sloop::otype()用来查看对象的OOP类型。\n\nlibrary(sloop)\n\notype(1:10)\n#&gt; [1] \"base\"\n\notype(mtcars)\n#&gt; [1] \"S3\"\n\nmle_obj &lt;- stats4::mle(function(x = 1) (x - 2)^2)\notype(mle_obj)\n#&gt; [1] \"S4\"",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Functional programming.html",
    "href": "Books/Advanced R(2e)/Functional programming.html",
    "title": "Introduction",
    "section": "",
    "text": "R 在本质上是一种泛函（functional）语言，呈现出一种以函数为中心的问题解决风格。下面我简要概述泛函语言的技术定义，重点讲解泛函编程风格，这对于日常的数据分析十分有用。\n泛函编程风格倾向于创建能够单独运行，便于自动优化或并行化的函数。它的传统缺点，如性能较差和有时不可预测的内存使用，近年来已经大大减少，可以作为面向对象编程的一种补充。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Functional programming.html#functional-programming-languages",
    "href": "Books/Advanced R(2e)/Functional programming.html#functional-programming-languages",
    "title": "Introduction",
    "section": "Functional programming languages",
    "text": "Functional programming languages\n每种编程语言都有函数，那么是什么使一种编程语言具有泛函呢？原因有很多，但有两个比较重要：\n\nfirst-class functions\n所谓第一类函数即是具有其他数据结构特点的函数。在R中，它可以：被赋值给其他变量，储存在list中，传递到其他函数中，被其他函数创建，甚至被函数返回。\n\n\npure function\n纯函数需要满足两个条件：\n\n输入决定输出：只要输入一样，输出就一样。反例：runif(),read.csv(),Sys.time()。\n没有副作用：比如改变全局变量，写入磁盘，在屏幕上显示。反例：print(),write.csv(),&lt;-。\n\n严格将，R 并不是一种泛函编程语言，因为它并不要求你的函数是纯函数。但是当你在编写泛函风格的代码时，你应当尽可能书写纯函数。通常，极端的纯函数或非纯函数更容易理解和改写。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Functional programming.html#functional-style",
    "href": "Books/Advanced R(2e)/Functional programming.html#functional-style",
    "title": "Introduction",
    "section": "Functional style",
    "text": "Functional style\n通俗的讲，泛函编程风格就是讲大问题分割为小问题，每个小问题编写一个函数去解决。解决小问题的函数是简单明了且能独立运行的，你需要做的是按照不同需求组合这些函数。\n接下来的三个章节分别讨论了三种泛函编程技巧：\n\n第9章（functional）：使用泛函编程改写for循环，如lapply()等函数。\n第10章（function factories）：输入向量，输出函数。\n第11章（function operators）：输入函数，输出函数。\n\n\n\n\n\n\n\n\nTip\n\n\n\n注意：图片中的Vector，它可以是任何数据结构，因为R中的所有数据结构都是基于Vector构建的。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html",
    "href": "Books/Advanced R(2e)/9 Functionals.html",
    "title": "9 Functionals",
    "section": "",
    "text": "泛函是一种以函数作为输入，输出向量的函数。下面是一个简单的泛函函数示例：\n\nrandomise &lt;- function(f) f(runif(1e3))\nrandomise(mean)\n#&gt; [1] 0.5049301\nrandomise(mean)\n#&gt; [1] 0.4924958\nrandomise(sum)\n#&gt; [1] 498.4892\n\nbase R 中常见的apply家族函数就属于泛函，还有purrr包中的map系列函数，以及一些数学泛函——integrate(),optim()。\nbase R 中的for循环优先级：泛函 &gt; for &gt; while &gt; repeat。如果你对for循环很熟悉，转换到泛函时，只需要从for循环中的提取函数，将其作为参数传入符合要求的泛函即可。当你找不到符合要求的泛函时，首先不要硬适配某种泛函，优先使用for循环，当类似的循环逻辑被重复使用时，考虑编写自己的泛函。\n\n\n\n9.2节：介绍purrr::map()。\n9.3节：讲解如何使用多个简单的泛函组合解决一个复杂问题，并且讨论purrr系列函数的使用风格。\n9.4节：介绍18个purrr::map()变体。\n9.5节：介绍另外一种风格的泛函——purrr::reduce()。\n9.6节：\n9.7节：介绍 base R 中的泛函。\n\n\n\n\n本章主要关注purrr包中的泛函，它们有着一致的使用风格，比较容易理解掌握。\n\nlibrary(purrr)",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#introduction",
    "href": "Books/Advanced R(2e)/9 Functionals.html#introduction",
    "title": "9 Functionals",
    "section": "",
    "text": "泛函是一种以函数作为输入，输出向量的函数。下面是一个简单的泛函函数示例：\n\nrandomise &lt;- function(f) f(runif(1e3))\nrandomise(mean)\n#&gt; [1] 0.5049301\nrandomise(mean)\n#&gt; [1] 0.4924958\nrandomise(sum)\n#&gt; [1] 498.4892\n\nbase R 中常见的apply家族函数就属于泛函，还有purrr包中的map系列函数，以及一些数学泛函——integrate(),optim()。\nbase R 中的for循环优先级：泛函 &gt; for &gt; while &gt; repeat。如果你对for循环很熟悉，转换到泛函时，只需要从for循环中的提取函数，将其作为参数传入符合要求的泛函即可。当你找不到符合要求的泛函时，首先不要硬适配某种泛函，优先使用for循环，当类似的循环逻辑被重复使用时，考虑编写自己的泛函。\n\n\n\n9.2节：介绍purrr::map()。\n9.3节：讲解如何使用多个简单的泛函组合解决一个复杂问题，并且讨论purrr系列函数的使用风格。\n9.4节：介绍18个purrr::map()变体。\n9.5节：介绍另外一种风格的泛函——purrr::reduce()。\n9.6节：\n9.7节：介绍 base R 中的泛函。\n\n\n\n\n本章主要关注purrr包中的泛函，它们有着一致的使用风格，比较容易理解掌握。\n\nlibrary(purrr)",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#my-first-functional-map",
    "href": "Books/Advanced R(2e)/9 Functionals.html#my-first-functional-map",
    "title": "9 Functionals",
    "section": "My first functional: map()",
    "text": "My first functional: map()\npurrr::map()函数接受一个‘list’或‘atomic vector’（.x）和函数（.f）作为输入，向量中的每个元素会被应用到函数中，最终返回一个list。即：map(1:3, f)等价于list(f(1), f(2), f(3))。\n\ntriple &lt;- function(x) x * 3\nmap(1:3, triple)\n#&gt; [[1]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 6\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n\n\n这里的map不是“地图”的意思，而是“映射”，意味着‘map’将输入中的向量与结果通过函数进行了映射。\n下面是map()函数的核心逻辑：创建一个与输入等长的list，for循环处理向量，并把结果赋给list的元素。\n\nsimple_map &lt;- function(x, f, ...) {\n  out &lt;- vector(\"list\", length(x))\n  for (i in seq_along(x)) {\n    out[[i]] &lt;- f(x[[i]], ...)\n  }\n  out\n}\n\n为了提高性能，map()函数其实是用C语言实现的。base R 中的lapply()函数与purrr::map()函数类似，但是lapply()函数不提供下面涉及到的额外功能。\n\nProducing atomic vectors\nmap()函数结果返回一个list，这赋予了其极大的灵活性，因为任何数据类型都可以储存在list中。但是有时返回的数据类型足够简单，我们无需再使用list储存。purrr 提供了四种特殊的变体函数——map_lgl(),map_int(),map_dbl()和map_chr()——分别返回布尔、整数、浮点数和字符向量。\n\n# map_chr() always returns a character vector\nmap_chr(mtcars, typeof)\n#&gt;      mpg      cyl     disp       hp     drat       wt     qsec       vs \n#&gt; \"double\" \"double\" \"double\" \"double\" \"double\" \"double\" \"double\" \"double\" \n#&gt;       am     gear     carb \n#&gt; \"double\" \"double\" \"double\"\n\n# map_lgl() always returns a logical vector\nmap_lgl(mtcars, is.double)\n#&gt;  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n#&gt; TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n# map_int() always returns a integer vector\nn_unique &lt;- function(x) length(unique(x))\nmap_int(mtcars, n_unique)\n#&gt;  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n#&gt;   25    3   27   22   22   29   30    2    2    3    6\n\n# map_dbl() always returns a double vector\nmap_dbl(mtcars, mean)\n#&gt;        mpg        cyl       disp         hp       drat         wt       qsec \n#&gt;  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n#&gt;         vs         am       gear       carb \n#&gt;   0.437500   0.406250   3.687500   2.812500\n\n所有map_*()系列函数要求返回的list长度与输入.x长度一致，所以.f必须返回一个值的结果，否则报错。\n\npair &lt;- function(x) c(x, x)\nmap_dbl(1:2, pair)\n#&gt; Error in `map_dbl()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error:\n#&gt; ! Result must be length 1, not 2.\n\n类似的，如果map_*()系列函数要求返回特定类型的值时，.f必须返回该类型的结果，否则报错。\n\nmap_dbl(1:2, as.character)\n#&gt; Error in `map_dbl()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error:\n#&gt; ! Can't coerce from a string to a double.\n\nmap()函数则没有上面的要求：\n\nmap(1:2, pair)\n#&gt; [[1]]\n#&gt; [1] 1 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 2 2\nmap(1:2, as.character)\n#&gt; [[1]]\n#&gt; [1] \"1\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"2\"\n\n在 base R 中，有两个函数类似map_*()系列函数，可以返回原子向量——sapply()和vapply()。作者不建议使用sapply()，因为它会对结果进行整理，导致生成不确定的或向量，或list，或矩阵的结果。相反，vapply()通过参数FUN.VALUE要求用户指定结果的数据类型。例如vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1))，其等价于map_dbl(x, mean, na.rm = TRUE)。\n\n\nAnonymous functions and shortcuts\n参数.f除提供函数名外，更多的是使用匿名函数或~ f(.x, ...)形式的公式。~ f(.x, ...)在传递其他参数时十分有用，也是最常用的方法。\n\nmap_dbl(mtcars, function(x) length(unique(x)))\n#&gt;  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n#&gt;   25    3   27   22   22   29   30    2    2    3    6\nmap_dbl(mtcars, ~ length(unique(.x)))\n#&gt;  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n#&gt;   25    3   27   22   22   29   30    2    2    3    6\n\n~ f(.x, ...)形式的背后是 rlang 支持的lambda表达式。表达式会将map*()系列函数的参数.x解析为f()的第一个参数，参数.y解析为第二个，依次类推.其他map*()系列函数未定义，但函数f()需要的参数会通过...传递。只有一个参数时可以使用.替换.x，但是不建议，.仅作为向下兼容的牺牲写法，会与其他功能的.冲突。\n\nas_mapper(~ length(unique(.x)))\n#&gt; &lt;lambda&gt;\n#&gt; function (..., .x = ..1, .y = ..2, . = ..1) \n#&gt; length(unique(.x))\n#&gt; attr(,\"class\")\n#&gt; [1] \"rlang_lambda_function\" \"function\"\n\nmap_*()系列函数可以用来批量提取数据中的某个元素（通过purrr::pluck()函数）。通过name信息，位置信息，或二者组合来提取数据，这在从JSON数据（或R对象）中提取数据时特别有用。\n\nx &lt;- list(\n  list(-1, x = 1, y = c(2), z = \"a\"),\n  list(-2, x = 4, y = c(5, 6), z = \"b\"),\n  list(-3, x = 8, y = c(9, 10, 11))\n)\n\n# Select by name\nmap_dbl(x, \"x\")\n#&gt; [1] 1 4 8\n\n# Or by position\nmap_dbl(x, 1)\n#&gt; [1] -1 -2 -3\n\n# Or by both\nmap_dbl(x, list(\"y\", 1))\n#&gt; [1] 2 5 9\n\n# You'll get an error if a component doesn't exist:\nmap_chr(x, \"z\")\n#&gt; Error in `map_chr()`:\n#&gt; ℹ In index: 3.\n#&gt; Caused by error:\n#&gt; ! Result must be length 1, not 0.\n\n# Unless you supply a .default value\nmap_chr(x, \"z\", .default = NA)\n#&gt; [1] \"a\" \"b\" NA\n\n下面是一个提取R t.test 结果对象中P值的示例：\n\ntrials &lt;- map(1:100, ~ t.test(rpois(10, 10), rpois(10, 7)))\nlibrary(ggplot2)\n\ndf_trials &lt;- tibble::tibble(p_value = map_dbl(trials, \"p.value\"))\n\ndf_trials %&gt;%\n  ggplot(aes(x = p_value, fill = p_value &lt; 0.05)) +\n  geom_dotplot(binwidth = .01) + # geom_histogram() as alternative\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\nPassing arguments with ...\n诚如上述，你可以使用...来传递参数给函数。例如na.rm = TRUE，既可以使用~ f(.x, ...)形式直接传递参数，也可以通过map函数进行传递。\n\nx &lt;- list(1:5, c(1:10, NA))\nmap_dbl(x, ~ mean(.x, na.rm = TRUE))\n#&gt; [1] 3.0 5.5\nmap_dbl(x, mean, na.rm = TRUE)\n#&gt; [1] 3.0 5.5\n\n\n注意：...传入的参数不会被map函数解析，而是直接传递。map函数的其他变体会对参数进行解析，详见9.4节。\n\n\n\n\n\n\n\nNote\n\n\n\n注意：使用~ f(.x, ...)形式直接传递参数和通过map函数进行传递，这两种方式有些许不同。前者会在每次调用函数f时都评估参数，后者只会在map函数中评估一次。当参数需要惰性评估时要特别注意，例如下面的参数是有runif()随机生成。\n\nplus &lt;- function(x, y) x + y\n\nx &lt;- c(0, 0, 0, 0)\nmap_dbl(x, plus, runif(1))\n#&gt; [1] 0.5065563 0.5065563 0.5065563 0.5065563\nmap_dbl(x, ~ plus(.x, runif(1)))\n#&gt; [1] 0.9697664 0.4930660 0.9313660 0.4334680\n\n\n\n\n\nArgument names\n当使用...传递参数时，推荐使用参数名称，而不是位置。例如，map(x, mean, trim = 0.1)要比map(x, mean, 0.1)更好。\nmap()函数的参数有两个——输入Vector，函数。考虑到尽量不与函数需要的参数名冲突，purrr 包分别使用了.x和.f作为参数名。如果使用了x,f（如最前面的simple_map()）作为参数名，那么就可能导致错误。此时只能使用匿名函数的形式来避免冲突。\n\nbootstrap_summary &lt;- function(x, f) {\n  f(sample(x, replace = TRUE))\n}\n\nsimple_map(mtcars, bootstrap_summary, f = mean)\n#&gt; Error in mean.default(x[[i]], ...): 'trim' must be numeric of length one\n\n# simple_map(mtcars, f = function(x) bootstrap_summary(x, mean))\n\nbase R 中，也有类似.x和.f的处理，如：\n\naplly 系函数使用大写字母X和FUN作为参数名。\ntransform()函数参数前使用前缀_。\n\n\n\nVarying another argument\n考虑下图中的情况：你需要将.x传递给函数的第二个参数，函数的第一个参数是固定的常量。例如你想计算固定向量在不同trim时的均值，mean(x, trim = trims)。\n\n\ntrims &lt;- c(0, 0.1, 0.2, 0.5)\nx &lt;- rcauchy(1000)\n\n有两种方法可以解决这个问题：\n\n使用匿名函数。\n\n\nmap_dbl(trims, ~ mean(x, trim = .x))\n#&gt; [1] 0.03016205 0.09579181 0.05505203 0.04010985\nmap_dbl(trims, function(trim) mean(x, trim = trim))\n#&gt; [1] 0.03016205 0.09579181 0.05505203 0.04010985\n\n\n使用mean()函数的参数自动匹配。你需要直到mean()函数的参数名称，\n\n\nmap_dbl(trims, mean, x = x)\n#&gt; [1] 0.03016205 0.09579181 0.05505203 0.04010985",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#purrr-style",
    "href": "Books/Advanced R(2e)/9 Functionals.html#purrr-style",
    "title": "9 Functionals",
    "section": "Purrr style",
    "text": "Purrr style\n在介绍其他map变体函数之前，我们先一窥使用purrr风格的函数的示例——数据分组建模，然后抽取模型系数。\n\n# 切分数据\nby_cyl &lt;- split(mtcars, mtcars$cyl)\n\n# 创建模型\nby_cyl %&gt;%\n  map(~ lm(mpg ~ wt, data = .x)) %&gt;%\n  map(coef) %&gt;%\n  map_dbl(2)\n#&gt;         4         6         8 \n#&gt; -5.647025 -2.780106 -2.192438\n\n下面使用 base R 中的函数来实现：\n\nby_cyl %&gt;%\n  lapply(function(data) lm(mpg ~ wt, data = data)) %&gt;%\n  lapply(coef) %&gt;%\n  vapply(function(x) x[[2]], double(1))\n#&gt;         4         6         8 \n#&gt; -5.647025 -2.780106 -2.192438\n\n去除管道符：\n\nmodels &lt;- lapply(by_cyl, function(data) lm(mpg ~ wt, data = data))\nvapply(models, function(x) coef(x)[[2]], double(1))\n#&gt;         4         6         8 \n#&gt; -5.647025 -2.780106 -2.192438\n\n使用for循环：\n\nslopes &lt;- double(length(by_cyl))\nfor (i in seq_along(by_cyl)) {\n  model &lt;- lm(mpg ~ wt, data = by_cyl[[i]])\n  slopes[[i]] &lt;- coef(model)[[2]]\n}\nslopes\n#&gt; [1] -5.647025 -2.780106 -2.192438\n\n有趣的是，从 purr 到 base R 中的 apply 系函数，再到 for 循环，循环函数使用的越少，循环一次做的事情就越多。",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#map-variants",
    "href": "Books/Advanced R(2e)/9 Functionals.html#map-variants",
    "title": "9 Functionals",
    "section": "Map variants",
    "text": "Map variants\nmap系函数主要有23种，从输入与输出两个维度，可以大致划分为下面的表格：\n\n\n\n输入\\输出\nList\nAtomic\nSame type\nNothing\n\n\n\n\nOne argument\nmap()\nmap_lgl(), …\nmodify()\nwalk()\n\n\nTwo arguments\nmap2()\nmap2_lgl(), …\nmodify2()\nwalk2()\n\n\nOne argument + index\nimap()\nimap_lgl(), …\nimodify()\niwalk()\n\n\nN arguments\npmap()\npmap_lgl(), …\n—\npwalk()\n\n\n\n从表格中我们可以看到，除了上面介绍过的map()和map_lgl(), map_int(), map_dbl(), map_chr()，剩余函数可以再划分为5大类：\n\nmap2()：支持迭代两个输入。\nimap()：支持迭代两个输入，其中一个是另外一个的索引。\npmap()：支持迭代多个输入。\nmodify()：输出类型与输入相同。\nwalk()：不返回输出。\n\n\nSame type of output as input: modify()\nmodify()函数最常见的使用场景就是修改数据框。map()虽然也可以修改数据框中的数据，但其结果是一个list，而modify()函数返回一个数据框。\n\ndf &lt;- data.frame(\n  x = 1:3,\n  y = 6:4\n)\n\nmap(df, ~ .x * 2)\n#&gt; $x\n#&gt; [1] 2 4 6\n#&gt; \n#&gt; $y\n#&gt; [1] 12 10  8\nmodify(df, ~ .x * 2)\n#&gt;   x  y\n#&gt; 1 2 12\n#&gt; 2 4 10\n#&gt; 3 6  8\n\nmodify()函数的本质可以用下面的函数表示：\n\nsimple_modify &lt;- function(x, f, ...) {\n  for (i in seq_along(x)) {\n    x[[i]] &lt;- f(x[[i]], ...)\n  }\n  x\n}\n\n\n\ntwo inputs: map2() and friends\n正如9.2节中提到，当你需要迭代两个参数时，map()函数无法满足，需要使用map2()函数。map2()函数对参数的迭代示意图如下：\n\n假设现在需要计算加权平均值，需要同时迭代xs,ws。map_dbl(xs, weighted.mean, w = ws)会报错，map2_dbl(xs, ws, weighted.mean)才是正确用法。\n\nxs &lt;- map(1:8, ~ runif(10))\nxs[[1]][[1]] &lt;- NA\nws &lt;- map(1:8, ~ rpois(10, 5) + 1)\n\nmap_dbl(xs, weighted.mean, w = ws)\n#&gt; Error in `map_dbl()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error in `weighted.mean.default()`:\n#&gt; ! 'x' and 'w' must have the same length\nmap2_dbl(xs, ws, weighted.mean)\n#&gt; [1]        NA 0.4606237 0.5007349 0.5202646 0.5972615 0.6376652 0.6391361\n#&gt; [8] 0.5283167\n\n传递额外参数na.rm = TRUE的方式与map()一样。\n\nmap2_dbl(xs, ws, weighted.mean, na.rm = TRUE)\n#&gt; [1] 0.4510721 0.4606237 0.5007349 0.5202646 0.5972615 0.6376652 0.6391361\n#&gt; [8] 0.5283167\n\n\nmap2()函数的本质可以用下面的函数表示：\n\nsimple_map2 &lt;- function(x, y, f, ...) {\n  out &lt;- vector(\"list\", length(x))\n  for (i in seq_along(x)) {\n    out[[i]] &lt;- f(x[[i]], y[[i]], ...)\n  }\n  out\n}\n\nmap2()函数与simple_map2()函数略微不同的地方是，当.x与.y长度不一致时，map2()会自动将短的向量重复补齐。\n\n\n\nNo outputs: walk() and friends\nmap()函数可以存储数据并输出，但有时我们并不需要返回结果，此时可以使用walk()函数。下面是循环打印信息的例子，map()函数返回的是一个都是NULL的list，walk()函数则不返回任何对象。\n\nwelcome &lt;- function(x) {\n  cat(\"Welcome \", x, \"!\\n\", sep = \"\")\n}\nnames &lt;- c(\"Hadley\", \"Jenny\")\n\n# As well as generate the welcomes, it also shows\n# the return value of cat()\nmap(names, welcome)\n#&gt; Welcome Hadley!\n#&gt; Welcome Jenny!\n#&gt; [[1]]\n#&gt; NULL\n#&gt; \n#&gt; [[2]]\n#&gt; NULL\n\nwalk(names, welcome)\n#&gt; Welcome Hadley!\n#&gt; Welcome Jenny!\n\nwalk2()函数通常用来将数据写入到磁盘。\n\ntemp &lt;- tempfile()\ndir.create(temp)\n\ncyls &lt;- split(mtcars, mtcars$cyl)\npaths &lt;- file.path(temp, paste0(\"cyl-\", names(cyls), \".csv\"))\nwalk2(cyls, paths, write.csv)\n\ndir(temp)\n#&gt; [1] \"cyl-4.csv\" \"cyl-6.csv\" \"cyl-8.csv\"\n\n\n\nIterating over values and indices: imap()\nbase R 中对向量进行循环时，有三种类型：\n\n迭代元素：for (x in xs)\n迭代元素位置索引：for (i in seq_along(xs))\n迭代元素名称索引：for (nm in names(xs))\n\nmap()函数使用的是第一种，imap()函数使用第二种和第三种。imap(x, f)本质上等价于map2(x, names(x), f)或者map2(x, seq_along(x), f)。\n\nimap_chr(iris, ~ paste0(\"The first value of \", .y, \" is \", .x[[1]]))\n#&gt;                             Sepal.Length \n#&gt; \"The first value of Sepal.Length is 5.1\" \n#&gt;                              Sepal.Width \n#&gt;  \"The first value of Sepal.Width is 3.5\" \n#&gt;                             Petal.Length \n#&gt; \"The first value of Petal.Length is 1.4\" \n#&gt;                              Petal.Width \n#&gt;  \"The first value of Petal.Width is 0.2\" \n#&gt;                                  Species \n#&gt;   \"The first value of Species is setosa\"\n\n\nx &lt;- map(1:6, ~ sample(1000, 10))\nimap_chr(x, ~ paste0(\"The highest value of \", .y, \" is \", max(.x)))\n#&gt; [1] \"The highest value of 1 is 711\" \"The highest value of 2 is 923\"\n#&gt; [3] \"The highest value of 3 is 876\" \"The highest value of 4 is 932\"\n#&gt; [5] \"The highest value of 5 is 944\" \"The highest value of 6 is 993\"\n\n\n\nAny number of inputs: pmap() and friends\npmap()函数的参数有两个:\n\n.l：参数列表，要求列表中每个元素长度相同，可以看作是一个数据框(data.frame)。\n.f：函数。\n\n这里要对map()函数中的参数.x = list时作个区分，map()中列表作为整体是一个参数，而pmap()中列表中的每个元素是一个参数。\n\nparams &lt;- tibble::tribble(\n  ~n, ~min, ~max,\n  1L, 0, 1,\n  2L, 10, 100,\n  3L, 100, 1000\n)\n\npmap(params, runif)\n#&gt; [[1]]\n#&gt; [1] 0.4314635\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 41.53247 22.65329\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 761.8707 221.1815 781.8033\n\n\nmap2(x, y, f) 可以等价为pmap(list(x, y), f)。\n\n\n\n\n\n\n\n\n\n\n\npmap_dbl(list(xs, ws), weighted.mean)\n#&gt; [1]        NA 0.4606237 0.5007349 0.5202646 0.5972615 0.6376652 0.6391361\n#&gt; [8] 0.5283167\n\npmap_dbl(list(xs, ws), weighted.mean, na.rm = TRUE)\n#&gt; [1] 0.4510721 0.4606237 0.5007349 0.5202646 0.5972615 0.6376652 0.6391361\n#&gt; [8] 0.5283167\n\n9.2.5小节中的情况也可以用pmap()解决。\n\ntrims &lt;- c(0, 0.1, 0.2, 0.5)\nx &lt;- rcauchy(1000)\n\npmap_dbl(list(trim = trims), mean, x = x)\n#&gt; [1] 0.38134836 0.15561469 0.13479472 0.06358648\n\nbase R 有两个等价的函数：Map()和mapply()，但是它们都有显著的缺陷。\n\nMap()：你无法提供额外的参数到函数中。\nmapply()：与sapply()一样，无法保证结果的一致性。",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#reduce-family",
    "href": "Books/Advanced R(2e)/9 Functionals.html#reduce-family",
    "title": "9 Functionals",
    "section": "Reduce family",
    "text": "Reduce family\nreduce系函数比较小众，不仅只有两种变体，还不常见。但它却为代数或处理大型数据提供了一种强有力的解决方法。\n\nBasics\nreduce()函数只接受两个参数：.x和.f。参数.f与map()不同，它要求初始输入.x中的两个元素，返回一个与输入类型相同的结果，然后将本次.f的结果作为下一次调用的输入。即reduce(1:4, f)等价于f(f(f(1,2),3),4)。\n\nreduce()函数的本质可以用下面的函数表示：\n\nsimple_reduce &lt;- function(x, f) {\n  out &lt;- x[[1]]\n  for (i in seq(2, length(x))) {\n    out &lt;- f(out, x[[i]])\n  }\n  out\n}\n\n举一个简单的例子：存在一个全是数字类型的列表，你想找到列表中每个元素都包含的数字。\n\n# 生成一个列表\nset.seed(123)\nl &lt;- map(1:4, ~ sample(1:10, 15, replace = T))\nstr(l)\n#&gt; List of 4\n#&gt;  $ : int [1:15] 3 3 10 2 6 5 4 6 9 10 ...\n#&gt;  $ : int [1:15] 3 8 10 7 10 9 3 4 1 7 ...\n#&gt;  $ : int [1:15] 10 7 5 7 5 6 9 2 5 8 ...\n#&gt;  $ : int [1:15] 5 9 10 4 6 8 6 6 7 1 ...\n\n# 使用intersect，找出列表中每个元素都包含的数字\nout &lt;- l[[1]]\nout &lt;- intersect(out, l[[2]])\nout &lt;- intersect(out, l[[3]])\nout &lt;- intersect(out, l[[4]])\nout\n#&gt; [1] 10  5  9\n\n使用reduce()函数，可以优雅地实现：\n\nreduce(l, intersect)\n#&gt; [1] 10  5  9\n\nreduce()函数通用支持传入额外参数到.f函数中：\n\n\n\nAccumulate\naccumulate()函数是reduce()函数的变体，它可以很清除地帮助我们看到reduce()是如何工作地。因为accumulate()函数会返回一个列表，列表中每个元素是.f函数的输出。\n\naccumulate(l, intersect)\n#&gt; [[1]]\n#&gt;  [1]  3  3 10  2  6  5  4  6  9 10  5  3  9  9  9\n#&gt; \n#&gt; [[2]]\n#&gt; [1]  3 10  5  4  9\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 10  5  9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 10  5  9\n\n另外一个理解 reduce 思想地例子是sum()函数：sum(x)可以等价为reduce(x,+)。accumulate(x,+)就等同于累加：\n\nx &lt;- c(4, 3, 10)\nreduce(x, `+`)\n#&gt; [1] 17\n\naccumulate(x, `+`)\n#&gt; [1]  4  7 17\n\n\n\n.init\n在有.init参数时，reduce()的处理逻辑如下：\n\n使用.init参数有两种作用：\n\n当reduce()的输入.x长度为1且没有提供参数.init，那么reduce()会直接返回.x。此时无法对输入类型进行判断，提供了.init参数后，相当于对输入的数据类型做出了规定。\n\n\nreduce(1, `+`)\n#&gt; [1] 1\nreduce(\"a\", `+`)\n#&gt; [1] \"a\"\nreduce(\"a\", `+`, .init = 0)\n#&gt; Error in .x + .y: non-numeric argument to binary operator\n\n\n当输入reduce()的.x长度为0且没有提供参数.init，那么reduce()会直接报错，反之不会。\n\n\nreduce(integer(), `+`)\n#&gt; Error in `reduce()`:\n#&gt; ! Must supply `.init` when `.x` is empty.\nreduce(integer(), `+`, .init = 0)\n#&gt; [1] 0\n\n使用reduce()函数一定要考虑到.x数据的长度，和.f函数的返回值类型。\n\n\nMultiple inputs\n极少数情况下，你需要向reduce()函数传递两个参数。例如，当你想将多个数据框进行join，但用于连接的变量.by因元素而异。此时你可以使用reduce2()。\nreduce2()的.y参数的长度取决于是否提供.init参数：若.x有4个元素，.f将只被调用3次，.y参数的长度是3，若同时提供.init，.f将被调用4次，.y参数的长度是4。\n\n\n\n\nMap-reduce\n你可能听说过 map-reduce, 这是一个为 Hadoop 等技术提供动力的概念。现在你可以看到这个基本概念是多么简单和强大：map-reduce 是一个结合了 reduce 的 map。对于大数据来说，区别在于数据分布在多台计算机上。每台计算机对其拥有的数据执行 map, 然后将结果发送回协调器，该协调器将各个结果 reduce 为单个结果。\n作为一个简单的例子，想象计算一个非常大的向量的平均值，这个向量如此之大，以至于必须分配给多台计算机。你可以让每台计算机计算总和和长度，然后将这些数据返回给协调器，由协调器通过将总和除以总长度来计算总平均值。",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#predicate-functionals",
    "href": "Books/Advanced R(2e)/9 Functionals.html#predicate-functionals",
    "title": "9 Functionals",
    "section": "Predicate functionals",
    "text": "Predicate functionals\npredicate 函数，也叫判断函数，返回值是单一的逻辑值——TRUE或FALSE，例如is.character(),is.null(),all(),any()。\n\n\n\n\n\n\nNote\n\n\n\nis.na()不符合判断函数的标准，它客户返回一个向量。anyNA()是判断函数，base R 不提供allNA()。\n\n\n\nBasics\npredicate functionals 就是将判断函数应用到.x中的每个元素，并返回一个逻辑向量。purrr包提过了7个有用的函数，可以分成3组：\n\nsome(.x, .p)：如果有任何元素匹配，返回TRUE，类似any(map_lgl(.x, .p))。\nevery(.x, .p)：如果所有元素匹配，返回TRUE，类似all(map_lgl(.x, .p))。\nnone(.x, .p) ：如果所有元素不匹配，返回TRUE，类似all(map_lgl(.x, negate(.p)))。\n虽然any(map_lgl(.x, .p)) 与some()类似，但前者需要将所有元素都判断后再运行any ,后者只要有一个TRUE 就返回。\ndetect(.x, .p)：返回.x中第一个匹配的元素。\ndetect_index(.x, .p)：返回.x中第一个匹配的元素的索引。\n\nkeep(.x, .p)：保留所有匹配的元素。\ndiscard(.x, .p)：丢弃所有匹配的元素。\n\n\ndf &lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\ndetect(df, is.factor)\n#&gt; NULL\ndetect_index(df, is.factor)\n#&gt; [1] 0\n\nstr(keep(df, is.factor))\n#&gt; 'data.frame':    3 obs. of  0 variables\nstr(discard(df, is.factor))\n#&gt; 'data.frame':    3 obs. of  2 variables:\n#&gt;  $ x: int  1 2 3\n#&gt;  $ y: chr  \"a\" \"b\" \"c\"\n\n\n\nMap variants\nmap()和modify()的有接受一个判断函数的变体——判断函数用来过滤.x。\n\ndf &lt;- data.frame(\n  num1 = c(0, 10, 20),\n  num2 = c(5, 6, 7),\n  chr1 = c(\"a\", \"b\", \"c\"),\n  stringsAsFactors = FALSE\n)\n\nstr(map_if(df, is.numeric, mean))\n#&gt; List of 3\n#&gt;  $ num1: num 10\n#&gt;  $ num2: num 6\n#&gt;  $ chr1: chr [1:3] \"a\" \"b\" \"c\"\nstr(modify_if(df, is.numeric, mean))\n#&gt; 'data.frame':    3 obs. of  3 variables:\n#&gt;  $ num1: num  10 10 10\n#&gt;  $ num2: num  6 6 6\n#&gt;  $ chr1: chr  \"a\" \"b\" \"c\"\nstr(map(keep(df, is.numeric), mean))\n#&gt; List of 2\n#&gt;  $ num1: num 10\n#&gt;  $ num2: num 6",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/9 Functionals.html#base-functionals",
    "href": "Books/Advanced R(2e)/9 Functionals.html#base-functionals",
    "title": "9 Functionals",
    "section": "Base functionals",
    "text": "Base functionals\n\nMatrices and arrays\nbase R 中的apply()函数存在一些使用上的陷阱，它更多的是用于矩阵的数值计算上。\n\n与sapply()一样，它无法控制返回值的数据类型，向量？矩阵？列表？\napply()函数不是幂等的，这是因为如果汇总函数是标识运算符，则输出并不总是与输入相同。\n\na2d &lt;- matrix(1:20, nrow = 5)\na1 &lt;- apply(a2d, 1, identity)\nidentical(a2d, a1)\n#&gt; [1] FALSE\n\na2 &lt;- apply(a2d, 2, identity)\nidentical(a2d, a2)\n#&gt; [1] TRUE\n\napply()函数不适合处理数据框。\n\ndf &lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\napply(df, 2, mean)\n#&gt; Warning in mean.default(newX[, i], ...): argument is not numeric or logical:\n#&gt; returning NA\n#&gt; Warning in mean.default(newX[, i], ...): argument is not numeric or logical:\n#&gt; returning NA\n#&gt;  x  y \n#&gt; NA NA\n\n\n\n\nMathematical concerns\n泛函再数学中广泛存在。泛函的计算方式与迭代密切相关。下面是一些 base R 中的计算函数。\n\nintegrate()：计算函数曲线下的面积。\nuniroot()：求函数f(x)=0的根。\noptimise()：求函数的最大或最小值。\n\n\nintegrate(sin, 0, pi)\n#&gt; 2 with absolute error &lt; 2.2e-14\nstr(uniroot(sin, pi * c(1 / 2, 3 / 2)))\n#&gt; List of 5\n#&gt;  $ root      : num 3.14\n#&gt;  $ f.root    : num 1.22e-16\n#&gt;  $ iter      : int 2\n#&gt;  $ init.it   : int NA\n#&gt;  $ estim.prec: num 6.1e-05\nstr(optimise(sin, c(0, 2 * pi)))\n#&gt; List of 2\n#&gt;  $ minimum  : num 4.71\n#&gt;  $ objective: num -1\nstr(optimise(sin, c(0, pi), maximum = TRUE))\n#&gt; List of 2\n#&gt;  $ maximum  : num 1.57\n#&gt;  $ objective: num 1",
    "crumbs": [
      "9 Functionals"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html",
    "href": "Books/Advanced R(2e)/7 Environments.html",
    "title": "7 Environments",
    "section": "",
    "text": "环境(enviroment, Env)是赋予作用域(scoping)能力的数据结构。本章会详细地介绍环境的数据结构，来提高对于作用域的理解。\n\n\n\n7.2节介绍环境的基本属性及如何创建一个自己的环境。\n7.3节通过一个模板函数，介绍与环境进行交互的方法。\n7.4节介绍4种特殊环境：R包环境，函数环境，函数执行换行，命名空间。\n7.5节介绍调用环境（caller）。\n7.6节简单讨论如何使用环境这一数据结构来解决一些特定问题。\n\n\n\n\n本章将使用 rlang 中的函数来处理环境。\n\nlibrary(rlang)\n\nrlang中的env_*()函数被设计用来在工作流中使用。所有的函数都接收一个环境作为参数，大多数会返回一个环境。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#introduction",
    "href": "Books/Advanced R(2e)/7 Environments.html#introduction",
    "title": "7 Environments",
    "section": "",
    "text": "环境(enviroment, Env)是赋予作用域(scoping)能力的数据结构。本章会详细地介绍环境的数据结构，来提高对于作用域的理解。\n\n\n\n7.2节介绍环境的基本属性及如何创建一个自己的环境。\n7.3节通过一个模板函数，介绍与环境进行交互的方法。\n7.4节介绍4种特殊环境：R包环境，函数环境，函数执行换行，命名空间。\n7.5节介绍调用环境（caller）。\n7.6节简单讨论如何使用环境这一数据结构来解决一些特定问题。\n\n\n\n\n本章将使用 rlang 中的函数来处理环境。\n\nlibrary(rlang)\n\nrlang中的env_*()函数被设计用来在工作流中使用。所有的函数都接收一个环境作为参数，大多数会返回一个环境。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#environment-basics",
    "href": "Books/Advanced R(2e)/7 Environments.html#environment-basics",
    "title": "7 Environments",
    "section": "Environment basics",
    "text": "Environment basics\n环境与有name属性的list很相似，但是有4点差异：\n\n要求name唯一。\nname不会被排序，即无法使用顺序来提取环境中的变量。\n赋值NULL时，环境会更改变量值，而不是删除变量。\n被修改时不会在内存中复制。\n\n\nBasics\n使用rlang::env()创建一个环境，与list类似，接收一个键值对集合。base::new.env()函数也可以创建环境，但是不能直接传递键值对集合，需要使用$&lt;-赋值。\n\ne1 &lt;- env(\n  a = FALSE,\n  b = \"a\",\n  c = 2.3,\n  d = 1:3,\n)\n\ne2 &lt;- new.env()\ne2$a &lt;- FALSE\ne2$b &lt;- \"a\"\ne2$c &lt;- 2.3\ne2$d &lt;- 1:3\n\nidentical(e1, e2)\n#&gt; [1] FALSE\n\n\n环境具有引用语义：与大多数R对象不同，当你修改环境时，你是在原地修改它们，而不是创建副本。一个重要的含义是，环境可以自我包含。\n\ne1$d &lt;- e1\n\n\n在终端中直接运行环境仅会显示内存地址，使用env_print()可以打印更多环境信息，也可以使用env_names()直接输出目前环境中绑定过的变量名。\n\ne1\n#&gt; &lt;environment: 0x0000023626de1260&gt;\nenv_print(e1)\n#&gt; &lt;environment: 0x0000023626de1260&gt;\n#&gt; Parent: &lt;environment: global&gt;\n#&gt; Bindings:\n#&gt; • a: &lt;lgl&gt;\n#&gt; • b: &lt;chr&gt;\n#&gt; • c: &lt;dbl&gt;\n#&gt; • d: &lt;env&gt;\nenv_names(e1)\n#&gt; [1] \"a\" \"b\" \"c\" \"d\"\n\n\n\nImportant environments\n这里介绍两个特殊环境，更多特殊的环境会在之后详细讲到：\n\n全局环境：就是当前交互终端的环境，global_env()或globalenv()获取。\n当前环境：当前执行代码的环境，current_env()或environment()获取，如果在终端中执行，那么等于全局环境。\n\n使用identical()判断两个环境是否相等，不能使用==，因为环境不是atomic或list类型。\n\nidentical(global_env(), current_env())\n#&gt; [1] TRUE\n\nglobal_env() == current_env()\n#&gt; Error in global_env() == current_env(): comparison (==) is possible only for atomic and list types\n\n\n\nParents\n使用env()创建环境时，提供一个没有name的参数即可设定环境的父环境。使用env_parent()或parent.env可以查看父环境。\n\ne2a &lt;- env(d = 4, e = 5)\ne2b &lt;- env(e2a, a = 1, b = 2, c = 3)\nenv_parent(e2b)\n#&gt; &lt;environment: 0x000002363364e698&gt;\nparent.env(e2b)\n#&gt; &lt;environment: 0x000002363364e698&gt;\nenv_parent(e2a)\n#&gt; &lt;environment: R_GlobalEnv&gt;\nparent.env(e2a)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n\n每个环境都有父环境，除了空环境（empty environment）。使用env_parents()可以查看所有父环境，空环境是根环境。\n\ne2c &lt;- env(empty_env(), d = 4, e = 5)\ne2d &lt;- env(e2c, a = 1, b = 2, c = 3)\nenv_parents(e2b)\n#&gt; [[1]]   &lt;env: 0x000002363364e698&gt;\n#&gt; [[2]] $ &lt;env: global&gt;\nenv_parents(e2d)\n#&gt; [[1]]   &lt;env: 0x0000023633b945c0&gt;\n#&gt; [[2]] $ &lt;env: empty&gt;\n\n\nenv_parents()通常只返回到全局环境。设置last = empty_env()可以返回所有父环境。\n\nenv_parents(e2b, last = empty_env())\n#&gt;  [[1]]   &lt;env: 0x000002363364e698&gt;\n#&gt;  [[2]] $ &lt;env: global&gt;\n#&gt;  [[3]] $ &lt;env: package:rlang&gt;\n#&gt;  [[4]] $ &lt;env: package:stats&gt;\n#&gt;  [[5]] $ &lt;env: package:graphics&gt;\n#&gt;  [[6]] $ &lt;env: package:grDevices&gt;\n#&gt;  [[7]] $ &lt;env: package:datasets&gt;\n#&gt;  [[8]] $ &lt;env: renv:shims&gt;\n#&gt;  [[9]] $ &lt;env: package:utils&gt;\n#&gt; [[10]] $ &lt;env: package:methods&gt;\n#&gt; [[11]] $ &lt;env: Autoloads&gt;\n#&gt; [[12]] $ &lt;env: package:base&gt;\n#&gt; [[13]] $ &lt;env: empty&gt;\n\n\n\nSuper assignment, &lt;&lt;-\n常规赋值&lt;-会在当前环境中创建变量。超赋值&lt;&lt;-从不会在当前环境中创建变量，只是会修改变量，当前环境中没有就依次在父环境中搜索，直到全局环境。如果全局环境中也没有这个变量，就会在全局环境中创建这个变量。\n\nx &lt;- 0\nf &lt;- function() {\n  x &lt;&lt;- 1\n}\nf()\nx\n#&gt; [1] 1\n\n\n\nGetting and setting\n获取环境中的变量方法有$，[[，env_get()。\n\ne3 &lt;- env(x = 1, y = 2)\ne3$x\n#&gt; [1] 1\ne3$z &lt;- 3\ne3[[\"z\"]]\n#&gt; [1] 3\nenv_get(e3, \"z\")\n#&gt; [1] 3\n\n需要注意的是：[[不适用于数字索引，也不可以使用[。\n\ne3[[1]]\n#&gt; Error in e3[[1]]: wrong arguments for subsetting an environment\n\ne3[c(\"x\", \"y\")]\n#&gt; Error in e3[c(\"x\", \"y\")]: object of type 'environment' is not subsettable\n\n变量不存在时，$，[[会返回NULL，但NULL在环境中有实际意义。 env_get()会返回错误，env_get()设置参数default后，可以给定默认值而不报错。\n\ne3$xyz\n#&gt; NULL\n\nenv_get(e3, \"xyz\")\n#&gt; Error in `env_get()`:\n#&gt; ! Can't find `xyz` in environment.\n\nenv_get(e3, \"xyz\", default = NA)\n#&gt; [1] NA\n\nrlang包还提供两种额外的设定变量值的方法：\n\nenv_poke()：只设置一个变量。\nenv_bind()：设置多个变量。\n\n\nenv_poke(e3, \"a\", 100)\ne3$a\n#&gt; [1] 100\n\nenv_bind(e3, a = 10, b = 20)\nenv_names(e3)\n#&gt; [1] \"x\" \"y\" \"z\" \"a\" \"b\"\n\n与list不同，当设置变量值为NULL时，并不会移除这个变量，而是实际有一个变量指向了NULL。env_has()可以用来检测环境中是否存在某个变量。env_unbind()会真实地解绑一个变量。\n\ne3$a &lt;- NULL\nenv_has(e3, \"a\")\n#&gt;    a \n#&gt; TRUE\n\nenv_unbind(e3, \"a\")\nenv_has(e3, \"a\")\n#&gt;     a \n#&gt; FALSE\n\n需要注意地是：env_unbind()不会删除变量，只是解绑变量与值的关系，删除变量是gc()的任务。在R base中存在功能与上述类似的函数：get()，assign()，exists()，rm()，这些函数被设计用来在当前环境中工作，其他环境中会略显不足。\n\n\nAdvanced bindings\nenv_bind()函数有两个变体：\n\nenv_bind_lazy()可以创建延迟绑定。在首次绑定前会先运行导致延迟的代码，然后再绑定。延迟绑定主要应用于R包中的autoload()，预先将数据集加载到内存中。\n\n\nenv_bind_lazy(current_env(), b = {\n  Sys.sleep(1)\n  1\n})\n\nsystem.time(print(b))\n#&gt; [1] 1\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    1.02\nsystem.time(print(b))\n#&gt; [1] 1\n#&gt;    user  system elapsed \n#&gt;       0       0       0\n\n\nenv_bind_active()可以创建实时绑定，每次重新绑定值。\n\n\nenv_bind_active(current_env(), z1 = function(val) runif(1))\n\nz1\n#&gt; [1] 0.08075014\nz1\n#&gt; [1] 0.834333\n\n更多见?delayedAssign()和?makeActiveBinding()。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#recursing-over-environments",
    "href": "Books/Advanced R(2e)/7 Environments.html#recursing-over-environments",
    "title": "7 Environments",
    "section": "Recursing over environments",
    "text": "Recursing over environments\n由于每个环境都会有一个父环境，可以利用这一特点，递归遍历环境，执行某些操作。下面是一个用来递归找到变量函数——where()的实现。\n\nwhere &lt;- function(name, env = caller_env()) {\n  if (identical(env, empty_env())) {\n    # Base case\n    stop(\"Can't find \", name, call. = FALSE)\n  } else if (env_has(env, name)) {\n    # Success case\n    env\n  } else {\n    # Recursive case\n    where(name, env_parent(env))\n  }\n}\n\n\n首先判断是否是empty_env()，如果是则返回“找不到变量”，如果不是\n则判断当前环境是否包含这个变量，如果有则返回当前环境，如果没有\n则递归查询父环境。\n\n\nwhere(\"yyy\")\n#&gt; Error: Can't find yyy\n\nx &lt;- 5\nwhere(\"x\")\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\nwhere(\"mean\")\n#&gt; &lt;environment: base&gt;\n\n用一幅图来展示上面的逻辑，想象下面两个环境：\n\ne4a &lt;- env(empty_env(), a = 1, b = 2)\ne4a\n#&gt; &lt;environment: 0x000002362eb27a00&gt;\ne4b &lt;- env(e4a, x = 10, a = 11)\ne4b\n#&gt; &lt;environment: 0x000002362bb59748&gt;\n\n\n\nwhere(\"a\", e4b)的结果是e4b。\nwhere(\"b\", e4b)的结果是e4a。\nwhere(\"c\", e4b)的结果是error。\n\n\nwhere(\"a\", e4b)\n#&gt; &lt;environment: 0x000002362bb59748&gt;\nwhere(\"b\", e4b)\n#&gt; &lt;environment: 0x000002362eb27a00&gt;\nwhere(\"c\", e4b)\n#&gt; Error: Can't find c\n\n总结这种递归查询环境的逻辑如下：\nf &lt;- function(..., env = caller_env()) {\n  if (identical(env, empty_env())) {\n    # base case\n  } else if (success) {\n    # success case\n  } else {\n    # recursive case\n    f(..., env = env_parent(env))\n  }\n}\n\nIteration versus recursion\n使用while循环改写上面的函数：\nf2 &lt;- function(..., env = caller_env()) {\n  while (!identical(env, empty_env())) {\n    if (success) {\n      # success case\n      return()\n    }\n    # inspect parent\n    env &lt;- env_parent(env)\n  }\n\n  # base case\n}",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#special-environments",
    "href": "Books/Advanced R(2e)/7 Environments.html#special-environments",
    "title": "7 Environments",
    "section": "Special environments",
    "text": "Special environments\n许多环境是由R自行创建，例如：包环境，函数环境等，本节介绍四种特殊的环境——包环境，函数环境，函数运行环境，命名空间。\n\nPackage environments and the search path\n每次使用library()或require()时，都会将包环境变成全局环境的父环境，最先加载的包环境是后加载包环境的父环境。\n\n\n这些环境的关系也被称作搜索路径。你可以使用base::search()或rlang::search_envs()查看搜索路径。搜索路径的最后两个环境是固定的——Autoloads和base。\n\nsearch()\n#&gt;  [1] \".GlobalEnv\"        \"package:rlang\"     \"package:stats\"    \n#&gt;  [4] \"package:graphics\"  \"package:grDevices\" \"package:datasets\" \n#&gt;  [7] \"renv:shims\"        \"package:utils\"     \"package:methods\"  \n#&gt; [10] \"Autoloads\"         \"package:base\"\n\nsearch_envs()\n#&gt;  [[1]] $ &lt;env: global&gt;\n#&gt;  [[2]] $ &lt;env: package:rlang&gt;\n#&gt;  [[3]] $ &lt;env: package:stats&gt;\n#&gt;  [[4]] $ &lt;env: package:graphics&gt;\n#&gt;  [[5]] $ &lt;env: package:grDevices&gt;\n#&gt;  [[6]] $ &lt;env: package:datasets&gt;\n#&gt;  [[7]] $ &lt;env: renv:shims&gt;\n#&gt;  [[8]] $ &lt;env: package:utils&gt;\n#&gt;  [[9]] $ &lt;env: package:methods&gt;\n#&gt; [[10]] $ &lt;env: Autoloads&gt;\n#&gt; [[11]] $ &lt;env: package:base&gt;\n\n\n\nThe function environment\n函数在被创建时，会自动绑定当前环境，这个环境被称作函数环境。函数与函数环境一起构成了“闭包”。\n使用rlang::fn_env()或base::environment()可以查看函数环境。\n\ny &lt;- 1\nf &lt;- function(x) x + y\nfn_env(f)\n#&gt; &lt;environment: R_GlobalEnv&gt;\nenvironment(f)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n\n上面的例子中，函数f()的函数环境是当前环境，其绑定的变量f在当前环境中。但实际上，这两种情况的环境并不总是相同。例如下面的例子，函数g()的函数环境是当前环境，但其绑定的变量g在环境e中。这两种的区别在于，前者定义了函数g如何寻找参数变量，后者定义了如何寻找函数g。（将e环境视作包环境，是不是更容易理解？）\n\ne &lt;- env()\ne$g &lt;- function(x) x\n\n\n\n\nExecution environments\n运行下面的函数，第一次，第二次，第n次的结果会怎样？\n\ng &lt;- function(x) {\n  if (!env_has(current_env(), \"a\")) {\n    message(\"Defining a\")\n    a &lt;- 1\n  } else {\n    a &lt;- a + 1\n  }\n  a\n}\n\n\ng(10)\n#&gt; [1] 1\ng(10)\n#&gt; [1] 1\n\n结果如上所示，每次的运行结果都相同。这是因为函数的执行环境在每次运行结束后，都会被清除，然后重新创建一个。它的父环境是函数环境。\n下面是一个各个环境之间关系的示意图：上面灰色方框表示执行环境，淡黄色框表示函数，右侧灰色框表示函数环境。\n\nh &lt;- function(x) {\n  # 1.\n  a &lt;- 2 # 2.\n  x + a\n}\ny &lt;- h(1) # 3.\n\n\n有些方法可以将执行环境保存下来。\n第一种就是直接返回：\n\nh2 &lt;- function(x) {\n  a &lt;- x * 2\n  current_env()\n}\n\ne &lt;- h2(x = 10)\nenv_print(e)\n#&gt; &lt;environment: 0x0000023632b02cc0&gt;\n#&gt; Parent: &lt;environment: global&gt;\n#&gt; Bindings:\n#&gt; • a: &lt;dbl&gt;\n#&gt; • x: &lt;dbl&gt;\n\n另一种是将执行环境绑定到某个对象上，例如函数，成为函数环境：\n\nplus &lt;- function(x) {\n  function(y) x + y\n}\n\nplus_one &lt;- plus(1)\nplus_one\n#&gt; function (y) \n#&gt; x + y\n#&gt; &lt;environment: 0x00000236313280b8&gt;\n\n\n\nplus_one(2)\n#&gt; [1] 3\n\n\n\n\nNamespaces\n命名空间规定了R包中的函数如何正确找到自己引用的函数。而不会因为前面加载的R包导致引用错误。\n例如下面的sd()函数：通过命名空间namespace指定var()函数来自于stats包。\n\nsd\n#&gt; function (x, na.rm = FALSE) \n#&gt; sqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n#&gt;     na.rm = na.rm))\n#&gt; &lt;bytecode: 0x0000023630c7eeb0&gt;\n#&gt; &lt;environment: namespace:stats&gt;\n\nR包中的函数都绑定一对环境——R包环境与命名空间。\n\nR包环境：针对使用者（user），告诉使用者如何引用函数。\n命名空间：针对R包自己，告诉R包中的函数如何引用其他函数。\n\n命名空间中的函数名集合包含R包环境中的，那些存在命名空间，但不存在R包环境中的函数，就是R包未导出的函数。\n\n前面说过，每个环境都有一个父环境。同样，每个命名空间环境都有一套相同的父环境：\n\n都有一个imports环境，定义了所有被R包使用的函数。R包开发者可以通过NAMESPACE文件来定义这个环境。\nimports环境的父环境是base包的命名空间。\nbase包的命名空间的父环境是全局环境R_GlobalEnv。\n\n\n最终父环境是全局环境，这一规则由于历史原因存在。按道理将不应该存在这一规则，因为这会导致在命名空间中不存在某个函数时，R会自动搜索全局环境。鉴于此，R CMD check会检查这种行为，并警告。\n将上述所有环境整合到一起，得到：\n\n函数和命名空间环境之间的绑定，是在加载R包时，因为创建了函数，触发创建函数环境导致的。也就是说，R包中函数的函数环境就是命名空间。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#call-stacks",
    "href": "Books/Advanced R(2e)/7 Environments.html#call-stacks",
    "title": "7 Environments",
    "section": "Call stacks",
    "text": "Call stacks\n最后一个特殊的环境是调用环境，可以通过rlang::caller_env()或base::parent.frame()查看。所谓调用环境就是调用函数的环境，如果你在函数A中调用函数B，那么B的调用环境就是函数A的函数环境。\n\ncaller_env()\n#&gt; &lt;environment: 0x0000023630864430&gt;\nparent.frame()\n#&gt; &lt;environment: 0x0000023630814b00&gt;\n\n\nSimple call stacks\n函数之间的调用，会形成调用栈。下面是一个简单的函数调用栈。\n\nf &lt;- function(x) {\n  g(x = 2)\n}\ng &lt;- function(x) {\n  h(x = 3)\n}\nh &lt;- function(x) {\n  stop()\n}\n\n使用traceback()可以查看调用栈。\n\nf(x = 1)\n#&gt; Error in h(x = 3):\ntraceback()\n#&gt; No traceback available\n\n除了搭配stop() + traceback()，也可以使用lobstr::cst()直接查看调用栈。二者不同点在于栈的顺序相反。\n\nh &lt;- function(x) {\n  lobstr::cst()\n}\nf(x = 1)\n#&gt;     ▆\n#&gt;  1. └─global f(x = 1)\n#&gt;  2.   └─global g(x = 2)\n#&gt;  3.     └─global h(x = 3)\n#&gt;  4.       └─lobstr::cst()\n\n\n\nLazy evaluation\n当函数的参数是函数返回值时，会触发惰性评估。调用栈会首先显示外层的调用，再显示内层的调用。\n\na &lt;- function(x) b(x)\nb &lt;- function(x) d(x)\nd &lt;- function(x) x\n\na(f())\n#&gt;     ▆\n#&gt;  1. ├─global a(f())\n#&gt;  2. │ └─global b(x)\n#&gt;  3. │   └─global d(x)\n#&gt;  4. └─global f()\n#&gt;  5.   └─global g(x = 2)\n#&gt;  6.     └─global h(x = 3)\n#&gt;  7.       └─lobstr::cst()\n\n\n\nFrames\n调用栈的每一层都被称为frame，是一种极其重要的内部数据结构，R代码只能访问其中部分数据，篡改frame会导致R崩溃。\n每个frame都三个关键点：\n\nexpr：调用的函数表达式，即在终端打印出的信息。\nenv：通常是函数表达式的执行环境。有两个例外：全局环境的frame是全局环境，eval()函数中的环境是任意的。\nparent：调用栈的栈（图中灰色线）。\n\n\n除此，frame还有退出机制on.exit(),return()等细节。\n\n\nDynamic scope\n在调用栈中检索变量的行为被称为动态作用域（dynamic scope）。动态作用域主要用于开发有助于交互式数据分析的函数，这是第 20 章讨论的主题之一。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/7 Environments.html#as-data-structures",
    "href": "Books/Advanced R(2e)/7 Environments.html#as-data-structures",
    "title": "7 Environments",
    "section": "As data structures",
    "text": "As data structures\n环境的数据结构可以作为一种引用语义，帮助解决下面三个常见的问题：\n\n避免大型数据的复制：在环境中，你永远也不会意外地创建副本。但是直接使用环境十分地不方便，推荐在第14章中讲到地R6类。\n管理R包的状态：创建一个额外的环境，在环境中记录状态。\n\n\nmy_env &lt;- new.env(parent = emptyenv())\nmy_env$a &lt;- 1\n\nget_a &lt;- function() {\n  my_env$a\n}\nset_a &lt;- function(value) {\n  old &lt;- my_env$a\n  my_env$a &lt;- value\n  invisible(old)\n}\n\n\n哈希映射 ：环境的数据结构就是一种哈希映射，可以缩短检索时间。",
    "crumbs": [
      "7 Environments"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/5 Control flow.html",
    "href": "Books/Advanced R(2e)/5 Control flow.html",
    "title": "5 Control flow",
    "section": "",
    "text": "R 中有两类主要的控制流类型工具：选择（if）和循环（for）。选择包含if、switch()等声明；循环包含for、while等声明。这里假定你已经学会了它们的基础用法，本章主要介绍一些技术细节和鲜为人知的高级用法。\n\n\n\n5.2节：介绍if、ifelse()、switch()函数。\n5.3节：介绍for、while、repeat等声明。",
    "crumbs": [
      "5 Control flow"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/5 Control flow.html#introduction",
    "href": "Books/Advanced R(2e)/5 Control flow.html#introduction",
    "title": "5 Control flow",
    "section": "",
    "text": "R 中有两类主要的控制流类型工具：选择（if）和循环（for）。选择包含if、switch()等声明；循环包含for、while等声明。这里假定你已经学会了它们的基础用法，本章主要介绍一些技术细节和鲜为人知的高级用法。\n\n\n\n5.2节：介绍if、ifelse()、switch()函数。\n5.3节：介绍for、while、repeat等声明。",
    "crumbs": [
      "5 Control flow"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/5 Control flow.html#choices",
    "href": "Books/Advanced R(2e)/5 Control flow.html#choices",
    "title": "5 Control flow",
    "section": "Choices",
    "text": "Choices\n下面是if-else语句的基本使用格式：\n\nif (condition) true_action\n\nif (condition) true_action else false_action\n\nif (condition1) {\n  true_action1\n} else if (condition2) {\n  true_action2\n} else {\n  false_action\n}\n\nif语句也可以进行赋值操作；在实际书写代码时，建议只有在if-else语句可以写为一行时，才使用赋值操作。\n\nx1 &lt;- if (TRUE) 1 else 2\nx2 &lt;- if (FALSE) 1 else 2\n\nc(x1, x2)\n#&gt; [1] 1 2\n\n当if-else语句只有if声明时，如果条件不满足，则返回NULL。函数c()、paste()会自动去除返回值中的NULL值。\n\ngreet &lt;- function(name, birthday = FALSE) {\n  paste0(\n    \"Hi \", name,\n    if (birthday) \" and HAPPY BIRTHDAY\"\n  )\n}\ngreet(\"Maria\", FALSE)\n#&gt; [1] \"Hi Maria\"\ngreet(\"Jaime\", TRUE)\n#&gt; [1] \"Hi Jaime and HAPPY BIRTHDAY\"\n\n\nInvalid inputs\n需要注意的是，if声明中的条件返回值只能是长度为1的布尔值。如果长度大于1，在R 4.0版本前会选择第一个值，但在R 4.0版本后会报错。其他类型的输入也会报错。\n\nif (\"x\") 1\n#&gt; Error in if (\"x\") 1: argument is not interpretable as logical\nif (logical()) 1\n#&gt; Error in if (logical()) 1: argument is of length zero\nif (NA) 1\n#&gt; Error in if (NA) 1: missing value where TRUE/FALSE needed\nif (c(TRUE, FALSE)) 1\n#&gt; Error in if (c(TRUE, FALSE)) 1: the condition has length &gt; 1\n\n\n\nVectorised if\nif-else 语句只能判断一次，假如你想要判断很多次，可以使用ifelse()函数。该函数接受三个参数：条件，返回值，其他值。如果条件为TRUE，返回值作为结果，否则返回其他值。条件处可以是向量，返回的也是向量。（可以理解for循环if-else语句）\n\nx &lt;- c(1:10, NA, 12)\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n#&gt;  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\" NA    \"12\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n#&gt;  [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n#&gt; [11] NA     \"even\"\n\n\n\n\n\n\n\nTip\n\n\n\n建议只有在yes和no条件的返回值类型一致时，再使用ifelse()函数。如果不同，因为c()是atomic向量，会强制进行类型转换。函数要求得条件如果不是布尔值，则会进行类型转换as.logical()，如果转换结果仍不是布尔值，则会返回转换后的值。\n\n\ndplyr包提供了等价函数case_when()，使用方法如下：\n\ndplyr::case_when(\n  x %% 35 == 0 ~ \"fizz buzz\",\n  x %% 5 == 0 ~ \"fizz\",\n  x %% 7 == 0 ~ \"buzz\",\n  is.na(x) ~ \"???\",\n  TRUE ~ as.character(x)\n)\n#&gt;  [1] \"1\"    \"2\"    \"3\"    \"4\"    \"fizz\" \"6\"    \"buzz\" \"8\"    \"9\"    \"fizz\"\n#&gt; [11] \"???\"  \"12\"\n\n\n\nswitch() statement\nswitch()语句是对if-else语句的压缩，例如你可以将下面的if-else语句:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\"\n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\n简化为switch()语句:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\n\n再判断条件的末尾添加错误信息，可以提高代码的可读性，因为当不满足匹配条件时，switch()语句返回NULL。\n\n(switch(\"c\",\n  a = 1,\n  b = 2\n))\n#&gt; NULL\n\n如果不同的输入条件返回值相同，可以省略返回值，switch()会自动向下匹配，例如:\n\nlegs &lt;- function(x) {\n  switch(x,\n    cow = ,\n    horse = ,\n    dog = 4,\n    human = ,\n    chicken = 2,\n    plant = 0,\n    stop(\"Unknown input\")\n  )\n}\nlegs(\"cow\")\n#&gt; [1] 4\nlegs(\"dog\")\n#&gt; [1] 4\n\n\n\n\n\n\n\nTip\n\n\n\nswitch()的输入可以是数值、字符串，但建议只使用字符串。\n\n\n\n\nExercises\n…",
    "crumbs": [
      "5 Control flow"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/5 Control flow.html#loops",
    "href": "Books/Advanced R(2e)/5 Control flow.html#loops",
    "title": "5 Control flow",
    "section": "Loops",
    "text": "Loops\nfor 循环的基本格式如下：\n\nfor (item in vector) perform_action\n\n有两种中断循环的方法：break和next。break用于跳出整个循环，next用于跳出当前循环，继续下一个循环。\n\nfor (i in 1:10) {\n  if (i &lt; 3) {\n    next\n  }\n\n  print(i)\n\n  if (i &gt;= 5) {\n    break\n  }\n}\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n\n\n\n\n\n\n\nNote\n\n\n\n要注意在环境变量中不要有与item名重复的变量。for循环会赋值给item变量，这样会导致item变量的值变化。\n\ni &lt;- 100\nfor (i in 1:3) {}\ni\n#&gt; [1] 3\n\n\n\n\nCommon pitfalls\n在使用for循环时，有三个常见的易错陷阱：\n\n进行赋值操作前，没有定义容纳结果的变量。\n使用1:length(x)作为索引，而不是seq_along(x)。\n直接索引S3对象。\n\n如果没有事先定义容器，会导致for循环十分缓慢。可以使用vector()函数，定义容器类型：\n\nmeans &lt;- c(1, 50, 20)\nout &lt;- vector(\"list\", length(means))\nfor (i in 1:length(means)) {\n  out[[i]] &lt;- rnorm(10, means[[i]])\n}\n\n1:length(x)在x的长度为0时，会报错。因为:对升序和降序都兼容，使用seq_along()函数可以变相的解决该问题。seq_along()函数返回一个长度与x相同的等差向量。\n\nx &lt;- c(1, 2, 3, 1,2,3)\ny &lt;- numeric(0)\n\n1:length(x)\n#&gt; [1] 1 2 3 4 5 6\nseq_along(x)\n#&gt; [1] 1 2 3 4 5 6\n\n1:length(y) # 在for循环中报错\n#&gt; [1] 1 0\nseq_along(y)\n#&gt; integer(0)\n\n\nmeans &lt;- c()\nout &lt;- vector(\"list\", length(means))\nfor (i in 1:length(means)) {\n  out[[i]] &lt;- rnorm(10, means[[i]])\n}\n#&gt; Error in rnorm(10, means[[i]]): invalid arguments\n\nout &lt;- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  out[[i]] &lt;- rnorm(10, means[[i]])\n}\n\n直接迭代S3对象时，for循环会丢掉S3对象的属性：\n\nxs &lt;- as.Date(c(\"2020-01-01\", \"2010-01-01\"))\nfor (x in xs) {\n  print(x)\n}\n#&gt; [1] 18262\n#&gt; [1] 14610\n\nfor (i in seq_along(xs)) {\n  print(xs[[i]])\n}\n#&gt; [1] \"2020-01-01\"\n#&gt; [1] \"2010-01-01\"\n\n\n\nRelated tools\n当你不知道要迭代多少次，可以使用while()和repeat()，它们的基本格式如下：\n\n# 当满足条件时，执行动作，直到条件不满足\nwhile (condition) action\n# 无限循环，直到遇到 break\nrepeat(action)\n\n\nx &lt;- 0\nwhile (x &lt; 10) {\n  x &lt;- x + 1\n  print(x)\n}\n#&gt; [1] 1\n#&gt; [1] 2\n#&gt; [1] 3\n#&gt; [1] 4\n#&gt; [1] 5\n#&gt; [1] 6\n#&gt; [1] 7\n#&gt; [1] 8\n#&gt; [1] 9\n#&gt; [1] 10\n\nrepeat {\n  print(x)\n  x &lt;- x + 1\n  if (x &gt; 10) {\n    break\n  }\n}\n#&gt; [1] 10\n\n理论上讲：所有for循环都可以用while循环替代，所有while循环都可以用repeat循环替代，但是反过来不行；这意味着for循环的灵活性很低，但是在实践中，我们应该使用灵活性低的for循环。\n在数据分析中，我们可以使用更高级的map()和apply()函数。\n\n\nExercises\n\n一定要避免使用1:length(x)，下面的例子，不会报错，但是返回结果不对。\n\n\nx &lt;- numeric()\nout &lt;- vector(\"list\", length(x))\nfor (i in 1:length(x)) {\n  out[i] &lt;- x[i]^2\n}\nout\n#&gt; [[1]]\n#&gt; [1] NA\n\nx &lt;- numeric()\nout &lt;- vector(\"list\", length(x))\nfor (i in seq_along(x)) {\n  out[i] &lt;- x[i]^2\n}\nout\n#&gt; list()\n\n\nR的for循环只评估一次输入，即使for循环中对评估进行了更新，也不会改变，避免了无限循环的可能。\n\n\nxs &lt;- c(1, 2, 3)\nfor (x in xs) {\n  xs &lt;- c(xs, x * 2)\n}\nxs\n#&gt; [1] 1 2 3 2 4 6\n\n\nR的for循环对于item的更新发生在每次迭代开始前，for循环中对item进行的更新无效。\n\n\nfor (i in 1:3) {\n  i &lt;- i * 2\n  print(i)\n}\n#&gt; [1] 2\n#&gt; [1] 4\n#&gt; [1] 6",
    "crumbs": [
      "5 Control flow"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html",
    "href": "Books/Advanced R(2e)/3 Vectors.html",
    "title": "3 Vectors",
    "section": "",
    "text": "R 中的Vectors可以分为两种：要求所有元素相同的atomic、元素可以是任意类型的list，还有一个类似零长度向量的NULL。下面是三者关系的示意图。\n\n\n\n\n\ngraph TD\n    A[Atomic] --&gt; B(Vector)\n    C[List] --&gt; B(Vector)\n    D(NULL)\n\n\n\n\n\n\n这里提到的vector就是我们常说的向量。在R中，所有的对象都是向量，对向量添加不同属性就构成了不同类型的数据。维度（dimension）和类（clas）是两个比较重要的属性，前者将一维向量升格为二维矩阵甚至多维数组，后者赋予了对象S3面向对象系统。\n\n\n\n3.2节：R中最简单的 atomic 向量：integer、double、character、logical。\n3.3节：向量的三个重要属性：name、dimension、class。\n3.4节：具有特殊属性的 atomic 向量：factor、date、date-time、duration。\n3.5节：list。\n3.6节：data.frame 和 tibble。",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#introduction",
    "href": "Books/Advanced R(2e)/3 Vectors.html#introduction",
    "title": "3 Vectors",
    "section": "",
    "text": "R 中的Vectors可以分为两种：要求所有元素相同的atomic、元素可以是任意类型的list，还有一个类似零长度向量的NULL。下面是三者关系的示意图。\n\n\n\n\n\ngraph TD\n    A[Atomic] --&gt; B(Vector)\n    C[List] --&gt; B(Vector)\n    D(NULL)\n\n\n\n\n\n\n这里提到的vector就是我们常说的向量。在R中，所有的对象都是向量，对向量添加不同属性就构成了不同类型的数据。维度（dimension）和类（clas）是两个比较重要的属性，前者将一维向量升格为二维矩阵甚至多维数组，后者赋予了对象S3面向对象系统。\n\n\n\n3.2节：R中最简单的 atomic 向量：integer、double、character、logical。\n3.3节：向量的三个重要属性：name、dimension、class。\n3.4节：具有特殊属性的 atomic 向量：factor、date、date-time、duration。\n3.5节：list。\n3.6节：data.frame 和 tibble。",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#atomic-vectors",
    "href": "Books/Advanced R(2e)/3 Vectors.html#atomic-vectors",
    "title": "3 Vectors",
    "section": "Atomic vectors",
    "text": "Atomic vectors\nR 中有四类常见的atomic向量：integer、double、character、logical。下面是这四类向量之间的关系图。还有两类不常见的：complex和raw，前者表示复数，后者表示二级制数据。\n\n\n\n\n\ngraph BT\n    B[Atomic] --&gt; A(Vector)\n    D[Logical] --&gt; B(Atomic)\n    C[Numeric] --&gt; B(Atomic)\n    E[Integer] --&gt; C(Numeric)\n    F[Double] --&gt; C(Numeric)\n    G[Character] --&gt; B(Atomic)\n    subgraph \" \"\n        D\n        E\n        F\n        G\n    end\n\n\n\n\n\n\n\nScalars\n上述四种atomic向量都有各自的语法：\n\nlogical：只能使用TRUE/T和FALSE/F。\ndouble：有三种形式：十进制、科学技术法和十六进制；还有三个特殊的符号：NaN、Inf和-Inf。\ninteger：形式同double，但是必须添加L后缀。\ncharacter：使用双引号或单引号包裹，\\进行转义。\n\n\n\nMaking longer vectors with c()\n使用c()函数可以创建长向量，使用typeof()查看对象的类型。\n\nlgl_var &lt;- c(TRUE, FALSE)\nint_var &lt;- c(1L, 6L, 10L)\ndbl_var &lt;- c(1, 2.5, 4.5)\nchr_var &lt;- c(\"these are\", \"some strings\")\n\ntypeof(lgl_var)\n#&gt; [1] \"logical\"\ntypeof(int_var)\n#&gt; [1] \"integer\"\ntypeof(dbl_var)\n#&gt; [1] \"double\"\ntypeof(chr_var)\n#&gt; [1] \"character\"\n\n\n\nMissing values\nNA是not applicable的缩写，表示缺失值。缺失值有一定的‘传染性’：许多包含NA的计算会返回NA。\n\nNA &gt; 5\n#&gt; [1] NA\n10 * NA\n#&gt; [1] NA\n!NA\n#&gt; [1] NA\n\n但有一些特例：\n\nNA^0\n#&gt; [1] 1\nNA | TRUE\n#&gt; [1] TRUE\nNA & FALSE\n#&gt; [1] FALSE\n\n使用is.na()判断是否是NA，而不是==。\n\nx &lt;- c(NA, 5, NA, 10)\nx == NA\n#&gt; [1] NA NA NA NA\nis.na(x)\n#&gt; [1]  TRUE FALSE  TRUE FALSE\n\n理论上讲：存在四种NA类型，分别对应上述的四种atomic向量：NA(logical)、NA_integer_(integer)、NA_real_(double)、NA_character(character)。但平常使用无需强调，R会自动强制转换。NA默认表示logical是因为下面的强制转换顺序。\n\n\nCoercion\n如上所述，当atomic向量中的元素类型不一致时，R会自动强制转换类型。转换的顺序次序为：character → double → integer → logical。例如，c(\"a\", 1)的结果是c(\"a\", \"1\")。\n\nc(\"a\", 1)\n#&gt; [1] \"a\" \"1\"\n\n强制转换的背后是as.*()函数，例如as.logical()、as.integer()、as.double()、as.character()。转换失败会生成警告信息和缺失值。\n\nas.integer(c(\"1\", \"1.5\", \"a\"))\n#&gt; Warning: NAs introduced by coercion\n#&gt; [1]  1  1 NA\n\n\n\nTesting\nR 中由类似判断是否是缺失值is.na()的其他函数，但是要小心使用。is.logical()、is.integer()、is.double()、is.character()这四个函数的用法与is.na()相同；is.vector()、is.atomic()、is.numeric()的用法则大不相同。\n\nis.vector()：根据mode参数，判断是否属于atomic向量、list、或expression，同时不能有除name外的其他属性。\nis.atomic()：判断是否属于atomic向量，NULL返回FALSE。\nis.numeric()：属于double、integer或任何可视为数值的向量。\n\n具体细节可以查看R文档。\n\n\nExercises\n…",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#attributes",
    "href": "Books/Advanced R(2e)/3 Vectors.html#attributes",
    "title": "3 Vectors",
    "section": "Attributes",
    "text": "Attributes\n\nGetting and setting\n属性可以看作是由键值对构成，附属于对象的元信息。可以使用attr(),attributes(),structure()来提取或设置对象的属性。\n\n# 单独设置\na &lt;- 1:3\nattr(a, \"x\") &lt;- \"abcdef\"\nattr(a, \"x\")\n#&gt; [1] \"abcdef\"\n\nattr(a, \"y\") &lt;- 4:6\nstr(attributes(a))\n#&gt; List of 2\n#&gt;  $ x: chr \"abcdef\"\n#&gt;  $ y: int [1:3] 4 5 6\n\n\n# 批量设置\na &lt;- structure(\n  1:3,\n  x = \"abcdef\",\n  y = 4:6\n)\nstr(attributes(a))\n#&gt; List of 2\n#&gt;  $ x: chr \"abcdef\"\n#&gt;  $ y: int [1:3] 4 5 6\n\n\n属性极易丢失，但有两个例外，通常会保留：name,dim；保留属性需要使用S3面向对象系统。\n\nattributes(a[1])\n#&gt; NULL\nattributes(sum(a))\n#&gt; NULL\n\n\n\nNames\n有三种为向量设定name属性的方法。去除name属性有两种方法.\n\n# When creating it:\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n#&gt; a b c \n#&gt; 1 2 3\nx &lt;- unname(x)\nx\n#&gt; [1] 1 2 3\n\n# By assigning a character vector to names()\nx &lt;- 1:3\nnames(x) &lt;- c(\"a\", \"b\", \"c\")\nx\n#&gt; a b c \n#&gt; 1 2 3\nnames(x) &lt;- NULL\nx\n#&gt; [1] 1 2 3\n\n# Inline, with setNames():\nx &lt;- setNames(1:3, c(\"a\", \"b\", \"c\"))\nx\n#&gt; a b c \n#&gt; 1 2 3\n\n在创建name属性时，应当保持name唯一且不为缺失值。需要注意这不是R的强制要求。\n\n\nDimensions\natomic向量添加dim属性，就可以生成二维matrix或多维array。在R中可以使用matrix(),array()或dim()来生成。\n\n# Two scalar arguments specify row and column sizes\nx &lt;- matrix(1:6, nrow = 2, ncol = 3)\nx\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    3    5\n#&gt; [2,]    2    4    6\n\n\n# One vector argument to describe all dimensions\ny &lt;- array(1:12, c(2, 3, 2))\ny\n#&gt; , , 1\n#&gt; \n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    3    5\n#&gt; [2,]    2    4    6\n#&gt; \n#&gt; , , 2\n#&gt; \n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    7    9   11\n#&gt; [2,]    8   10   12\n\n# You can also modify an object in place by setting dim()\nz &lt;- 1:6\ndim(z) &lt;- c(3, 2)\nz\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    4\n#&gt; [2,]    2    5\n#&gt; [3,]    3    6\n\natomic向量的许多函数都对matrix和array进行了推广。\n\n\n\nVector\nMatrix\nArray\n\n\n\n\nnames()\nrownames(),colnames()\ndimnames()\n\n\nlength()\nnrow(),ncol()\ndim()\n\n\nc()\nrbind(),cbind()\nabind::abind()\n\n\n-\nt()\naperm()\n\n\nis.null(dim(x))\nis.matrix()\nis.array()\n\n\n\n不能简单地将没有维度的atomic向量视作只有1行或1列的矩阵、1维的数组，因为函数对它们的处理结果是不一样的。\n\nstr(1:3) # 1d vector\n#&gt;  int [1:3] 1 2 3\nstr(matrix(1:3, ncol = 1)) # column vector\n#&gt;  int [1:3, 1] 1 2 3\nstr(matrix(1:3, nrow = 1)) # row vector\n#&gt;  int [1, 1:3] 1 2 3\nstr(array(1:3, 3)) # \"array\" vector\n#&gt;  int [1:3(1d)] 1 2 3\n\n\n\nExercises\n\ncomment属性比较特殊，不会被print打印出来，只能使用attr()或attributes()来提取。",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#s3-atomic-vectors",
    "href": "Books/Advanced R(2e)/3 Vectors.html#s3-atomic-vectors",
    "title": "3 Vectors",
    "section": "S3 atomic vectors",
    "text": "S3 atomic vectors\nclass是另外一个重要的属性，它赋予了对象S3面向对象的能力。每个S3对象由base type构建（base type内容见12章）。本节主要讨论下面四种S3 atomic vectors：\n\nfactor：分类数据。\nDate：日期数据。\nPOSIXct：日期时间数据。\ndifftime：持续时间数据。\n\n\n\n\n\n\ngraph BT\n    B[Atomic] --&gt; A(Vector)\n    D[Logical] --&gt; B(Atomic)\n    C[Numeric] --&gt; B(Atomic)\n    E[Integer] --&gt; C(Numeric)\n    F[Double] --&gt; C(Numeric)\n    G[Character] --&gt; B(Atomic)\n    subgraph \"base atomic\"\n        D\n        E\n        F\n        G\n    end\n    H[factor] --&gt; E\n    I[POSIXct] --&gt; F\n    J[Date] --&gt; F\n    subgraph \"S3 atomic\"\n        H\n        I\n        J\n    end\n\n\n\n\n\n\n\nFactors\nfactor 是在integer类型向量基础上，添加class = fcator和levels两个属性构成。\n\nx &lt;- factor(c(\"a\", \"b\", \"b\", \"a\"))\nx\n#&gt; [1] a b b a\n#&gt; Levels: a b\n\ntypeof(x)\n#&gt; [1] \"integer\"\nattributes(x)\n#&gt; $levels\n#&gt; [1] \"a\" \"b\"\n#&gt; \n#&gt; $class\n#&gt; [1] \"factor\"\n\n\nfactor 有一个变体——ordered factor，它的属性levels具有了顺次意义，例如：low,medium,high等。\n\ngrade &lt;- ordered(c(\"b\", \"b\", \"a\", \"c\"), levels = c(\"c\", \"b\", \"a\"))\ngrade\n#&gt; [1] b b a c\n#&gt; Levels: c &lt; b &lt; a\n\nbase R 中的函数如read.csv(),data.frame()会自动将字符串转换为factor，这种行为很没有道理，因为有时数据中不可能包含全部的level。可以通过stringsAsFactors = FALSE来禁用这种行为。\n\n\nDates\nDate 是在double类型向量基础上，添加class = Date属性构成。去除属性后，可以看到原来的double向量。\n\ntoday &lt;- Sys.Date()\n\ntypeof(today)\n#&gt; [1] \"double\"\nattributes(today)\n#&gt; $class\n#&gt; [1] \"Date\"\n\n# 底层double向量，思考一下这个数字的含义？\nunclass(today)\n#&gt; [1] 20347\n\n\n\nDate-times\nbase R 提供了两种储存 date-time 的数据格式：POSIXct和POSIXlt。这里我们只关注POSIXct，因为它构建于double类型向上，且在数据框中广泛应用。POSIXct有两个属性：class = POSIXct和tzone。\n\nnow_ct &lt;- as.POSIXct(\"2018-08-01 22:00\", tz = \"UTC\")\nnow_ct\n#&gt; [1] \"2018-08-01 22:00:00 UTC\"\n\ntypeof(now_ct)\n#&gt; [1] \"double\"\nattributes(now_ct)\n#&gt; $class\n#&gt; [1] \"POSIXct\" \"POSIXt\" \n#&gt; \n#&gt; $tzone\n#&gt; [1] \"UTC\"\n\ntzone属性值控制date-time的显示形式，不改变其本质的double数值。\n\nnew_ct &lt;- structure(now_ct, tzone = \"Asia/Tokyo\")\nnew_ct\n#&gt; [1] \"2018-08-02 07:00:00 JST\"\nunclass(now_ct)\n#&gt; [1] 1533160800\n#&gt; attr(,\"tzone\")\n#&gt; [1] \"UTC\"\nunclass(new_ct)\n#&gt; [1] 1533160800\n#&gt; attr(,\"tzone\")\n#&gt; [1] \"Asia/Tokyo\"\n\n\n\nDurations\ndurations 表示date或date-time之间的持续时间，它是在double类型向量基础上，添加class = difftime属性构成，同时有属性units，表示持续时间的单位。\n\none_week_1 &lt;- as.difftime(1, units = \"weeks\")\none_week_1\n#&gt; Time difference of 1 weeks\n\ntypeof(one_week_1)\n#&gt; [1] \"double\"\n#&gt; [1] \"double\"\nattributes(one_week_1)\n#&gt; $class\n#&gt; [1] \"difftime\"\n#&gt; \n#&gt; $units\n#&gt; [1] \"weeks\"\n\none_week_2 &lt;- as.difftime(7, units = \"days\")\none_week_2\n#&gt; Time difference of 7 days\n\ntypeof(one_week_2)\n#&gt; [1] \"double\"\nattributes(one_week_2)\n#&gt; $class\n#&gt; [1] \"difftime\"\n#&gt; \n#&gt; $units\n#&gt; [1] \"days\"\n\n\n\nExercises\n\n仔细观察下面三个因子的不同。\n\n\n# 数据和level同时反转\nf1 &lt;- factor(letters)\nf1\n#&gt;  [1] a b c d e f g h i j k l m n o p q r s t u v w x y z\n#&gt; Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nas.integer(f1)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n#&gt; [25] 25 26\nlevels(f1) &lt;- rev(levels(f1))\nf1\n#&gt;  [1] z y x w v u t s r q p o n m l k j i h g f e d c b a\n#&gt; Levels: z y x w v u t s r q p o n m l k j i h g f e d c b a\nas.integer(f1)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n#&gt; [25] 25 26\n\n# 仅数据反转\nf2 &lt;- rev(factor(letters))\nf2\n#&gt;  [1] z y x w v u t s r q p o n m l k j i h g f e d c b a\n#&gt; Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nas.integer(f2)\n#&gt;  [1] 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n#&gt; [25]  2  1\n\n# 仅level反转\nf3 &lt;- factor(letters, levels = rev(letters))\nf3\n#&gt;  [1] a b c d e f g h i j k l m n o p q r s t u v w x y z\n#&gt; Levels: z y x w v u t s r q p o n m l k j i h g f e d c b a\nas.integer(f3)\n#&gt;  [1] 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n#&gt; [25]  2  1",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#lists",
    "href": "Books/Advanced R(2e)/3 Vectors.html#lists",
    "title": "3 Vectors",
    "section": "Lists",
    "text": "Lists\n虽然看起来list中的元素是不同，诚如前章所述，list中的元素有其自己的内存地址，也就是说，list中的元素本质是相同的，只是其索引的内容类型不同。\n\nCreating\n使用函数list()创建list。\n\nl1 &lt;- list(\n  1:3,\n  \"a\",\n  c(TRUE, FALSE, TRUE),\n  c(2.3, 5.9)\n)\n\ntypeof(l1)\n#&gt; [1] \"list\"\n\nstr(l1)\n#&gt; List of 4\n#&gt;  $ : int [1:3] 1 2 3\n#&gt;  $ : chr \"a\"\n#&gt;  $ : logi [1:3] TRUE FALSE TRUE\n#&gt;  $ : num [1:2] 2.3 5.9\n\n如上所述，list的元素只是索引，所以list的实际内存大小可能与你的期望相差甚远。\n\nlobstr::obj_size(mtcars)\n#&gt; 7.21 kB\n\nl2 &lt;- list(mtcars, mtcars, mtcars, mtcars)\nlobstr::obj_size(l2)\n#&gt; 7.29 kB\n\nlist 中的元素可以是其他的list。\n\nl3 &lt;- list(list(list(1)))\nstr(l3)\n#&gt; List of 1\n#&gt;  $ :List of 1\n#&gt;   ..$ :List of 1\n#&gt;   .. ..$ : num 1\n\n\n前面讲到函数c()要求元素类型一致，否则执行强制转换；当list和其他atomic向量使用c()合并时，atomic向量会被强制添加到list中。\n\nl4 &lt;- list(list(1, 2), c(3, 4))\nl5 &lt;- c(list(1, 2), c(3, 4))\nstr(l4)\n#&gt; List of 2\n#&gt;  $ :List of 2\n#&gt;   ..$ : num 1\n#&gt;   ..$ : num 2\n#&gt;  $ : num [1:2] 3 4\nstr(l5)\n#&gt; List of 4\n#&gt;  $ : num 1\n#&gt;  $ : num 2\n#&gt;  $ : num 3\n#&gt;  $ : num 4\n\n\n\n\nTesting and coercion\n\n使用typeof()查看是否为list。\n使用as.list()强制转换为list。\n使用unlist()强制转换为atomic向量。\n\n\n# 注意as.list的不同\nl6 &lt;- list(1:3)\nl6\n#&gt; [[1]]\n#&gt; [1] 1 2 3\n\ntypeof(l6)\n#&gt; [1] \"list\"\n\nas.list(1:3)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 2\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 3\n\n\n\nMatrices and arrays\natomic向量添加dim属性后创建出matrix，list添加dim属性后可以创建此出list-matrix、list-array。这些数据结构可以使得处理特殊数据时更加灵活。\n\nl &lt;- list(1:3, \"a\", TRUE, 1.0)\ndim(l) &lt;- c(2, 2)\nl\n#&gt;      [,1]      [,2]\n#&gt; [1,] integer,3 TRUE\n#&gt; [2,] \"a\"       1\n\nl[[1, 1]]\n#&gt; [1] 1 2 3\n\n\n\nExercises\n\n使用[函数提取元素时，当超出范围，atomic向量返回NA，list返回NULL。\n\n\n# Subsetting atomic vectors\n(1:2)[3]\n#&gt; [1] NA\n(1:2)[NA]\n#&gt; [1] NA NA\n\n# Subsetting lists\nas.list(1:2)[3]\n#&gt; [[1]]\n#&gt; NULL\nas.list(1:2)[NA]\n#&gt; [[1]]\n#&gt; NULL\n#&gt; \n#&gt; [[2]]\n#&gt; NULL",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#data-frames-and-tibbles",
    "href": "Books/Advanced R(2e)/3 Vectors.html#data-frames-and-tibbles",
    "title": "3 Vectors",
    "section": "Data frames and tibbles",
    "text": "Data frames and tibbles\n最重要的两个以list为基础构建的S3 对象是：data.frame和tibble。\n\n\n\n\n\ngraph BT\n    B[List] --&gt; A(Vector)\n    D[data.frame] --&gt; B(List)\n    C[tibble] --&gt; B(List)\n\n\n\n\n\n\ndataframe有三个属性names、class = data.frame、row.names。names是list本身的属性，表示为有name的list，在dataframe中表示列名；row.names表示行名。同时dataframe要求list中的每个元素长度都相等。作用于list的函数如names()，作用于dataframe时返回的是列名，等价于colnames()、length()返回的是列数，等价于ncol()。\n\ndf1 &lt;- data.frame(x = 1:3, y = letters[1:3])\ntypeof(df1)\n#&gt; [1] \"list\"\n\nattributes(df1)\n#&gt; $names\n#&gt; [1] \"x\" \"y\"\n#&gt; \n#&gt; $class\n#&gt; [1] \"data.frame\"\n#&gt; \n#&gt; $row.names\n#&gt; [1] 1 2 3\n\nnames(df1)\n#&gt; [1] \"x\" \"y\"\nlength(df1)\n#&gt; [1] 2\n\ntibble由R包tibble提供，是对dataframe的一种补充，二者由唯一的不同是属性class，tibble有其自己独特的类tbl_df,tbl。类的不同赋予了tibble格式在某些行为上与dataframe有着巨大的差别。下面会详细介绍二者的不同。\n\nlibrary(tibble)\n\ndf2 &lt;- tibble(x = 1:3, y = letters[1:3])\ntypeof(df2)\n#&gt; [1] \"list\"\n\nattributes(df2)\n#&gt; $class\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n#&gt; \n#&gt; $row.names\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $names\n#&gt; [1] \"x\" \"y\"\n\n\nCreating\n在使用data.frame()和tibble()函数创建dataframe或tibble时，主要有下面四点不同：\n\n列为字符串时，是否自动转换为因子：在R 4.0.0之前的版本，data.frame()函数默认stringsAsFactors = TRUE，而tibble()函数始终不会将字符串转换为因子。\n\n\ndf1 &lt;- data.frame(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\"),\n  stringsAsFactors = FALSE\n)\nstr(df1)\n#&gt; 'data.frame':    3 obs. of  2 variables:\n#&gt;  $ x: int  1 2 3\n#&gt;  $ y: chr  \"a\" \"b\" \"c\"\n\ndf2 &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\")\n)\nstr(df2)\n#&gt; tibble [3 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ x: int [1:3] 1 2 3\n#&gt;  $ y: chr [1:3] \"a\" \"b\" \"c\"\n\n\n列名不规范时，是否自动转换为合法列名：data.frame()在参数check.names = FALSE时不会转换列名，tibble()函数始终不会转换列名，而是使用“`”包裹列名。\n\n\nnames(data.frame(`1` = 1))\n#&gt; [1] \"X1\"\n\nnames(tibble(`1` = 1))\n#&gt; [1] \"1\"\n\n\n在构建数据框时，如果列之间的长度不同：data.frame()函数会将较短的列自动循环，tibble()函数只会循环长度为1的列，否则报错。\n\n\ndata.frame(x = 1:4, y = 1:2)\n#&gt;   x y\n#&gt; 1 1 1\n#&gt; 2 2 2\n#&gt; 3 3 1\n#&gt; 4 4 2\ndata.frame(x = 1:4, y = 1:3)\n#&gt; Error in data.frame(x = 1:4, y = 1:3): arguments imply differing number of rows: 4, 3\n\ntibble(x = 1:4, y = 1)\n#&gt; # A tibble: 4 × 2\n#&gt;       x     y\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     1     1\n#&gt; 2     2     1\n#&gt; 3     3     1\n#&gt; 4     4     1\ntibble(x = 1:4, y = 1:2)\n#&gt; Error in `tibble()`:\n#&gt; ! Tibble columns must have compatible sizes.\n#&gt; • Size 4: Existing data.\n#&gt; • Size 2: Column `y`.\n#&gt; ℹ Only values of size one are recycled.\n\n\n在构建数据框时，tibble()函数可以使用前面定义好的列的数据。\n\n\ntibble(\n  x = 1:3,\n  y = x * 2\n)\n#&gt; # A tibble: 3 × 2\n#&gt;       x     y\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     1     2\n#&gt; 2     2     4\n#&gt; 3     3     6\n\n\n\nRow names\ndataframe可以使用data.frame(row.names = ...)或rownames(df) &lt;- ...来赋予数据框行名，但是tibble不支持设置行名（实际是可以设置的，只是不建议这样做）。\n\ndf3 &lt;- data.frame(\n  age = c(35, 27, 18),\n  hair = c(\"blond\", \"brown\", \"black\"),\n  row.names = c(\"Bob\", \"Susan\", \"Sam\")\n)\ndf3\n#&gt;       age  hair\n#&gt; Bob    35 blond\n#&gt; Susan  27 brown\n#&gt; Sam    18 black\n\nrownames(df3)\n#&gt; [1] \"Bob\"   \"Susan\" \"Sam\"\n\ndf3[\"Bob\", ]\n#&gt;     age  hair\n#&gt; Bob  35 blond\n\n对于tibble为什么不赞成为数据框设置行名，可以参考原文。dataframe转换为tibble时可以使用rownames_to_column()或as_tibble(rownames = \"rownames\")来保留行名信息。\n\n\nPrinting\n在终端以dataframe或tibble格式分别打印数据，会发现：dataframe将所有行列都打印了出来，而tibble只打印前几行及前几列，同时显示未打印列及行的信息，并且显示每列数据的类型，压缩内容过长的单元格，显示特殊数据比如NA。\n\ndplyr::starwars\n#&gt; # A tibble: 87 × 14\n#&gt;   name           height  mass hair_color  skin_color  eye_color birth_year\n#&gt;   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Luke Skywalker    172    77 blond       fair        blue            19  \n#&gt; 2 C-3PO             167    75 &lt;NA&gt;        gold        yellow         112  \n#&gt; 3 R2-D2              96    32 &lt;NA&gt;        white, blue red             33  \n#&gt; 4 Darth Vader       202   136 none        white       yellow          41.9\n#&gt; 5 Leia Organa       150    49 brown       light       brown           19  \n#&gt; 6 Owen Lars         178   120 brown, grey light       blue            52  \n#&gt; # ℹ 81 more rows\n#&gt; # ℹ 7 more variables: sex &lt;chr&gt;, gender &lt;chr&gt;, homeworld &lt;chr&gt;, …\n\n\n\nSubsetting\n在第4章会学到更多有关提取子集的内容，这里介绍两个dataframe在提取子集时的两个缺点：\n\ndf[, vars]提取的列为1时，会自动降维成向量，除非添加参数df[, vars, drop = FALSE]。\ndf$x根据列名提取某列时，如果没有该列，会尝试返回一个字符串开头是x字符的列。\n\n\ndf1 &lt;- data.frame(xyz = \"a\")\ndf2 &lt;- tibble(xyz = \"a\")\n\nstr(df1$x)\n#&gt;  chr \"a\"\nstr(df2$x)\n#&gt;  NULL\n\n\n\nTesting and coercing\nis.data.frame()可以用来检查是否是dataframe或tibble，is_tibble()只能检查是否是tibble（注意两个函数在形式上的区别.,_）。\n\nis.data.frame(df1)\n#&gt; [1] TRUE\nis.data.frame(df2)\n#&gt; [1] TRUE\n\nis_tibble(df1)\n#&gt; [1] FALSE\nis_tibble(df2)\n#&gt; [1] TRUE\n\n\n\nList columns\n前面我们说过，数据框本质是一个list，所以数据框中的列可以包含任意类型的元素，包括list。tibble本身支持list column，dataframe需要额外的I()函数辅助。\n\n# tibble\ntibble(\n  x = 1:3,\n  y = list(1:2, 1:3, 1:4)\n)\n#&gt; # A tibble: 3 × 2\n#&gt;       x y        \n#&gt;   &lt;int&gt; &lt;list&gt;   \n#&gt; 1     1 &lt;int [2]&gt;\n#&gt; 2     2 &lt;int [3]&gt;\n#&gt; 3     3 &lt;int [4]&gt;\n\n# dataframe\ndf &lt;- data.frame(x = 1:3)\ndf$y &lt;- list(1:2, 1:3, 1:4)\n\ndata.frame(\n  x = 1:3,\n  y = I(list(1:2, 1:3, 1:4))\n)\n#&gt;   x          y\n#&gt; 1 1       1, 2\n#&gt; 2 2    1, 2, 3\n#&gt; 3 3 1, 2, 3, 4\n\n\n\nMatrix and data frame columns\n同上面的list一样，你可以在数据框中添加行数相等的矩阵和数据框。\n\n\ndfm &lt;- data.frame(\n  x = 1:3 * 10\n)\ndfm$y &lt;- matrix(1:9, nrow = 3)\ndfm$z &lt;- data.frame(a = 3:1, b = letters[1:3], stringsAsFactors = FALSE)\n\nstr(dfm)\n#&gt; 'data.frame':    3 obs. of  3 variables:\n#&gt;  $ x: num  10 20 30\n#&gt;  $ y: int [1:3, 1:3] 1 2 3 4 5 6 7 8 9\n#&gt;  $ z:'data.frame':   3 obs. of  2 variables:\n#&gt;   ..$ a: int  3 2 1\n#&gt;   ..$ b: chr  \"a\" \"b\" \"c\"\n\n\n\n\nExercises\n…",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/3 Vectors.html#null",
    "href": "Books/Advanced R(2e)/3 Vectors.html#null",
    "title": "3 Vectors",
    "section": "NULL",
    "text": "NULL\nNULL 的长度为0，类型为NULL，且无法拥有任何属性。\n\ntypeof(NULL)\n#&gt; [1] \"NULL\"\n\nlength(NULL)\n#&gt; [1] 0\n\nx &lt;- NULL\nattr(x, \"y\") &lt;- 1\n#&gt; Error in attr(x, \"y\") &lt;- 1: attempt to set an attribute on NULL",
    "crumbs": [
      "3 Vectors"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html",
    "href": "Books/Advanced R(2e)/24 Improving performance.html",
    "title": "24 Improving performance",
    "section": "",
    "text": "Tip\n\n\n\nWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.\n— Donald Knuth\n\n\n本节介绍四种通用的优化工具和一个通用的性能优化策略，帮助你确保优化后的代码仍然结果正确。但要注意：优化需建立在对实际瓶颈的识别之上，避免在非关键部分浪费精力，同时抓住真正影响性能的核心环节。如果你想更多地了解R语言的性能特点，推荐Evaluating the Design of the R Language这本书，该书通过将一个经过修改的R解释器与大量实际应用中的代码相结合，得出了相关结论。\n\n\n24.2节：介绍如何组织代码，使优化尽可能简单、无bug。\n24.3节：提醒你去寻找已有的解决方案。\n24.4节：强调“懒惰”的重要性：使函数运行快的最简单方法就是让函数做最简单的事。\n24.5节：介绍向量化，并展示如何最大限度地利用内置函数。\n24.6节：讨论复制数据的性能风险。\n24.7节：将所有片段整合成一个案例研究，展示如何将重复t检验的速度提高约1000倍。\n24.8节：提供了更多帮助你编写快速代码资源的指针。\n\n\nlibrary(bench)",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#introduction",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#introduction",
    "title": "24 Improving performance",
    "section": "",
    "text": "Tip\n\n\n\nWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.\n— Donald Knuth\n\n\n本节介绍四种通用的优化工具和一个通用的性能优化策略，帮助你确保优化后的代码仍然结果正确。但要注意：优化需建立在对实际瓶颈的识别之上，避免在非关键部分浪费精力，同时抓住真正影响性能的核心环节。如果你想更多地了解R语言的性能特点，推荐Evaluating the Design of the R Language这本书，该书通过将一个经过修改的R解释器与大量实际应用中的代码相结合，得出了相关结论。\n\n\n24.2节：介绍如何组织代码，使优化尽可能简单、无bug。\n24.3节：提醒你去寻找已有的解决方案。\n24.4节：强调“懒惰”的重要性：使函数运行快的最简单方法就是让函数做最简单的事。\n24.5节：介绍向量化，并展示如何最大限度地利用内置函数。\n24.6节：讨论复制数据的性能风险。\n24.7节：将所有片段整合成一个案例研究，展示如何将重复t检验的速度提高约1000倍。\n24.8节：提供了更多帮助你编写快速代码资源的指针。\n\n\nlibrary(bench)",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#code-organisation",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#code-organisation",
    "title": "24 Improving performance",
    "section": "Code organisation",
    "text": "Code organisation\n在尝试优化代码时，通常会掉入两个陷阱中：\n\n代码运行快速但不正确。\n你认为运行快速，但实际效果并不好（没有进行基准测试）。\n\n下面的策略会帮助你避免这些陷阱。\n在进行多种方法的基准测试前，可以将这个方法打包成一个函数。因为函数内的环境是独立的，不会存在干扰，也方便检查返回的结果是否正确。例如，对比两种不同计算均值的方法：\n\nmean1 &lt;- function(x) mean(x)\nmean2 &lt;- function(x) sum(x) / length(x)\n\n建议你记录所有尝试的内容，甚至包括失败。如果将来发生类似的问题，查看你尝试过的所有内容会很有用。推荐使用RMarkdown或quarto, 这使得将代码与详细的注释和说明混合在一起变得容易。\n接下来，生成一个具有代表性的测试示例。这个示例应该足够大，以捕捉问题的本质，但又要足够小，运行最多只需要几秒钟。你不希望花费太长时间，因为你需要多次运行测试示例来比较方法。另一方面，你也不希望示例太小，因为那样结果可能无法扩展到真正的问题。这里使用100,000个数字来进行测试：\n\nx &lt;- runif(1e5)\n\n现在使用bench::mark()来精确比较变量。bench::mark()会自动检查所有调用是否返回相同类型的值。这并不能保证函数对所有输入的行为都相同，因此在理想情况下，还需要进行单元测试，以确保不会意外地改变函数的行为。\n\nbench::mark(\n  mean1(x),\n  mean2(x)\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 2 × 4\n#&gt;   expression      min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 mean1(x)    135.8µs  149.7µs     6354.\n#&gt; 2 mean2(x)     66.4µs   69.7µs    13130.\n\n从结果上看，mean()意外地要比sum(x) / length(x)慢一些。这是因为mean()在计算时，会进行一些额外的步骤，来提升结果地精度。如果你对这种计算策略感兴趣，可以查看：\n\nhttp://stackoverflow.com/questions/22515525#22518603\nhttp://stackoverflow.com/questions/22515175#22515856\nhttp://stackoverflow.com/questions/3476015#22511936",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#checking-for-existing-solutions",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#checking-for-existing-solutions",
    "title": "24 Improving performance",
    "section": "Checking for existing solutions",
    "text": "Checking for existing solutions\n当你尝试过自己很多种想法后，仍然很难解决问题时，你可以检查是否已经有成熟地解决方案了。下面是两个好的检索开始：\n\nCRAN task views，根据任务收集CRAN上的包。\n在Rcpp的CRAN主页上，可以找到一些使用Rcpp的包，这些包都使用C++语言编写，可能会更快些。\n\n除此之外，你需要将你的问题描述清楚，并使用搜索引擎（现在用AI啦😊）搜索。同时，你要广泛地阅读相关书籍，积攒的专业知识有助于你更快速的检索并理解答案。将自己解决问题的过程和最终答案记录下来，长时间的积累后，可以使用某些工具进行构建自己的知识库以便日后查阅。",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#doing-as-little-as-possible",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#doing-as-little-as-possible",
    "title": "24 Improving performance",
    "section": "Doing as little as possible",
    "text": "Doing as little as possible\n尽可能将函数的功能限定在某个范围内，接受特定的输入，输出特定的结果。例如：\n\nrowSums()，colSums()，rowMeans()，colMeans()要比应用apply()来计算快很多。\nvapply()比sapply()更快，因为它规定了输出的类型。\nany(x == 10)要比10 %in% x更快。\n\n某些函数的输入要求特定类型，当输入不符合时，函数可能会执行额外的类型转换工作。例如，应用apply()到data.frame时，会自动将data.frame转换为matrix。\n如果提供更多问题的信息，某些函数会减少一些工作量。例如：\n\nread.csv()中使用colClasses指定已知列类型。\nfactor()使用levels参数指定已知因子级别。\ncut()设置labels = FALSE可以避免产生标签。\nunlist(x, use.names = FALSE)要比unlist(x)更快。\ninteraction()设置drop = TRUE可以丢掉不必要的因子水平。\n\n下面以mean()和as.data.frame()为例，展示如何使用这种策略来提高性能。\nmean()\n由于R的大多数函数使用了S3或S4面向对象，因此，我们可以通过避免方法派发来提高性能。这在一个大型循环任务中会很有效。\n\nS3，可以直接调用generic.class()函数。\nS4，需要使用selectMethod()函数获取方法，然后赋值给环境变量进行调用。\n\n例如，mean.default()计算小型数值向量时要比mean()快上些：\n\nx &lt;- runif(1e2)\n\nbench::mark(\n  mean(x),\n  mean.default(x)\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 2 × 4\n#&gt;   expression           min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 mean(x)            3.9µs    4.3µs   202369.\n#&gt; 2 mean.default(x)    1.4µs    1.8µs   499333.\n\n这种优化方式存在一定风险，当x不是数值向量时，mean.default()会报错。你甚至可以直接调用.Internal()函数来极大的提升性能，同时有也将引入更大的风险——无法对NA值进行处理。\n\nx &lt;- runif(1e2)\nbench::mark(\n  mean(x),\n  mean.default(x),\n  .Internal(mean(x))\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 3 × 4\n#&gt;   expression              min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt;         &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 mean(x)               3.9µs    4.2µs   202416.\n#&gt; 2 mean.default(x)       1.4µs    1.7µs   543593.\n#&gt; 3 .Internal(mean(x))    100ns    200ns  3989468.\n\n注意：这些差异之所以出现，是因为x很小。如果你增加大小，这些差异基本上就会消失，因为大部分时间都用在计算平均值上，而不是进行方法派发。这很好地提醒了我们，输入的大小很重要，你应该根据真实的数据来进行优化。\n\nx &lt;- runif(1e5)\nbench::mark(\n  mean(x),\n  mean.default(x),\n  .Internal(mean(x))\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 3 × 4\n#&gt;   expression              min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt;         &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 mean(x)               136µs    137µs     6607.\n#&gt; 2 mean.default(x)       132µs    142µs     6712.\n#&gt; 3 .Internal(mean(x))    131µs    144µs     6567.\n\nas.data.frame()\n能够确定输入的数据类型是另外一种加开代码运行的方式。例如，as.data.frame()的转换过程分两步，先将每个元素强制转换为数据框，然后再使用rbind()将结果拼接起来。如果你已经知道列表有name属性且元素等长，那么你可以直接将其转换为数据框（R中的所有数据结构都是向量，只是属性class不同）：\n\nquickdf &lt;- function(l) {\n  class(l) &lt;- \"data.frame\"\n  attr(l, \"row.names\") &lt;- .set_row_names(length(l[[1]]))\n  l\n}\n\nl &lt;- lapply(1:26, function(i) runif(1e3))\nnames(l) &lt;- letters\n\nbench::mark(\n  as.data.frame = as.data.frame(l),\n  quick_df      = quickdf(l)\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 2 × 4\n#&gt;   expression         min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt;    &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 as.data.frame   1.04ms   1.11ms      843.\n#&gt; 2 quick_df           6µs    6.7µs   129486.\n\n当然，这种快速的方法牺牲的是对结果正确性的保证。如果你的输入错误，那么你将会得到错误的结果：\n\nquickdf(list(x = 1, y = 1:2))\n#&gt; Warning in format.data.frame(if (omit) x[seq_len(n0), , drop = FALSE] else\n#&gt; x, : corrupt data frame: columns will be truncated or padded with NAs\n#&gt;   x y\n#&gt; 1 1 1\n\n为了得到这个最小化方法，作者仔细阅读并重写了as.data.frame.list()和data.frame()的源代码，并做了许多小的修改，每次都检查是否破坏了现有的行为；经过几个小时的工作，能够分离出上面显示的最小化代码。这是一种非常有用的技术：大多数base R函数是为了灵活性和功能性而编写的，而不是为了性能。因此，根据特定需求重写通常可以带来显著的改进。要做到这一点，需要阅读源代码，它可能很复杂和令人困惑，但不要放弃！",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#vectorise",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#vectorise",
    "title": "24 Improving performance",
    "section": "Vectorise",
    "text": "Vectorise\n如果你使用过R一段时间，你可能听说过这样的话——“向量化你的代码”。但是究竟什么是“向量化”呢？“向量化”不仅仅只是避免使用for循环，而是一种整体化解决问题的思路，即你要处理的是一个向量，而不是向量中的每个标量。一个“向量化”的函数通常有两个关键特点：\n\n简化了问题逻辑：从“逐个处理”到“整体处理”。\n提升了运行速度：底层使用C语言而非R。\n\n在实践中，除了使用map()或lapply()来实现“向量化”，也可以使用已经“向量化”的函数。base R提供了许多已经“向量化”的函数：\n\n\nrowSums(),colSums(),rowMeans(),colMeans()：可以使用它们构建新的“向量化”函数：\nrowAny &lt;- function(x) rowSums(x) &gt; 0\nrowAll &lt;- function(x) rowSums(x) == ncol(x)\n\n向量化提取自己可以极大地提升运行速度（见4.5节）：可以一步进行提取赋值多个值，例如，当x是向量、矩阵、数据框时，x[is.na(x)] &lt;- 0会替换所有缺失值为0。\n可以是使用cut和findInterval()函数来将连续变量离散化。\n其他“向量化”函数，如cumsum()和diff()。\n\n线性代数的运行通常是向量化的，它们的循环使用了外部库，如BLAS。如果你的问题可以使用线性代数来解决，那么运行速度通常会很快。\n“向量化”的缺点是：很难预测性能，无法简单地进行线性估算。如下例，查询100个字符的运行时间并不是处理单个字符的100倍运行时间，而仅是10倍。这背后的逻辑是：“向量化”会动态的切换策略——操作量高于某个阈值时，会采用耗时的“初始化”+不耗时的“处理”策略。\n\nlookup &lt;- setNames(as.list(sample(100, 26)), letters)\n\nx1 &lt;- \"j\"\nx10 &lt;- sample(letters, 10)\nx100 &lt;- sample(letters, 100, replace = TRUE)\n\nbench::mark(\n  lookup[x1],\n  lookup[x10],\n  lookup[x100],\n  check = FALSE\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 3 × 4\n#&gt;   expression        min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt;   &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 lookup[x1]      300ns    500ns  1875223.\n#&gt; 2 lookup[x10]       1µs    1.2µs   766783.\n#&gt; 3 lookup[x100]    2.4µs    3.5µs   246153.\n\n向量化并不能解决所有问题，而且与其费力地将现有算法强行改成使用向量化的方法，不如使用C++ 编写自己的向量化函数。我们将在第25章学习如何做到这一点。",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#avoiding-copies",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#avoiding-copies",
    "title": "24 Improving performance",
    "section": "Avoiding copies",
    "text": "Avoiding copies\nR 代码运行缓慢的一个究极原因是在for循环中不断创建额外的对象。当你使用c()，append()，cbind()，rbind()，paste()组合创建新的对象时，R必须首先创建一个新的对象，然后将旧对象的内容复制到新的对象中。当你在for循环中使用这些函数时，就会不断地创建额外对象。\n下面是一个示例：collapse()函数使用for循环将多个字符串连接成一个字符串；对比直接使用paste()函数中的参数collapse。\n\nrandom_string &lt;- function() {\n  paste(sample(letters, 50, replace = TRUE), collapse = \"\")\n}\nstrings10 &lt;- replicate(10, random_string())\nstrings100 &lt;- replicate(100, random_string())\n\ncollapse &lt;- function(xs) {\n  out &lt;- \"\"\n  for (x in xs) {\n    out &lt;- paste0(out, x)\n  }\n  out\n}\n\nbench::mark(\n  loop10 = collapse(strings10),\n  loop100 = collapse(strings100),\n  vec10 = paste(strings10, collapse = \"\"),\n  vec100 = paste(strings100, collapse = \"\"),\n  check = FALSE\n)[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 4 × 4\n#&gt;   expression      min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 loop10       21.2µs   24.5µs    36165.\n#&gt; 2 loop100       569µs  590.6µs     1561.\n#&gt; 3 vec10         4.1µs    4.6µs   196385.\n#&gt; 4 vec100         24µs   26.7µs    35106.\n\n因为“修改后复制”的机制，x[i] &lt;- y也会触发复制，详见第2章。",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#case-study-t-test",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#case-study-t-test",
    "title": "24 Improving performance",
    "section": "Case study: t-test",
    "text": "Case study: t-test\n下面，我们使用上述介绍的方法来加快“t-test”中t统计量的批量计算。\n假设我们有1000次实验（行），每次实验有50个样本（列），前25个样本为一组，后25个样本为另一组，生成测试数据：\n\nm &lt;- 1000\nn &lt;- 50\nX &lt;- matrix(rnorm(m * n, mean = 10, sd = 3), nrow = m)\ngrp &lt;- rep(1:2, each = n / 2)\n\n有两种方法来批量计算t-test的t统计量：\n\nsystem.time(\n  for (i in 1:m) {\n    t.test(X[i, ] ~ grp)$statistic\n  }\n)\n#&gt;    user  system elapsed \n#&gt;    0.53    0.00    0.53\nsystem.time(\n  for (i in 1:m) {\n    t.test(X[i, grp == 1], X[i, grp == 2])$statistic\n  }\n)\n#&gt;    user  system elapsed \n#&gt;    0.13    0.00    0.13\n\n当然，我们也可以使用map_dbl()来批量计算：\n\ncompT &lt;- function(i) {\n  t.test(X[i, grp == 1], X[i, grp == 2])$statistic\n}\nsystem.time(t1 &lt;- purrr::map_dbl(1:m, compT))\n#&gt;    user  system elapsed \n#&gt;    0.16    0.00    0.16\n\n首先我们可以使用减少函数额外工作的策略优化，查看stats::t.test.default()的源码，你会发现它不仅计算了t统计量，还计算了p值和打印输出。我们可以只计算t统计量：\n\nmy_t &lt;- function(x, grp) {\n  t_stat &lt;- function(x) {\n    m &lt;- mean(x)\n    n &lt;- length(x)\n    var &lt;- sum((x - m)^2) / (n - 1)\n\n    list(m = m, n = n, var = var)\n  }\n\n  g1 &lt;- t_stat(x[grp == 1])\n  g2 &lt;- t_stat(x[grp == 2])\n\n  se_total &lt;- sqrt(g1$var / g1$n + g2$var / g2$n)\n  (g1$m - g2$m) / se_total\n}\n\nsystem.time(t2 &lt;- purrr::map_dbl(1:m, ~ my_t(X[., ], grp)))\n#&gt;    user  system elapsed \n#&gt;    0.03    0.00    0.03\nstopifnot(all.equal(t1, t2))\n\n针对上面计的for循环计算策略，我们可以使用向量化函数来优化。\n\nrowtstat &lt;- function(X, grp) {\n  t_stat &lt;- function(X) {\n    m &lt;- rowMeans(X)\n    n &lt;- ncol(X)\n    var &lt;- rowSums((X - m)^2) / (n - 1)\n\n    list(m = m, n = n, var = var)\n  }\n\n  g1 &lt;- t_stat(X[, grp == 1])\n  g2 &lt;- t_stat(X[, grp == 2])\n\n  se_total &lt;- sqrt(g1$var / g1$n + g2$var / g2$n)\n  (g1$m - g2$m) / se_total\n}\nsystem.time(t3 &lt;- rowtstat(X, grp))\n#&gt;    user  system elapsed \n#&gt;    0.02    0.00    0.01\nstopifnot(all.equal(t1, t3))",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/24 Improving performance.html#other-techniques",
    "href": "Books/Advanced R(2e)/24 Improving performance.html#other-techniques",
    "title": "24 Improving performance",
    "section": "Other techniques",
    "text": "Other techniques\n写出运行速度快的代码是成为优秀程序员的一部分。除了本章介绍的策略外，你可以通过下面的方式来提升自己的通用编程技巧：\n\n浏览R blogs，了解其他人如何解决性能问题。\n阅读其他R编程数据，如”The Art of R Programming”，“R Inferno”。\n参加一门算法和数据结构课程，学习一些众所周知的方法来解决某些类型的问题。\n学习如何并行地运行代码，如”Parallel R”和”Parallel Computing for Data Science”。\n阅读通用的有关优化的数据，如”Mature optimisation”，“Pragmatic Programmer”。",
    "crumbs": [
      "24 Improving performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html",
    "href": "Books/Advanced R(2e)/22 Debugging.html",
    "title": "22 Debugging",
    "section": "",
    "text": "当R代码运行时抛出错误，你会怎样处理？你会使用什么工具来检测和解决？本章就上面两点，先介绍通用的错误处理方法，然后介绍一些特殊的工具。\n注意：在编写新函数时，你不应该使用这些工具。如果你发现自己经常在新代码中使用这些工具，请重新考虑你的方法。与其试图一次性编写一个大函数，不如交互式地分段编写。如果从小处着手，你可以快速确定为什么某些事情不能正常工作，而不需要复杂的调试工具。\n\n\n22.2节：介绍找到和解决bug的通用策略。\n22.3节：介绍traceback()，锁定错误的调用栈。\n22.4节：介绍中断函数执行，并交互探索具体发生的事件。\n22.5节：讨论非交互状态下的调试。\n22.6节：讨论一些偶尔也需要调试的非错误问题。",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#introduction",
    "href": "Books/Advanced R(2e)/22 Debugging.html#introduction",
    "title": "22 Debugging",
    "section": "",
    "text": "当R代码运行时抛出错误，你会怎样处理？你会使用什么工具来检测和解决？本章就上面两点，先介绍通用的错误处理方法，然后介绍一些特殊的工具。\n注意：在编写新函数时，你不应该使用这些工具。如果你发现自己经常在新代码中使用这些工具，请重新考虑你的方法。与其试图一次性编写一个大函数，不如交互式地分段编写。如果从小处着手，你可以快速确定为什么某些事情不能正常工作，而不需要复杂的调试工具。\n\n\n22.2节：介绍找到和解决bug的通用策略。\n22.3节：介绍traceback()，锁定错误的调用栈。\n22.4节：介绍中断函数执行，并交互探索具体发生的事件。\n22.5节：讨论非交互状态下的调试。\n22.6节：讨论一些偶尔也需要调试的非错误问题。",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#overall-approach",
    "href": "Books/Advanced R(2e)/22 Debugging.html#overall-approach",
    "title": "22 Debugging",
    "section": "Overall approach",
    "text": "Overall approach\n\n\n\n\n\n\nTip\n\n\n\nFinding your bug is a process of confirming the many things that you believe are true — until you find one which is not true.\n—Norm Matloff\n\n\n找到问题的根本原因总是具有挑战性。大多数bug都很微妙且难以发现，因为如果它们很明显，你一开始就会避开它们。一个好的策略会有所帮助。以下是我发现有用的四步流程概述：\n\n\nGoogle!\n无论何时看到错误消息，都可以先在浏览器上搜索它（现在用AI啦）。如果你幸运的话，你会发现这是一个已知解决方案中常见的错误。在搜索时，通过删除任何与你的问题特定的变量名称或值来提高匹配成功的概率。\n可以使用“errorist”和“searcher”包自动执行这个过程，参阅它们的网站以了解更多详细信息。\n\n\nMake it repeatable\n为了找到错误的根本原因，你需要在考虑和拒绝假设时多次执行代码。为了使迭代尽可能快速，值得进行一些前期投资，使问题既容易又快速地重现。\n首先创建一个可重复的示例 (参见 1.7 节)。接下来，通过删除代码和简化数据来使示例最小化。在这个过程中，你可能会发现一些不会触发错误的输入。请记住这些输入：它们将有助于诊断根本原因。\n如果你正在使用自动化测试，这也是创建自动化测试用例的好时机。如果你现有的测试覆盖率较低，可以趁机添加一些附近的测试，以确保保留现有的良好行为。这样可以降低创建新 bug 的可能性。\n\n\nFigure out where it is\n如果你幸运的话，下一节中的一个工具将帮助你快速识别导致 bug 的代码行。然而，通常情况下，你需要对问题进行更多的思考。采用科学的方法是一个很好的主意。生成假设，设计实验来测试它们，并记录你的结果。这可能看起来是很多工作，但系统的方法最终会节省你的时间。我经常浪费大量时间依靠直觉来解决 bug (“哦，这一定是一个差一错误，所以我就在这里减去 1”), 而实际上我采用系统的方法会更好。\n如果这个方法失败了，你可能需要向其他人寻求帮助。如果你遵循了上一步，你就会有一个容易与他人分享的小例子。这会让其他人更容易看到问题，也更有可能帮助你找到解决方案。\n\n\nFix it and test it\n一旦你发现了 bug, 你需要弄清楚如何修复它，并检查修复是否真的有效。同样，有自动化测试非常有用。这不仅有助于确保你确实修复了 bug, 还有助于确保你在过程中没有引入任何新的 bug。在没有自动化测试的情况下，请确保仔细记录正确的输出，并检查之前失败的输入。",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#locating-errors",
    "href": "Books/Advanced R(2e)/22 Debugging.html#locating-errors",
    "title": "22 Debugging",
    "section": "Locating errors",
    "text": "Locating errors\n一旦你使错误可重复，下一步就是找出它的来源。在这个过程中，最重要的工具是traceback(), 它向你显示导致错误的调用序列 (也称为调用栈，见 7.5 节)。\n下面是一个简单的例子。你可以看到 f() 调用 g() 调用 h() 调用 i(), 后者检查其参数是否为数值：\n\nf &lt;- function(a) g(a)\ng &lt;- function(b) h(b)\nh &lt;- function(c) i(c)\ni &lt;- function(d) {\n  if (!is.numeric(d)) {\n    stop(\"`d` must be numeric\", call. = FALSE)\n  }\n  d + 10\n}\n\nf(\"a\")\n#&gt; Error: `d` must be numeric\n\n当你在Rstudio中运行f(\"a\")时，你会看到：\n\n错误信息的右侧出现两个选项：“Show Traceback”和“Rerun with Debug”，如果你点击“Show Traceback”，你可以看到：\n\n如果你没有使用Rstudio，你可以使用traceback()来查看这些信息。\ntraceback()\n#&gt; 5: stop(\"`d` must be numeric\", call. = FALSE) at #3\n#&gt; 4: i(c) at #1\n#&gt; 3: h(b) at #1\n#&gt; 2: g(a) at #1\n#&gt; 1: f(\"a\")\n\n\n\n\n\n\nNote\n\n\n\n注意：如果你使用source()加载的函数，调用traceback()后会显示错误的具体未知，filename.r#linenumber。\n\n\nLazy evaluation\ntraceback()的一个缺点是始终线性展示调用栈，如果调用过程包含大量的惰性评估，就会显得混乱。例如，在f()内惰性评估j()产生的调用栈：\n\nj &lt;- function() k()\nk &lt;- function() stop(\"Oops!\", call. = FALSE)\nf(j())\n#&gt; Error: Oops!\n\ntraceback()\n#&gt; 7: stop(\"Oops!\", call. = FALSE) at #1\n#&gt; 6: k() at #1\n#&gt; 5: j() at #1\n#&gt; 4: i(c) at #1\n#&gt; 3: h(b) at #1\n#&gt; 2: g(a)\n#&gt; 1: f(j())\n可以使用rlang::abort()来替代stop()，使用rlang::last_trace()来替换traceback()。\nk &lt;- function() rlang::abort(\"Oops!\")\nf(j())\n#&gt; rlang::last_trace()\n#&gt; &lt;error/rlang_error&gt;\n#&gt; Error in `k()`:\n#&gt; ! Oops!\n#&gt; ---\n#&gt; Backtrace:\n#&gt;     ▆\n#&gt;  1. ├─global f(j())\n#&gt;  2. │ └─global g(a)\n#&gt;  3. │   └─global h(b)\n#&gt;  4. │     └─global i(c)\n#&gt;  5. └─global j()\n#&gt;  6.   └─global k()\n#&gt; Run rlang::last_trace(drop = FALSE) to see 1 hidden frame.",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#interactive-debugger",
    "href": "Books/Advanced R(2e)/22 Debugging.html#interactive-debugger",
    "title": "22 Debugging",
    "section": "Interactive debugger",
    "text": "Interactive debugger\n对于找到并修复错误，有时错误的位置信息已经足够；但通常需要更多信息，最简单的方法是使用交互式调试器，它允许暂停函数的执行，并交互式地探索其状态。\n如果你正在使用RStudio, 进入交互式调试器最简单的方式是通过RStudio的“Rerun with Debug” 工具，它会重新运行创建错误的命令，暂停发生错误的执行。否则，你需要在暂停的地方插入一个browser()调用，然后重新运行该函数。例如，可以在g()中插入一个browser()调用：\n\ng &lt;- function(b) {\n  browser()\n  h(b)\n}\nf(10)\n#&gt; Called from: g(a)\n#&gt; debug: h(b)\n#&gt; [1] 20\n\nbrowser()是一个常规函数，你可以使用if语句包裹它：\n\ng &lt;- function(b) {\n  if (b &lt; 0) {\n    browser()\n  }\n  h(b)\n}\n\nbrowser()会在终端生成Browse[1]&gt;样式的提示，表示你正在browser()启动的环境中交互。如果你在Rstudio中点击“Rerun with Debug”，会在环境栏中显示当前环境中的对象，在traceback栏中显示调用栈。\n\nbrowser() commands\n除了可以运行常规R代码外，browser()提供了一些特殊的命令，可以直接在终端运行它们的缩写或在Rstudio中工具栏：\n\n\nNext, n：运行函数下一步. 如果你有一个变量名是 n，你需要运行print(n)来展示值。\nStep into,  or s：功能类似next，但是如果下一步运行的是函数，会跳入到该函数中进行交互。\nFinish,  or f：结束当前循环或函数。\nContinue, c：如果你已经修复了错误状态，并想要检查函数是否正常运行，运行它，会离开交互式调试，继续正常执行函数。\nStop, Q：停止调试，终止函数，然后返回全局工作空间。一旦你确定了问题所在，并且准备好修复它并重新加载代码，就可以使用这个方法。\n\n还有两个稍微不那么有用且在工具栏中不可用的命令：\n\nEnter：重复之前的命令。该功能极易激活，使用options(browserNLdisabled = TRUE)可以将其关闭。\nwhere：打印活动调用的堆栈跟踪 (相当于Rstudio交互式 的traceback) 。\nAlternatives\n有三种替代browser()的方法：在Rstudio中设置断点、options(error = recover)、debug()。\nBreakpoints\n在Rstudio中，你可以点击左侧代码行或者使用Shift + F9来设置断点。设置断点相当于你在这行代码中插入了browser()。这样，你可以直观地看到插入的断点，也可以无需在源码中插入browser()来调试。但它有两个小缺点：\n\n有极少的情况下，断点会失效。阅读breakpoint troubleshooting了解更多信息。\n目前Rstudio不支持在条件情况中设置断点。\nrecover()\n设置options(error = recover)后，报错时会自动进入交互式调试。设置options(error = NULL)即可恢复默认行为。\n\noptions(error = recover)\nf(\"x\")\n#&gt; Error: `d` must be numeric\n\ndebug()\n另一种方法是调用一个能插入browser()的函数：\n\ndebug()插入browser()到函数f的开头，undebug()移除，debugonce()仅在下一次调用函数时触发，再次调用消失。\nutils::setBreakpoint()与debug()类似，除函数名外，还可以接受行数、文件名找到恰当的函数。\n\n这两个函数都是trace()的特殊情况，它在现有函数中的任意位置插入任意代码。当调试没有源代码的代码时，trace()偶尔会很有用。要从函数中移除“trace”，可以使用untrace()。每个函数只能执行一个“trace”, 但这一个“trace”可以调用多个函数。\nCall stack\n不幸地是，traceback(), browser() & where, and recover() 产生的调用栈并不一致。下表是对上面f(\"a\")调用栈的总结。\n\n\ntraceback()\nwhere\nrecover()\nrlang functions\n\n\n\n5: stop(\"...\")\n\n\n\n\n\n4: i(c)\nwhere 1: i(c)\n1: f()\n1. └─global::f(10)\n\n\n3: h(b)\nwhere 2: h(b)\n2: g(a)\n2. └─global::g(a)\n\n\n2: g(a)\nwhere 3: g(a)\n3: h(b)\n3. └─global::h(b)\n\n\n1: f(\"a\")\nwhere 4: f(\"a\")\n4: i(\"a\")\n4. └─global::i(\"a\")\n\n\n\n在RStudio展示的调用顺序与traceback()一致；“rlang”函数产生的调用顺序与recover()一致。\nCompiled code\n编译代码 (如 C 或 C++)也可以使用交互式调试器 (gdb 或 lldb) 。遗憾的是，这超出了本书的范围，但有一些资源你可能会发现很有用。\n\nhttp://r-pkgs.had.co.nz/src.html#src-debugging\nhttps://github.com/wch/r-debug/blob/master/debugging-r.md\nhttp://kevinushey.github.io/blog/2015/04/05/debugging-with-valgrind/\nhttps://www.jimhester.com/2018/08/22/debugging-rstudio/",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#non-interactive-debugging",
    "href": "Books/Advanced R(2e)/22 Debugging.html#non-interactive-debugging",
    "title": "22 Debugging",
    "section": "Non-interactive debugging",
    "text": "Non-interactive debugging\n当无法交互式地运行代码时，debug会很困难。典型地如，在远程服务器运行某个流程，或者在交互运行式不会出错。当无法进行交互式探索时，花费时间将问题设置得尽可能小，以便快速迭代会十分重要。例如，\n你需要格外关注下面几个常见的问题：\n\n全局环境是否前后不同，是否加载了不同的包，是否有值不同的变量？\n是否工作目录不同？\n是否PATH环境变量不同，导致调用外部命令时失败？\n是否R_LIBS环境变量不同，导致加载包时失败或版本不一致。\n\ndump.frames()\ndump.frames()等价于recover()。它会在工作目录下储存一个last.dump.rda的文件，你可以使用laod(\"last.dump.rda\")在交互终端中加载文件，接着使用debugger()进入交互环境中进行debug。\n# In batch R process ----\ndump_and_quit &lt;- function() {\n  # Save debugging info to file last.dump.rda\n  dump.frames(to.file = TRUE)\n  # Quit R with error status\n  q(status = 1)\n}\noptions(error = dump_and_quit)\n\n# In a later interactive session ----\nload(\"last.dump.rda\")\ndebugger()\nPrint debugging\n如果dump.frames()不起作用，另一个策略是使用print debugging——在代码中插入一些print来标记代码位置，进而锁定出错的范围。这种方法低效且原始，但往往会有效果。\n\nf &lt;- function(a) {\n  cat(\"f()\\n\")\n  g(a)\n}\ng &lt;- function(b) {\n  cat(\"g()\\n\")\n  cat(\"b =\", b, \"\\n\")\n  h(b)\n}\nh &lt;- function(c) {\n  cat(\"i()\\n\")\n  i(c)\n}\n\nf(10)\n#&gt; f()\n#&gt; g()\n#&gt; b = 10 \n#&gt; i()\n#&gt; [1] 20\n\nprint debugging对于编译的代码也特别有用，因为编译器经常会修改代码，以至于即使在交互式调试器中，也无法找出根本问题。\nRMarkdown\n在RMarkdown文件中进行debug需要一些特殊的工具。首先你需要使用命令行rmarkdown::render(\"path/to/file.Rmd\")来编译文件，这会在当前终端中运行RMarkdown中的代码，使得debug容易些。如果这样做没有产生错误，那么你要检查你得环境，因为肯定不一致。\n如果问题依然存在，你需要使用上面的交互式debug技巧。但无论何种方法，你都需要添加额外的步骤：在错误设置中添加sink()。重新设置sink()会移除knitr的默认”sink”设置，能够确保日志在终端中输出。例如，在RMarkdown中使用recover()：\noptions(error = function() {\n  sink()\n  recover()\n})\n这种设置会生成“no sink to remove”的警告信息，你可以完全忽略它。\n如果你只是想生成调用栈，最简单的方式是使用rlang::trace_back()，并对rlang_trace_top_env设置。它会确保只会返回RMarkdown中代码的调用栈，而不是RMarkdown和knitr的所有调用栈。\noptions(rlang_trace_top_env = rlang::current_env())\noptions(error = function() {\n  sink()\n  print(rlang::trace_back(bottom = sys.frame(-1)), simplify = \"none\")\n})",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/22 Debugging.html#non-error-failures",
    "href": "Books/Advanced R(2e)/22 Debugging.html#non-error-failures",
    "title": "22 Debugging",
    "section": "Non-error failures",
    "text": "Non-error failures\n除了抛出错误之外，还有其他方式导致函数失败：\n\n函数可能会产生意外的警告。追踪警告最简单的方法是options(warn = 2)将其转换为错误，并使用调用栈，例如doWithOneRestart()、withOneRestart()和常规调试工具。当这样做时，你会看到一些额外的restarts()和.signalSimpleWarning()调用。忽略这些：它们是用于将警告转换为错误的内部函数。\n一个函数可能永远不会返回。这特别难以自动调试，但有时终止函数并查看traceback()会提供一些信息。也可以使用打印调试。\n最糟糕的情况是，你的代码可能会完全崩溃，使你无法交互式地调试代码。这表明编译的(C 或 C++)代码中存在bug。\n\n如果是你的编译代码出错，你可以阅读22.4节中的链接，自行debug。如果错误来自base R或某个R包，你需要联系相关管理员进行维护。",
    "crumbs": [
      "22 Debugging"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html",
    "href": "Books/Advanced R(2e)/20 Evaluation.html",
    "title": "20 Evaluation",
    "section": "",
    "text": "“引用”有两个反面——“解引用”和“评估”。通常，“解引用”面向使用者，赋予了使用者选择性评估“表达式”的能力；而“评估”面向开发者，赋予了开发者在自定义环境中评估“表达式”的能力。\n本章从最纯粹的评估模式开始讨论，介绍eval()如何在环境中评估一个表达式，及如何使用它实现许多重要的base R 函数。然后延申评估，介绍两种重要思想：\n\nQuoseure: 一种用于捕获表达式及其关联环境的数据结构。\n数据掩码：使在“数据框环境”中评估表达式更加容易。\n\n总之，准引用、quosure、数据掩码共同构成了我们所说的整洁评估（tidy-eval）。整洁评估为非标准评估提供了一种原则性的方法，使得可以交互使用这些函数并将其与其他函数进行嵌套。整洁评估是所有这些理论中最重要的实际含义，因此将花一些时间来探讨它们的含义。本章最后讨论了base R的最相关方法，以及如何围绕它们的缺点进行编程。\n\n\n\n20.2节：介绍base::eval()函数，及如何使用它实现local()和source()。\n20.3节：介绍quosure数据结构，及如何生成与评估它。\n20.4节：介绍数据掩码和避免歧义的声明。\n20.5节：介绍使用整洁评估的实例。\n20.6节：介绍base R中的非标准性评估及其缺陷。\n\n\n\n\n要求熟悉前两章内容和第7章有关环境的内容。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#introduction",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#introduction",
    "title": "20 Evaluation",
    "section": "",
    "text": "“引用”有两个反面——“解引用”和“评估”。通常，“解引用”面向使用者，赋予了使用者选择性评估“表达式”的能力；而“评估”面向开发者，赋予了开发者在自定义环境中评估“表达式”的能力。\n本章从最纯粹的评估模式开始讨论，介绍eval()如何在环境中评估一个表达式，及如何使用它实现许多重要的base R 函数。然后延申评估，介绍两种重要思想：\n\nQuoseure: 一种用于捕获表达式及其关联环境的数据结构。\n数据掩码：使在“数据框环境”中评估表达式更加容易。\n\n总之，准引用、quosure、数据掩码共同构成了我们所说的整洁评估（tidy-eval）。整洁评估为非标准评估提供了一种原则性的方法，使得可以交互使用这些函数并将其与其他函数进行嵌套。整洁评估是所有这些理论中最重要的实际含义，因此将花一些时间来探讨它们的含义。本章最后讨论了base R的最相关方法，以及如何围绕它们的缺点进行编程。\n\n\n\n20.2节：介绍base::eval()函数，及如何使用它实现local()和source()。\n20.3节：介绍quosure数据结构，及如何生成与评估它。\n20.4节：介绍数据掩码和避免歧义的声明。\n20.5节：介绍使用整洁评估的实例。\n20.6节：介绍base R中的非标准性评估及其缺陷。\n\n\n\n\n要求熟悉前两章内容和第7章有关环境的内容。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#evaluation-basics",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#evaluation-basics",
    "title": "20 Evaluation",
    "section": "Evaluation basics",
    "text": "Evaluation basics\neval()函数有两个参数：epxr，env。\nexpr参数是待评估的“表达式”或符号。由于eval()函数不会对输入引用，所以需要与expr()一同使用：\n\nx &lt;- 10\neval(expr(x))\n#&gt; [1] 10\n\ny &lt;- 2\neval(expr(x + y))\n#&gt; [1] 12\n\nenv参数用来指定评估“表达式”的“环境”，如果没有指定，则默认为当前环境。\n\neval(expr(x + y), env(x = 1000))\n#&gt; [1] 1002\n\n当指定了环境，却没有引用输入时，会导致错误的结果：\n\neval(print(x + 1), env(x = 1000))\n#&gt; [1] 11\n#&gt; [1] 11\n\neval(expr(print(x + 1)), env(x = 1000))\n#&gt; [1] 1001\n\n在了解了基础知识后，让我们探索一些应用，根据底层原理重新实现base R中的某些函数。\n\nApplication:local()\n有时我们会创建一些临时变量来执行一系列计算，这些临时变量不会长期使用，可能也会相当占用内存，需要在使用结束后删除。一种方法是使用rm()清楚临时变量；另一种是将计算过程打包为函数，仅调用一次。更优雅的方式是使用local()函数，它可以创建一个临时环境，并执行其中的代码。\n\n# Clean up variables created earlier\nrm(x, y)\n\nfoo &lt;- local({\n  x &lt;- 10\n  y &lt;- 200\n  x + y\n})\n\nfoo\n#&gt; [1] 210\nx\n#&gt; Error: object 'x' not found\ny\n#&gt; Error: object 'y' not found\n\nlocal()函数的本质很简单，我们可以采用下面的策略实现它。首先捕获输入的“表达式”，然后使用local()函数的执行环境作为eval()的调用环境参与评估。\n\nlocal2 &lt;- function(expr) {\n  env &lt;- env(caller_env())\n  eval(enexpr(expr), env)\n}\n\nfoo &lt;- local2({\n  x &lt;- 10\n  y &lt;- 200\n  x + y\n})\n\nfoo\n#&gt; [1] 210\nx\n#&gt; Error: object 'x' not found\ny\n#&gt; Error: object 'y' not found\n\n但base::local()的底层实现很复杂，它使用了eval()和substitute()。\n\n\nApplication:source()\n我们可以通过组合eval()和parse_expr()来实现source()的功能。首先从磁盘中读取文件，然后使用parse_expr()将字符串转换成“表达式”列表，最后使用eval()评估“表达式”。实现如下：\n\nsource2 &lt;- function(path, env = caller_env()) {\n  file &lt;- paste(readLines(path, warn = FALSE), collapse = \"\\n\")\n  exprs &lt;- parse_exprs(file)\n\n  res &lt;- NULL\n  for (i in seq_along(exprs)) {\n    res &lt;- eval(exprs[[i]], env)\n  }\n\n  invisible(res)\n}\n\n真实的base::source()函数更加复杂，会打印输入输出信息，同时有许多额外参数控制行为。\n\nExpression vectors\n上一章讲到，base::parse()函数解析字符串时，如果捕获到多个“表达式”，会返回一个包含多个表达式的向量。base::eval()函数可以直接评估这个向量，而不用上面的for循环。\n\nsource3 &lt;- function(file, env = parent.frame()) {\n  lines &lt;- parse(file)\n  res &lt;- eval(lines, envir = env)\n  invisible(res)\n}\n\n\n\n\nGotcha:function()\n如果你使用eval()和expr()来生成函数，有一个小小的漏洞需要注意：\n\nx &lt;- 10\ny &lt;- 20\nf &lt;- eval(expr(function(x, y) !!x + !!y))\nf\n#&gt; function (x, y) \n#&gt; 10 + 20\n\n这个函数看起来不像能正常运行，其实可以：\n\nf()\n#&gt; [1] 30\n\n这是因为，如果函数有“srcref”属性，就会打印它，但“srcref”是一个base R的特性，它无法识别准引用。\n要解决这个问题，可以使用new_function()或删除“srcref”属性：\n\nattr(f, \"srcref\") &lt;- NULL\nf\n#&gt; function (x, y) \n#&gt; 10 + 20",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#quosures",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#quosures",
    "title": "20 Evaluation",
    "section": "Quosures",
    "text": "Quosures\n几乎eval()的所有使用都包括“表达式”和“环境”两个参数，但是base R中没有能同时提供这两个参数的数据结构，“rlang”包创建了这种数据结构——“quosures”。quosures是“quoting”和“closure”的复合体，意味着它同时包含了“表达式”和环境。\n在本节中，你将学习如何创建和操作quosure, 以及一些关于如何实现它。\n\nCreating\n有三中方式创建quosure:\n\n使用enquo()和enquos()，它们会同时捕获表达式和环境。许多quosure都是由此创建的。\n\n\nfoo &lt;- function(x) enquo(x)\nfoo(a + b)\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^a + b\n#&gt; env:  global\n\n\n使用quo()和quos()，与enquo()和enquos()的关系可以参考expr()``enexpr()。使用的场景很少。\n\n\nquo(x + y + z)\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^x + y + z\n#&gt; env:  global\n\n\n使用new_quosure()，输入“表达式”和环境来创建quosure。使用场景也极少。\n\n\nnew_quosure(expr(x + y), env(x = 1, y = 10))\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^x + y\n#&gt; env:  0x000001c76004dd30\n\n\n\nEvaluting\n只能使用eval_tidy()来评估quosure。\n\nq1 &lt;- new_quosure(expr(x + y), env(x = 1, y = 10))\neval_tidy(q1)\n#&gt; [1] 11\n\n\n\nDots\nenquos()可以正确识别...中传入的参数及其绑定的环境。例如，下面的qs对象，正确评估了global和f的环境。\n\nf &lt;- function(...) {\n  x &lt;- 1\n  g(..., f = x)\n}\ng &lt;- function(...) {\n  enquos(...)\n}\n\nx &lt;- 0\nqs &lt;- f(global = x)\nqs\n#&gt; &lt;list_of&lt;quosure&gt;&gt;\n#&gt; \n#&gt; $global\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^x\n#&gt; env:  global\n#&gt; \n#&gt; $f\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^x\n#&gt; env:  0x000001c75ec5b668\nmap_dbl(qs, eval_tidy)\n#&gt; global      f \n#&gt;      0      1\n\n\n\nUnder the hood\nQuosures 数据结构受R中的“formulas”启发，因为“formula”同样也是同时捕获“表达式”与“环境”。早期也确实使用“formula”来进行评估，但因为无法简单的将~变为准引用函数，所以放弃使用“formula”。\n\nf &lt;- ~ runif(3)\nstr(f)\n#&gt; Class 'formula'  language ~runif(3)\n#&gt;   ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt;\n\nQuosures 同样也是“formula”的子类：\n\nq4 &lt;- new_quosure(expr(x + y + z))\nclass(q4)\n#&gt; [1] \"quosure\" \"formula\"\n\n这意味着一些函数可以直接作用于Quosures：\n\nis_call(q4)\n#&gt; [1] TRUE\n\nq4[[1]]\n#&gt; Warning: Subsetting quosures with `[[` is deprecated as of rlang 0.4.0\n#&gt; Please use `quo_get_expr()` instead.\n#&gt; This warning is displayed once every 8 hours.\n#&gt; `~`\nq4[[2]]\n#&gt; x + y + z\n\n有一个用于存放环境的属性：\n\nattr(q4, \".Environment\")\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n但是不建议使用上面的函数，而是使用get_expr()和get_env()来获取表达式和环境：\n\nget_expr(q4)\n#&gt; x + y + z\nget_env(q4)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n\n\nNested quosures\n准引用支持在“表达式”中引入quosures，这是一种高级技术，使得创建嵌套quosures变得可能。例如下面的“表达式”中嵌套了两个短语。\n\nq2 &lt;- new_quosure(expr(x), env(x = 1))\nq3 &lt;- new_quosure(expr(x), env(x = 10))\n\nx &lt;- expr(!!q2 + !!q3)\n\n它可以被正确地评估，但是如果打印它，你会发现它的“formula”形式：\n\neval_tidy(x)\n#&gt; [1] 11\nx\n#&gt; (~x) + ~x\n\n可以使用rlang::expr_print()来更好的展示，在终端中根据不同环境源显示不同颜色：\n\nexpr_print(x)\n#&gt; (^x) + (^x)",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#data-masks",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#data-masks",
    "title": "20 Evaluation",
    "section": "Data masks",
    "text": "Data masks\n本节介绍数据掩码（data mask）相关内容，这是一种同时在“环境”与“数据框构成的环境”中评估“表达式”的技术。它的核心思想与base R中的with(),subset()和transform()类似，被广泛应用在“tidyverse”系列包中。\n\n\n\n\n\n\nNote\n\n\n\n注意：enquo()保证了能在不同环境中正确评估“表达式”中的变量，expr()不能，这是一个重要的区别。但是本节所有示例中的enquo()与expr()都可以替换，不影响结果。\n\n\n\nBasics\n数据掩码允许你混合环境来源和数据框来源的变量。你可以将数据框当作环境变量传递给eval_tidy()的第二个参数。\n\nq1 &lt;- new_quosure(expr(x * y), env(x = 100))\ndf &lt;- data.frame(y = 1:10)\n\neval_tidy(q1, df)\n#&gt;  [1]  100  200  300  400  500  600  700  800  900 1000\n\n上面的代码可能有些难以理解，我们可以作一些拆分：\n\nx &lt;- 100\ndf &lt;- data.frame(y = 1:10)\neval_tidy(expr(x * y), df)\n#&gt;  [1]  100  200  300  400  500  600  700  800  900 1000\n\n稍加修改，改写为类似base::with()的函数：\n\nwith2 &lt;- function(data, expr) {\n  expr &lt;- enquo(expr)\n  eval_tidy(expr, data)\n}\n\nwith2(df, x * y)\n#&gt;  [1]  100  200  300  400  500  600  700  800  900 1000\n\nbase::eval()可以实现类似的效果，传递数据框到第二个参数，环境到第三个参数。\n\nwith3 &lt;- function(data, expr) {\n  expr &lt;- substitute(expr)\n  eval(expr, data, caller_env())\n}\n\n\n\nPronouns\n数据掩码会引起歧义。例如，在以下代码中，除非你知道df中包含哪些变量，否则你无法知道x是来自数据掩码还是环境。\n\nwith2(df, x)\n#&gt; [1] 100\n\n为了解决歧义问题，数据掩码提供了两个声明：.data和.env。\n\n.data$x 表示数据掩码中的变量x。\n.env$x 表示环境中的变量x。\n\n\nx &lt;- 1\ndf &lt;- data.frame(x = 2)\n\nwith2(df, .data$x)\n#&gt; [1] 2\nwith2(df, .env$x)\n#&gt; [1] 1\n\n对于两个声明，你可以使用[[，但是要注意它们是特殊的对象，和真实的数据框、环境不同。例如，如果找不到变量，它会抛出错误：\n\ndf$y\n#&gt; NULL\nwith2(df, .data$y)\n#&gt; Error in `.data$y`:\n#&gt; ! Column `y` not found in `.data`.\n\n\n\nApplication: subset()\n下面是subset()的使用场景之一：直接通过某个“表达式”进行过滤数据框的行。\n\nsample_df &lt;- data.frame(a = 1:5, b = 5:1, c = c(5, 3, 1, 4, 1))\n\n# Shorthand for sample_df[sample_df$a &gt;= 4, ]\nsubset(sample_df, a &gt;= 4)\n#&gt;   a b c\n#&gt; 4 4 2 4\n#&gt; 5 5 1 1\n\n# Shorthand for sample_df[sample_df$b == sample_df$c, ]\nsubset(sample_df, b == c)\n#&gt;   a b c\n#&gt; 1 1 5 5\n#&gt; 5 5 1 1\n\nsubset()的核心逻辑是：\n\n两个参数：数据框data和“表达式”rows。\n在数据框data中，评估rows，并返回结果逻辑向量。\n根据逻辑向量，返回数据框的行。\n\n\nsubset2 &lt;- function(data, rows) {\n  rows &lt;- enquo(rows)\n  rows_val &lt;- eval_tidy(rows, data)\n  stopifnot(is.logical(rows_val))\n\n  data[rows_val, , drop = FALSE]\n}\n\nsubset2(sample_df, a &gt;= 4)\n#&gt;   a b c\n#&gt; 4 4 2 4\n#&gt; 5 5 1 1\n\n\n\nApplication: transform()\ntransform()函数类似dplyr::mutate()，可以在数据框中添加新的一列。\n\ndf &lt;- data.frame(x = c(2, 3, 1), y = runif(3))\ntransform(df, x = -x, y2 = 2 * y)\n#&gt;    x         y        y2\n#&gt; 1 -2 0.9811686 1.9623372\n#&gt; 2 -3 0.4927797 0.9855595\n#&gt; 3 -1 0.2881747 0.5763493\n\n下面是transform()的简单等价实现：\n\ntransform2 &lt;- function(.data, ...) {\n  dots &lt;- enquos(...)\n\n  for (i in seq_along(dots)) {\n    name &lt;- names(dots)[[i]]\n    dot &lt;- dots[[i]]\n\n    .data[[name]] &lt;- eval_tidy(dot, .data)\n  }\n\n  .data\n}\n\ntransform2(df, x2 = x * 2, y = -y)\n#&gt;   x          y x2\n#&gt; 1 2 -0.9811686  4\n#&gt; 2 3 -0.4927797  6\n#&gt; 3 1 -0.2881747  2\n\n\n\nApplication: select()\n数据掩码不总是作用于数据框，也可以是list。这是subset()的另一个使用场景——根据“表达式”选择某些列——的底层逻辑。\n\ndf &lt;- data.frame(a = 1, b = 2, c = 3, d = 4, e = 5)\nsubset(df, select = b:d)\n#&gt;   b c d\n#&gt; 1 2 3 4\n\n它的关键思想是创建一个有name属性的list，list的每个元素是对应列的位置索引。\n\nvars &lt;- as.list(set_names(seq_along(df), names(df)))\nstr(vars)\n#&gt; List of 5\n#&gt;  $ a: int 1\n#&gt;  $ b: int 2\n#&gt;  $ c: int 3\n#&gt;  $ d: int 4\n#&gt;  $ e: int 5\n\n然后在list中进行评估，返回位置索引：\n\nselect2 &lt;- function(.data, ...) {\n  dots &lt;- enquos(...)\n\n  vars &lt;- as.list(set_names(seq_along(.data), names(.data)))\n  cols &lt;- unlist(map(dots, eval_tidy, vars))\n\n  .data[, cols, drop = FALSE]\n}\nselect2(df, b:d)\n#&gt;   b c d\n#&gt; 1 2 3 4",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#using-tidy-evaluation",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#using-tidy-evaluation",
    "title": "20 Evaluation",
    "section": "Using tidy evaluation",
    "text": "Using tidy evaluation\n本节将会给出一些使用tidy evaluation的函数例子。\n\nQuoting and unquoting\n嵌套函数传递“表达式”时，函数内部一定要先使用enquo()引用“表达式”，然后再使用!!解引用。\n假设有这样一个随机排序数据框的函数：\n\nresample &lt;- function(df, n) {\n  idx &lt;- sample(nrow(df), n, replace = TRUE)\n  df[idx, , drop = FALSE]\n}\n\n现在要结合上面的subset2()函数，同时实现筛选和随机排序：\n\nsubsample &lt;- function(df, cond, n = nrow(df)) {\n  df &lt;- subset2(df, cond)\n  resample(df, n)\n}\n\ndf &lt;- data.frame(x = c(1, 1, 1, 2, 2), y = 1:5)\nrm(x)\nsubsample(df, x == 1)\n#&gt; Error: object 'x' not found\n\n由于subsample()函数没有对cond进行引用，所以导致subset2()无法正确评估x。这种嵌套传递“表达式”需要“引用-解引用”式的中间步骤。\n\nsubsample &lt;- function(df, cond, n = nrow(df)) {\n  cond &lt;- enquo(cond)\n  df &lt;- subset2(df, !!cond)\n  resample(df, n)\n}\n\nsubsample(df, x == 1)\n#&gt;     x y\n#&gt; 1   1 1\n#&gt; 1.1 1 1\n#&gt; 2   1 2\n\n\n\nHandling ambiguity\n当既有指向数据框的参数也有指向环境的参数时，会导致引用歧义，产生不符合预期的结果。\n假设现在有一个根据提供的参数值来过滤数据框的函数。\n\nthreshold_x &lt;- function(df, val) {\n  subset2(df, x &gt;= val)\n}\n\n\n当x在环境中存在，但不在数据框中时：\n\n\nx &lt;- 10\nno_x &lt;- data.frame(y = 1:3)\nthreshold_x(no_x, 2)\n#&gt;   y\n#&gt; 1 1\n#&gt; 2 2\n#&gt; 3 3\n\n\n当数据框中有val列时：\n\n\nhas_val &lt;- data.frame(x = 1:3, val = 9:11)\nthreshold_x(has_val, 2)\n#&gt; [1] x   val\n#&gt; &lt;0 rows&gt; (or 0-length row.names)\n\n特殊情况会产生不符合预期的结果，所以我们需要声明参数来源。\n\nthreshold_x &lt;- function(df, val) {\n  subset2(df, .data$x &gt;= .env$val)\n}\n\nx &lt;- 10\nthreshold_x(no_x, 2)\n#&gt; Error in `.data$x`:\n#&gt; ! Column `x` not found in `.data`.\nthreshold_x(has_val, 2)\n#&gt;   x val\n#&gt; 2 2  10\n#&gt; 3 3  11\n\n通常使用.env声明的参数也可以使用!!来替代：\n\nthreshold_x &lt;- function(df, val) {\n  subset2(df, .data$x &gt;= !!val)\n}\n\n二者的区别在于何时评估参数val：如果使用!!，val会被enquo()评估，如果使用.env，val会被eval_tidy()评估。这种差别的影响微乎其微。\n\n\nQuoting and ambiguity\n\n\n\n\n\n\nWarning\n\n\n\n本小节的内容，没有搞懂作者的意图。\n\n\n将上面的threshold_x()函数的筛选列由固定的x改为参数var提供，可以使用.data[[var]]来访问列：\n\nthreshold_var &lt;- function(df, var, val) {\n  var &lt;- as_string(ensym(var))\n  subset2(df, .data[[var]] &gt;= !!val)\n}\n\ndf &lt;- data.frame(x = 1:10)\nthreshold_var(df, x, 8)\n#&gt;     x\n#&gt; 8   8\n#&gt; 9   9\n#&gt; 10 10\n\n也可以使用enquo()和!!来处理列：\n\nthreshold_expr &lt;- function(df, expr, val) {\n  expr &lt;- enquo(expr)\n  subset2(df, !!expr &gt;= !!val)\n}\n\nthreshold_expr(df, x, 8)\n#&gt;     x\n#&gt; 8   8\n#&gt; 9   9\n#&gt; 10 10",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/20 Evaluation.html#base-evaluation",
    "href": "Books/Advanced R(2e)/20 Evaluation.html#base-evaluation",
    "title": "20 Evaluation",
    "section": "Base evaluation",
    "text": "Base evaluation\n本节介绍base R中替代tidy evaluation的两种常用函数：\n\nsubstitute()和在调用环境中评估（base::subset()使用的函数）。\nmatch.call()控制调用和在调用环境中评估（stats::lm()使用的函数）。\n\n\nsubstitute()\nbase R中最常见的非标准性评估（NSE）模式是substitute() + eval()。下面是使用这种模式编写的subset()。二者的主要区别是评估的环境不同，前者在调用环境中评估，后者在“表达式”定义时的环境中评估。\n\nsubset_base &lt;- function(data, rows) {\n  rows &lt;- substitute(rows)\n  rows_val &lt;- eval(rows, data, caller_env())\n  stopifnot(is.logical(rows_val))\n\n  data[rows_val, , drop = FALSE]\n}\n\nsubset_tidy &lt;- function(data, rows) {\n  rows &lt;- enquo(rows)\n  rows_val &lt;- eval_tidy(rows, data, env = caller_env())\n  stopifnot(is.logical(rows_val))\n\n  data[rows_val, , drop = FALSE]\n}\n\n\nProgramming with subset()\nsubset()的文档中由这样的警告：\n\n\n\n\n\n\nWarning\n\n\n\nThis is a convenience function intended for use interactively. For programming it is better to use the standard subsetting functions like [, and in particular the non-standard evaluation of argument subset can have unanticipated consequences.\n\n\n它存在三个主要问题：\n\nbase::subset()总是在调用环境中评估rows，这可能会导致评估错误。\n\nf1 &lt;- function(df, ...) {\n  xval &lt;- 3\n  subset_base(df, ...)\n  # subset_tidy(df, ...)\n}\n\nmy_df &lt;- data.frame(x = 1:3, y = 3:1)\nxval &lt;- 1\nf1(my_df, x == xval)\n#&gt;   x y\n#&gt; 3 3 1\n\n这也意味着subset_base()类型的函数不能与map()或lapply()一起使用。\n\nlocal({\n  zzz &lt;- 2\n  dfs &lt;- list(data.frame(x = 1:3), data.frame(x = 4:6))\n  lapply(dfs, subset_base, x == zzz)\n})\n#&gt; Error in eval(rows, data, caller_env()): object 'zzz' not found\n\n从另一个函数调用subset()需要注意：你必须使用substitute()来捕获对substitute()完整表达式的调用，然后进行求值。这段代码很难理解，因为substitute()没有使用语法标记来取消引用。\n\nf2 &lt;- function(df1, expr) {\n  call &lt;- substitute(subset_base(df1, expr))\n  expr_print(call)\n  eval(call, caller_env())\n}\n\nmy_df &lt;- data.frame(x = 1:3, y = 3:1)\nf2(my_df, x == 1)\n#&gt; subset_base(my_df, x == 1)\n#&gt;   x y\n#&gt; 1 1 3\n\neval()不提供任何“声明”，所以无法准确区分“表达式”来源。据我所知，除非手动检查df中是否存在z变量，否则无法确保以下函数的安全。\n\nf3 &lt;- function(df) {\n  call &lt;- substitute(subset_base(df, z &gt; 0))\n  expr_print(call)\n  eval(call, caller_env())\n}\n\nmy_df &lt;- data.frame(x = 1:3, y = 3:1)\nz &lt;- -1\nf3(my_df)\n#&gt; subset_base(my_df, z &gt; 0)\n#&gt; [1] x y\n#&gt; &lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\nWhat about [?\n既然tidy-eval很复杂，为什么不直接使用[？首先，[只能交互使用，不能应用在函数中，会不具有通用性。其次，相较于[，subset()有两个有点：\n\n它默认设置了drop = FALSE，保证返回的始终是数据框。\n它丢掉了条件是NA的行。\n\n这意味着，subset(df, x == y) 不等于df[x == y, ]，而是等价于df[x == y & !is.na(x == y), , drop = FALSE]。而且，类似subset()的函数，如dplyr::filter()甚至可以将R语言的过滤规则转化为SQL语言，使得它在编程方面应用广泛。\n\n\n\nmatch.call()\nbase R中另外一种NSE模式是使用match.call()。与substitute()类似，都会捕获”表达式“，但match.call()会捕获完整的调用”表达式“，并修改然后评估它。”rlang“中没有与其等价的函数。\n\ng &lt;- function(x, y, z) {\n  match.call()\n}\ng(1, 2, z = 3)\n#&gt; g(x = 1, y = 2, z = 3)\n\nmatch.call()的一个重要有应用是write.csv()，write.csv()捕获调用write.table()的表达式，然后修改参数，最后评估：\n\nwrite.csv &lt;- function(...) {\n  call &lt;- match.call(write.table, expand.dots = TRUE)\n\n  call[[1]] &lt;- quote(write.table)\n  call$sep &lt;- \",\"\n  call$dec &lt;- \".\"\n\n  eval(call, parent.frame())\n}\n\n但是也可以不使用NSE直接实现这种功能：\n\nwrite.csv &lt;- function(...) {\n  write.table(..., sep = \",\", dec = \".\")\n}\n\n\nWrapping modelling functions\nmatch.call()的另一个重要应用是lm()。但这一技术同时也导致打印捕获的”formula“不完整。让我们思考下面这一简单地对lm()的包装：\n\nlm2 &lt;- function(formula, data) {\n  lm(formula, data)\n}\n\n这个包装函数可以成功运行，但无法准确捕获到”formula“：\n\nlm2(mpg ~ disp, mtcars)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = formula, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         disp  \n#&gt;    29.59985     -0.04122\n\n为了修复这一错误，我们需要使用准引用技术。\n\nlm3 &lt;- function(formula, data, env = caller_env()) {\n  formula &lt;- enexpr(formula)\n  data &lt;- enexpr(data)\n\n  lm_call &lt;- expr(lm(!!formula, data = !!data))\n  expr_print(lm_call)\n  eval(lm_call, env)\n}\nlm3(mpg ~ disp, mtcars)\n#&gt; lm(mpg ~ disp, data = mtcars)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = mpg ~ disp, data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         disp  \n#&gt;    29.59985     -0.04122\n\n当你想要封装一个base R中的NSE函数时，你需要注意下面三点：\n\n使用enexpr()捕获未评估的参数，使用caller_env()获取调用函数的env。\n使用expr()与!!组合新的调用“表达式”。\n要在caller_env()中评估新的“表达式”。\n\n\n\nEvaluation environment\n如果在lm3()中，对data进行了某种处理，如resample()，那么会导致data的环境由外部的调用环境变为内部的运行环境，最终导致报错。\n\nresample_lm0 &lt;- function(formula, data, env = caller_env()) {\n  formula &lt;- enexpr(formula)\n  resample_data &lt;- resample(data, n = nrow(data))\n\n  lm_call &lt;- expr(lm(!!formula, data = resample_data))\n  expr_print(lm_call)\n  eval(lm_call, env)\n}\n\ndf &lt;- data.frame(x = 1:10, y = 5 + 3 * (1:10) + round(rnorm(10), 2))\nresample_lm0(y ~ x, data = df)\n#&gt; lm(y ~ x, data = resample_data)\n#&gt; Error in eval(mf, parent.frame()): object 'resample_data' not found\n\n有两种方法可以避免报错：\n\n直接将data解引用进行传递，但这会导致打印出的data很奇怪：\n\nresample_lm1 &lt;- function(formula, data, env = caller_env()) {\n  formula &lt;- enexpr(formula)\n  resample_data &lt;- resample(data, n = nrow(data))\n\n  lm_call &lt;- expr(lm(!!formula, data = !!resample_data))\n  expr_print(lm_call)\n  eval(lm_call, env)\n}\nresample_lm1(y ~ x, data = df)$call\n#&gt; lm(y ~ x, data = &lt;df[,2]&gt;)\n#&gt; lm(formula = y ~ x, data = list(x = c(10L, 7L, 6L, 8L, 7L, 6L, \n#&gt; 9L, 5L, 7L, 6L), y = c(34.45, 24.36, 22.83, 28.5, 24.36, 22.83, \n#&gt; 32.83, 19.01, 24.36, 22.83)))\n\n将修改后的data重新添加到caller_env()中：\n\nresample_lm2 &lt;- function(formula, data, env = caller_env()) {\n  formula &lt;- enexpr(formula)\n  resample_data &lt;- resample(data, n = nrow(data))\n\n  lm_env &lt;- env(env, resample_data = resample_data)\n  lm_call &lt;- expr(lm(!!formula, data = resample_data))\n  expr_print(lm_call)\n  eval(lm_call, lm_env)\n}\nresample_lm2(y ~ x, data = df)\n#&gt; lm(y ~ x, data = resample_data)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = resample_data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;       4.961        2.915",
    "crumbs": [
      "20 Evaluation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html",
    "title": "19 Quasiquotation",
    "section": "",
    "text": "准引用（quasiquotation）包括两部分——引用和解引用，引用就是我们上章介绍过的捕获“表达式”，解引用相当于“评估”“表达式”。但是要注意，解引用函数只在引用函数内有效，它的目的是“评估”要捕获的“表达式”中的部分元素。准引用使得组合“函数创建者的函数”与“函数使用者的函数”更加容易。\n准引用是组成tidy-evaluation的三大基石之一（quosures和data-mask将在下章介绍），在tidy-evaluation中，所有支持引用的函数都支持解引用。\n\n\n\n19.2节：通过一个示例函数——cement()介绍为什么使用准引用。\n19.3节：介绍引用函数。\n19.4节：介绍解引用函数。\n19.5节：讨论base R中类似的“解引用”。\n19.6节：介绍使用!!!的另外两种情况。\n19.7节：介绍一些示例。\n\n\n\n\n需要熟悉第17，18章的内容。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice\n\n\n\n\n引用函数与Lisp语言中的宏紧密相关。宏通常在编译时运行，这在R中不存在，并且它的输出也是AST结构。引用函数与更为深奥的Lisp fexprs关系更为密切，后者是默认引用所有参数的函数。当在其他编程语言中寻找相关工作时，了解这些术语是有用的。",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#introduction",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#introduction",
    "title": "19 Quasiquotation",
    "section": "",
    "text": "准引用（quasiquotation）包括两部分——引用和解引用，引用就是我们上章介绍过的捕获“表达式”，解引用相当于“评估”“表达式”。但是要注意，解引用函数只在引用函数内有效，它的目的是“评估”要捕获的“表达式”中的部分元素。准引用使得组合“函数创建者的函数”与“函数使用者的函数”更加容易。\n准引用是组成tidy-evaluation的三大基石之一（quosures和data-mask将在下章介绍），在tidy-evaluation中，所有支持引用的函数都支持解引用。\n\n\n\n19.2节：通过一个示例函数——cement()介绍为什么使用准引用。\n19.3节：介绍引用函数。\n19.4节：介绍解引用函数。\n19.5节：讨论base R中类似的“解引用”。\n19.6节：介绍使用!!!的另外两种情况。\n19.7节：介绍一些示例。\n\n\n\n\n需要熟悉第17，18章的内容。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice\n\n\n\n\n引用函数与Lisp语言中的宏紧密相关。宏通常在编译时运行，这在R中不存在，并且它的输出也是AST结构。引用函数与更为深奥的Lisp fexprs关系更为密切，后者是默认引用所有参数的函数。当在其他编程语言中寻找相关工作时，了解这些术语是有用的。",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#motivation",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#motivation",
    "title": "19 Quasiquotation",
    "section": "Motivation",
    "text": "Motivation\n我们将从一个具体的例子开始，激发准引用的使用需求。假设你要创建一个类似paste()的函数：\n\npaste(\"Good\", \"morning\", \"Hadley\")\n#&gt; [1] \"Good morning Hadley\"\npaste(\"Good\", \"afternoon\", \"Alice\")\n#&gt; [1] \"Good afternoon Alice\"\n\n如果你厌倦了每次都输入引号““”，那么你可以创建下面的函数（后续会详细介绍如何使用）：\n\ncement &lt;- function(...) {\n  args &lt;- ensyms(...)\n  paste(purrr::map(args, as_string), collapse = \" \")\n}\n\ncement(Good, morning, Hadley)\n#&gt; [1] \"Good morning Hadley\"\ncement(Good, afternoon, Alice)\n#&gt; [1] \"Good afternoon Alice\"\n\n上面的函数使得我们无需在每个字符两边添加引号，但它有个问题是：当我们使用变量指代字符时，它无法识别变量。paste()函数则可以正常识别\n\nname &lt;- \"Hadley\"\ntime &lt;- \"morning\"\n\ncement(Good, time, name)\n#&gt; [1] \"Good time name\"\npaste(\"Good\", time, name)\n#&gt; [1] \"Good morning Hadley\"\n\n我们可以使用特殊的解引符号!!处理上面这种情况：\n\ncement(Good, !!time, !!name)\n#&gt; [1] \"Good morning Hadley\"\n\n\n\n\n\n\n\nNote\n\n\n\n上面的示例可能具有误导性。前面我们讲到过，常量的“表达式”就是常量，所以\"Hadley\"等价于expr(\"Hadley\")，!!始终作用的是一个“表达式”，并且它会被传入到ensyms()中，也即解引函数始终在引用函数内生效。\n引用函数ensyms()的返回值是一个表达式，我们可以对他进行“评估”或as_string()，但这些都已经不属于准引用的范畴了。\n\n\n\nVocabulary\n“引用”与“评估”参数之间的不同是重要的：\n\n一个被评估的参数遵循R的常规评估规则。\n一个被引用的参数被保存为表达式，遵循自定义的处理规则。\n\n上面的paste()的参数被评估，cement()的参数被引用。\n如果你不确定一个参数是被评估还是被引用，你可以在函数外面的环境中允许这个参数，如果报错，那么这个参数被引用。例如：\n\n# works\nlibrary(MASS)\n\n# fails\nMASS\n#&gt; Error: object 'MASS' not found",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#quoting",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#quoting",
    "title": "19 Quasiquotation",
    "section": "Quoting",
    "text": "Quoting\n准引用的第一个部分是引用——捕获表达式但不进行评估。引用相关的函数通常是成对出现，因为要考虑直接引用和间接引用（函数中的惰性评估）。本节首先介绍“rlang”中的引用函数，然后介绍base R中的。\n\nCapturing expressions\n捕获表达式的函数有四个：\n\n\n\n\n单个引用\n多个引用\n\n\n\n\n交互场景\nexpr()\nexprs()\n\n\n函数场景\nenexpr()\nenexprs()\n\n\n\n在交互场景下，expr()和exprs()都直接捕获它们的参数，前者捕获单个参数，后者捕获多个参数。\n\nexpr(x + y)\n#&gt; x + y\nexpr(1 / 2 / 3)\n#&gt; 1/2/3\n\nexprs(x = x^2, y = y^3, z = z^4)\n#&gt; $x\n#&gt; x^2\n#&gt; \n#&gt; $y\n#&gt; y^3\n#&gt; \n#&gt; $z\n#&gt; z^4\n# shorthand for\n# list(x = expr(x ^ 2), y = expr(y ^ 3), z = expr(z ^ 4))\n\n在函数场景下，因为惰性评估的原因，需要使用额外的函数enexpr()和enexprs()。\n\n# expr 直接捕获参数\nf1 &lt;- function(x) expr(x)\nf1(a + b + c)\n#&gt; x\n\n# exprs 捕获函数的参数评估后的结果\nf2 &lt;- function(x) enexpr(x)\nf2(a + b + c)\n#&gt; a + b + c\n\n捕获特殊参数...的结果只能使用enexprs()。\n\nf &lt;- function(...) enexprs(...)\nf(x = 1, y = 10 * z)\n#&gt; $x\n#&gt; [1] 1\n#&gt; \n#&gt; $y\n#&gt; 10 * z\n\n\n\nCapturing symbols\nensym()和ensyms()函数是enexpr()和enexprs()的变体，专门用于捕获符号或将字符串转换为符号，输入不符合时会报错。\n\nf &lt;- function(...) ensyms(...)\nf(x)\n#&gt; [[1]]\n#&gt; x\nf(\"x\")\n#&gt; [[1]]\n#&gt; x\n\n\n\nWith base R\n上述讲到的函数在base R中都有对应的等价函数，它们之间的主要不同是base R中的函数不支持解引用。\n\n\n\nbase R\nrlang\n\n\n\n\nquote()\nexpr()\n\n\nalist()\nexprs()\n\n\nsubstitute()\nenexpr()\n\n\nas.list(substitute(...()))\nenexprs()\n\n\n\n\nquote(x + y)\n#&gt; x + y\n\nalist(x = 1, y = x + 2)\n#&gt; $x\n#&gt; [1] 1\n#&gt; \n#&gt; $y\n#&gt; x + 2\n\nf3 &lt;- function(x) substitute(x)\nf3(x + y)\n#&gt; x + y\n\nf &lt;- function(...) as.list(substitute(...()))\nf(x = 1, y = 10 * z)\n#&gt; $x\n#&gt; [1] 1\n#&gt; \n#&gt; $y\n#&gt; 10 * z\n\n除此之外，base R中还有两个重要的引用函数：\n\nbquote()：提供了有限的准引用形式，将在19.5节中讨论。\n~：能够捕获环境的引用函数，将在20.3.4中讨论。\n\n\n\nSubstitution\nsubstitute()函数除了执行“引用”的功能外，还执行“替换”的功能。\n\nf4 &lt;- function(x) substitute(x * 2)\nf4(a + b + c)\n#&gt; (a + b + c) * 2\n\n如果你在交互场景中使用它的“替换”功能时，推荐添加第二个参数来指定哪些是要替换的。\n\nsubstitute(x * y * z, list(x = 10, y = quote(a + b)))\n#&gt; 10 * (a + b) * z\n\n\n\nSummary\n在使用引用功能时，你始终要注意两点：\n\n要引用的对象是固定（交互场景）还是不固定的（函数场景）。\n要引用单个还是多个。\n\n\n\n\n\nDeveloper\nUser\n\n\n\n\nOne\nexpr()\nenexpr()\n\n\nMany\nexprs()\nenexprs()\n\n\n\n\n\n\n\nDeveloper\nUser\n\n\n\n\nOne\nquote()\nsubstitute()\n\n\nMany\nalist()\nas.list(substitute(...()))",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#unquoting",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#unquoting",
    "title": "19 Quasiquotation",
    "section": "Unquoting",
    "text": "Unquoting\n解引用允你许选择性地评估“表达式”中的部分内容，其余仍然被引用。这样，你可以使用一个AST模板去生成其他AST。base R需要使用另外的技术来实现，我们将在19.5中介绍。\n与解引用类似的是eval()函数（20章），但他们是不同的，解引用在引用函数内使用——expr(!!x)，评估实在函数外使用——eval(expr(x))，它们最终的结果也可能不同。\n\nUnquoting one argument\n!!会“评估”一个“表达式”，并返回的值插入到整体的“表达式”中：\n\nx &lt;- expr(-1)\nexpr(f(!!x, y))\n#&gt; f(-1, y)\n\n下面是!!运行的底层逻辑示例图：\n\n!!当然也可以作用于符号或常量：\n\na &lt;- sym(\"y\")\nb &lt;- 1\nexpr(f(!!a, !!b))\n#&gt; f(y, 1)\n\n\n!!作用于返回“表达式”的函数：\n\nmean_rm &lt;- function(var) {\n  var &lt;- ensym(var)\n  expr(mean(!!var, na.rm = TRUE))\n}\nexpr(!!mean_rm(x) + !!mean_rm(y))\n#&gt; mean(x, na.rm = TRUE) + mean(y, na.rm = TRUE)\n\n!!会保留操作符的优先级：\n\nx1 &lt;- expr(x + 1)\nx2 &lt;- expr(x + 2)\n\nexpr(!!x1 / !!x2)\n#&gt; (x + 1)/(x + 2)\n\n\n如果我们只是简单地将表达式的文本粘贴在一起，我们最终会得到x + 1 / x + 2，这是完全不同的AST:\n\n\n\nUnquoting a function\n通常!!用来解引用参数，但是!!也可以用来解引用函数。需要注意的是expr(!!f(x))解引用的是f(x)，需要使用额外的括号表示解引函数——expr((!!f)(x))。\n\nf &lt;- expr(foo)\nexpr((!!f)(x, y))\n#&gt; foo(x, y)\n\n函数f来自于包的写法也可以：\n\nf &lt;- expr(pkg::foo)\nexpr((!!f)(x, y))\n#&gt; pkg::foo(x, y)\n\n\n上面的代码也可以使用rlang::call2()改写：\n\nf &lt;- expr(pkg::foo)\ncall2(f, expr(x), expr(y))\n#&gt; pkg::foo(x, y)\n\n\n\nUnquoting a missing argument\n在极少数情况下，我们需要解引用缺失值参数（missing argument），但直接解引用会失效：\n\narg &lt;- missing_arg()\nexpr(foo(!!arg, !!arg))\n#&gt; Error: argument \"arg\" is missing, with no default\n\n我们需要使用rlang::maybe_missing()来处理缺失值参数：\n\nexpr(foo(!!maybe_missing(arg), !!maybe_missing(arg)))\n#&gt; foo(, )\n\n\n\nUnquoting in special forms\n某些函数的infix形式的解引用会失败，例如$：它必须始终跟随变量名称，而不是其他表达式，强制使用会报错：\n\nx &lt;- expr(x)\nexpr(df$!!x)\n#&gt; Error in parse(text = input): &lt;text&gt;:2:9: unexpected '!'\n#&gt; 1: x &lt;- expr(x)\n#&gt; 2: expr(df$!\n#&gt;            ^\n\n需要将函数转变为prefix形式：\n\nx &lt;- expr(x)\nexpr(`$`(df, !!x))\n#&gt; df$x\n\n\n\nUnquoting many arguments\n!!是一对一地解引用，!!!是一对多的解引用。可以是“表达式”列表或带name属性的向量和列表。\n\nxs &lt;- exprs(1, a, -b)\nexpr(f(!!!xs, y))\n#&gt; f(1, a, -b, y)\n\n# Or with names\nys &lt;- set_names(xs, c(\"a\", \"b\", \"c\"))\nexpr(f(!!!ys, d = 4))\n#&gt; f(a = 1, b = a, c = -b, d = 4)\n\n\n!!!也可以应用在call2()中：\n\ncall2(\"f\", !!!xs, expr(y))\n#&gt; f(1, a, -b, y)\n\n\n\nThe polite fiction of !!\n!!和!!!与R中的类似+,-,!等运算符不同，在R的视角里，!!就是运行了两次的!：\n\n!!TRUE\n#&gt; [1] TRUE\n!!!TRUE\n#&gt; [1] FALSE\n\n!!和!!!必须在准引用函数中才能变得类似+,-,!等运算符。在准引用函数外使用，会被视为!作用于“表达式”，导致报错：\n\nx &lt;- quote(variable)\n!!x\n#&gt; Error in !x: invalid argument type\n\n但是因为!可以作用于数字，所以有时会产生错误的结果：\n\ndf &lt;- data.frame(x = 1:5)\ny &lt;- 100\nwith(df, x + !!y)\n#&gt; [1] 2 3 4 5 6\n\n\n\nNon-standard ASTs\n在解引用时，可能会轻易地创建出非标准的AST，例如解引用的对象不是“表达式”时。这些非标准AST是有效的，偶尔也很有用，但它们的正确使用超出了本书的范围。然而，了解它们很重要，因为它们可能会被解析，从而以误导性的方式被打印出来。\n例如，如果你内联更复杂的对象，它们的属性就不会打印出来。这可能会导致输出混乱：\n\nx1 &lt;- expr(class(!!data.frame(x = 10)))\nx1\n#&gt; class(list(x = 10))\neval(x1)\n#&gt; [1] \"data.frame\"\n\n有两个工具可以消除这种混乱：rlang::expr_print()和lobstr::ast():\n\nexpr_print(x1)\n#&gt; class(&lt;df[,1]&gt;)\nlobstr::ast(!!x1)\n#&gt; █─class \n#&gt; └─&lt;inline data.frame&gt;\n\n另外一种插入整数语句造成地混乱：\n\nx2 &lt;- expr(f(!!c(1L, 2L, 3L, 4L, 5L)))\nx2\n#&gt; f(1:5)\nexpr_print(x2)\n#&gt; f(&lt;int: 1L, 2L, 3L, 4L, 5L&gt;)\nlobstr::ast(!!x2)\n#&gt; █─f \n#&gt; └─&lt;inline integer&gt;\n\n也可以创建由于运算符优先级而无法从代码中生成的常规AST。在这种情况下，R将打印AST中不存在的括号：\n\nx3 &lt;- expr(1 + !!expr(2 + 3))\nx3\n#&gt; 1 + (2 + 3)\n\nlobstr::ast(!!x3)\n#&gt; █─`+` \n#&gt; ├─1 \n#&gt; └─█─`+` \n#&gt;   ├─2 \n#&gt;   └─3",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#non-quoting",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#non-quoting",
    "title": "19 Quasiquotation",
    "section": "Non-quoting",
    "text": "Non-quoting\nbase R中的bquote()函数支持准引用，使用.()来解引用。\n\nxyz &lt;- bquote(x + y + z)\nbquote(-.(xyz) / 2)\n#&gt; -(x + y + z)/2\n\n但bquote()函数在base R中并没有被广泛使用，也没有对R的书写方式产生任何轻微的影响。主要有三个原因：\n\n它无法将创作者的函数和使用者的函数组合在一起，只适合自己使用。\n它不支持解引用多个表达式。\n它不支持提供的环境及在环境中处理代码。\n\nbase R中具有引用参数功能的函数使用的其他技术路径（不是bquote()），在需要“解引用”时，它们选择性地关闭引用功能，不是真正的“解引用”，这里称之为非引用。\n引用与非引用在base R中有四种基本形式：\n\n成对出现的引用和非引用函数。例如，$有两个参数，第二个参数引用的。把mtcars$cyl写成prefix型式`$`(mtcars, cyl)，就会很容易发现它的引用特性。更直接的对比是非引用函数[[：\n\n\nx &lt;- list(var = 1, y = 2)\nvar &lt;- \"y\"\n\nx$var\n#&gt; [1] 1\nx[[var]]\n#&gt; [1] 2\n\nbase R中还有三个与$紧密相关的函数：subset()、transform()和with()。它们被视为仅适用于交互场景使用的$包装，因此都具有相同的非引用替代函数：[&lt;-/assign()、::/getExportedValue()。\n\n成对出现的引用和非引用参数。例如，rm()函数允许在...中使用引用参数或在list中提供非引用参数。同样的还有data(),save()等。\n\n\nx &lt;- 1\nrm(x)\n\ny &lt;- 2\nvars &lt;- c(\"y\", \"vars\")\nrm(list = vars)\n\n\n通过某个参数控制另外一个参数是否是引用的。例如，library()函数的character.only参数控制package参数是否是引用的。同样的还有demo(),detach(),example(),require()。\n\n\nlibrary(MASS)\n\npkg &lt;- \"MASS\"\nlibrary(pkg, character.only = TRUE)\n\n\n进行尝试，如果评估失败尝试引用。例如，help()函数的第一个参数要求是非引用的字符串，如果评估失败会尝试引用参数。同样的还有ls(),page(),match.fun()。\n\n\n# Shows help for var\nhelp(var)\n#&gt; No documentation for 'y' in specified packages and libraries:\n#&gt; you could try '??y'\n\nvar &lt;- \"mean\"\n# Shows help for mean\nhelp(var)\n#&gt; starting httpd help server ... done\n\nvar &lt;- 10\n# Shows help for var\nhelp(var)\n\nbase R中另一类重要的引用函数是建模和绘图函数，它们遵循所谓的标准非标准评估规则。例如，lm()函数的formula参数，绘图函数plot()中的映射类参数col,cex,pch等等。\n\npalette(RColorBrewer::brewer.pal(3, \"Set1\"))\nplot(\n  Sepal.Length ~ Petal.Length,\n  data = iris,\n  col = Species,\n  pch = 20,\n  cex = 2\n)",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#dot-dot-dot",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#dot-dot-dot",
    "title": "19 Quasiquotation",
    "section": "...(dot-dot-dot)",
    "text": "...(dot-dot-dot)\n本节介绍两种常见的使用!!!和:=的场景。\n\n使用!!!解引用参数，释放到...中。考虑下面的数据框列表，如果要按行合并它们，你可以直接使用rbind(dfs$a, dfs$b)，但如果list中包含多个数据框呢，或者数量未知呢？\n\n\ndfs &lt;- list(\n  a = data.frame(x = 1, y = 2),\n  b = data.frame(x = 3, y = 4)\n)\n\n可以使用!!!直接解引用参数，释放到...中。\n\ndplyr::bind_rows(!!!dfs)\n#&gt;   x y\n#&gt; 1 1 2\n#&gt; 2 3 4\n\n在这种情况下使用时，!!!的行为在Ruby、Go、PHP和Julia中被称为“splatting”。它与Python中的*args(star-args)和*kwarg(star-star-kwargs)密切相关，这些行为有时被称为参数解包。\n\n如何自定义符号的名字。例如，你想创建一个单列的数据框，并且它的列名由变量var决定，你可以使用setNames(data.frame(val), var)，但这样并不“优雅”。\n\n\nvar &lt;- \"x\"\nval &lt;- c(4, 3, 9)\n\n可以使用:=可以在=的左边解引用参数：\n\ntibble::tibble(!!var := val)\n#&gt; # A tibble: 3 × 1\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1     4\n#&gt; 2     3\n#&gt; 3     9\n\n因为R不支持使用“表达式”作为参数名称，所以使用了新的操作符:=：\n\ntibble::tibble(!!var = val)\n#&gt; Error in parse(text = input): &lt;text&gt;:1:22: unexpected '='\n#&gt; 1: tibble::tibble(!!var =\n#&gt;                          ^\n\n作者称支持这些无需引用参数工具的函数，都有tidy dots。创建具有tidy dots的函数只需要使用list2()。\n\nExamples\n下面是一个设置属性函数的例子，它允许我们灵活的设置属性。\n\nset_attr &lt;- function(.x, ...) {\n  attr &lt;- rlang::list2(...)\n  attributes(.x) &lt;- attr\n  .x\n}\n\nattrs &lt;- list(x = 1, y = 2)\nattr_name &lt;- \"z\"\n\n1:10 %&gt;%\n  set_attr(w = 0, !!!attrs, !!attr_name := 3) %&gt;%\n  str()\n#&gt;  int [1:10] 1 2 3 4 5 6 7 8 9 10\n#&gt;  - attr(*, \"w\")= num 0\n#&gt;  - attr(*, \"x\")= num 1\n#&gt;  - attr(*, \"y\")= num 2\n#&gt;  - attr(*, \"z\")= num 3\n\n\n\nexec()\n如果你想再没有tidy-dots的函数中使用!!!和:=，你可以使用exec()。它的使用方法与call2()类似，但call2()返回“表达式”，而exec()直接评估了“表达式”。\n使用!!!解析参数列表：\n\n# Directly\nexec(\"mean\", x = 1:10, na.rm = TRUE, trim = 0.1)\n#&gt; [1] 5.5\n\n# Indirectly\nargs &lt;- list(x = 1:10, na.rm = TRUE, trim = 0.1)\nexec(\"mean\", !!!args)\n#&gt; [1] 5.5\n\n# Mixed\nparams &lt;- list(na.rm = TRUE, trim = 0.1)\nexec(\"mean\", x = 1:10, !!!params)\n#&gt; [1] 5.5\n\n使用:=解析不固定参数：\n\narg_name &lt;- \"na.rm\"\narg_val &lt;- TRUE\nexec(\"mean\", 1:10, !!arg_name := arg_val)\n#&gt; [1] 5.5\n\nexec()也可以和泛函配合使用：\n\nx &lt;- c(runif(10), NA)\nfuns &lt;- c(\"mean\", \"median\", \"sd\")\n\npurrr::map_dbl(funs, exec, x, na.rm = TRUE)\n#&gt; [1] 0.6283860 0.7322721 0.3160886\n\n\n\ndots_list()\nlist2()提供了一个方便的功能：它会自动忽略最后一个空参数。这意味你可以在诸如tibble::tibble()这样的函数中不必理会最后一个元素后的逗号。\n\n# Can easily move x to first entry:\ntibble::tibble(\n  y = 1:5,\n  z = 3:-1,\n  x = 5:1,\n) %&gt;%\n  dplyr::mutate(\n    a = 1:5,\n    b = 5:1,\n    c = 1:5,\n  )\n#&gt; # A tibble: 5 × 6\n#&gt;       y     z     x     a     b     c\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     3     5     1     5     1\n#&gt; 2     2     2     4     2     4     2\n#&gt; 3     3     1     3     3     3     3\n#&gt; 4     4     0     2     4     2     4\n#&gt; 5     5    -1     1     5     1     5\n\n# Need to remove comma from z and add comma to x\ndata.frame(\n  y = 1:5,\n  z = 3:-1,\n  x = 5:1\n)\n#&gt;   y  z x\n#&gt; 1 1  3 5\n#&gt; 2 2  2 4\n#&gt; 3 3  1 3\n#&gt; 4 4  0 2\n#&gt; 5 5 -1 1\n\nlist2()函数是对rlang::dots_list()函数的封装，设置了一些常用设置为默认值。你可以直接使用dots_list()函数进行个性化的设置。\n\n.ignore_empty参数控制忽略哪些参数。默认情况下，设置ignore_empty = \"trailing\"会忽略最后一个空参数，这会导致上述行为，但你可以选择忽略所有缺失的参数（\"all\"），或者不忽略任何缺失的参数（\"none\"）。\n.homonyms参数控制同名参数的处理。\n\n\nstr(dots_list(x = 1, x = 2))\n#&gt; List of 2\n#&gt;  $ x: num 1\n#&gt;  $ x: num 2\nstr(dots_list(x = 1, x = 2, .homonyms = \"first\"))\n#&gt; List of 1\n#&gt;  $ x: num 1\nstr(dots_list(x = 1, x = 2, .homonyms = \"last\"))\n#&gt; List of 1\n#&gt;  $ x: num 2\nstr(dots_list(x = 1, x = 2, .homonyms = \"error\"))\n#&gt; Error:\n#&gt; ! Arguments in `...` must have unique names.\n#&gt; ✖ Multiple arguments named `x` at positions 1 and 2.\n\n\n如果存在未被忽略的空参数，.preserve_empty控制如何处理它们。默认情况下会抛出一个错误；设置.preserve_empty = TRUE则会返回缺失符号。如果使用dots_list()生成函数调用，这将很有用。\n\n\n\nWith base R\nbase R 提供了一个瑞士军刀般的函数——do.call()。该函数接受两个参数，首个参数what接受一个函数名，第二个参数args接受一个参数列表。例如，do.call(\"f\", list(x, y ,z))等价于f(x, y, z)。\n\ndo.call()解决rbind()多个数据框：\n\n\ndo.call(\"rbind\", dfs)\n#&gt;   x y\n#&gt; a 1 2\n#&gt; b 3 4\n\n\ndo.call()解决变量名的问题：\n\n\nargs &lt;- list(val)\nnames(args) &lt;- var\n\ndo.call(\"data.frame\", args)\n#&gt;   x\n#&gt; 1 4\n#&gt; 2 3\n#&gt; 3 9\n\n一些base R中的函数（interaction(), expand.grid(), options(), par()）使用了一种技巧去规避使用do.call()：如果...中的第一个参数是列表，则直接使用这个列表，跳过...中的剩余参数。\nf &lt;- function(...) {\n  dots &lt;- list(...)\n  if (length(dots) == 1 && is.list(dots[[1]])) {\n    dots &lt;- dots[[1]]\n  }\n\n  # Do something\n  ...\n}\nRCurl::getURL()函数采取了另外一种技巧：同时处理...和.dots。\nf &lt;- function(..., .dots) {\n  dots &lt;- c(list(...), .dots)\n  # Do something\n}",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/19 Quasiquotation.html#case-studies",
    "href": "Books/Advanced R(2e)/19 Quasiquotation.html#case-studies",
    "title": "19 Quasiquotation",
    "section": "Case studies",
    "text": "Case studies\n为了使准引用的概念具体化，本节包含一些用它来解决实际问题的小案例，这些案例还使用了purrr。\n\nlobstr::ast()\n准引用使得lobstr::ast()的输入可以是一个“表达式”。\n\nz &lt;- expr(foo(x, y))\nlobstr::ast(z)\n#&gt; z\n\nlobstr::ast(!!z)\n#&gt; █─foo \n#&gt; ├─x \n#&gt; └─y\n\n\n\nMap-reduce to generate code\n准引用赋予了我们强大的“生成”代码的能力，尤其是与purrr::map()及purrr::reduce()结合使用时。例如，假设你有一个由以下系数指定的线性模型，你需要将其转换为一个函数式——10 + (x1 * 5) + (x2 * -4)。\n\nintercept &lt;- 10\ncoefs &lt;- c(x1 = 5, x2 = -4)\n\n你可以遵循以下步骤构建函数式：\n\n首先使用rlang::syms()将名称x1,x2转换为符号列表。\n\n\ncoef_sym &lt;- syms(names(coefs))\ncoef_sym\n#&gt; [[1]]\n#&gt; x1\n#&gt; \n#&gt; [[2]]\n#&gt; x2\n\n\n然后使用rlang::expr()和purrr::map2()将系数与对应的未知数组合。\n\n\nsummands &lt;- map2(coef_sym, coefs, ~ expr((!!.x * !!.y)))\nsummands\n#&gt; [[1]]\n#&gt; (x1 * 5)\n#&gt; \n#&gt; [[2]]\n#&gt; (x2 * -4)\n\n\n最后使用purrr::reduce()将所有项组合成一个表达式。\n\n\nreduce(c(intercept, summands), ~ expr(!!.x + !!.y))\n#&gt; 10 + (x1 * 5) + (x2 * -4)\n\n\n\nSlicing an array\nbase R中缺少一个根据给定维度和索引从数组中提取切片的函数。例如，编写函数slice(x, 2, 1)即x[, 1, ]，实现沿着第二个维度提取第一个切片。这是一个略具挑战性的问题，因为它要处理缺失的参数。\n我们采取的策略是首先默认全都是缺失的参数，然后根据提供的维度和索引，替换缺失的参数。\n\nindices &lt;- rep(list(missing_arg()), 3)\nexpr(x[!!!indices])\n#&gt; x[, , ]\n\nindices[[2]] &lt;- 1\nexpr(x[!!!indices])\n#&gt; x[, 1, ]\n\n我们可以进一步优化为一个函数：\n\nslice &lt;- function(x, along, index) {\n  stopifnot(length(along) == 1)\n  stopifnot(length(index) == 1)\n\n  nd &lt;- length(dim(x))\n  indices &lt;- rep(list(missing_arg()), nd)\n  indices[[along]] &lt;- index\n\n  expr(x[!!!indices])\n}\n\nx &lt;- array(sample(30), c(5, 2, 3))\nslice(x, 1, 3)\n#&gt; x[3, , ]\nslice(x, 2, 2)\n#&gt; x[, 2, ]\nslice(x, 3, 1)\n#&gt; x[, , 1]\n\n一个真实的slice()函数会在第20章中介绍。\n\n\nCreating functions\n另外一个有用的应用是使用rlang::new_function()“手动”创建函数，创建函数的三要素——参数，主体，环境。\n\nnew_function(\n  exprs(x = , y = ),\n  expr({\n    x + y\n  })\n)\n#&gt; function (x, y) \n#&gt; {\n#&gt;     x + y\n#&gt; }\n\n注意：exprs()中的空参数产生没有默认值的函数参数。\nnew_function()一个用法是作为函数工厂的替代，例如之前讲到的幂函数：\n\npower &lt;- function(exponent) {\n  new_function(\n    exprs(x = ),\n    expr({\n      x^!!exponent\n    }),\n    caller_env()\n  )\n}\npower(0.5)\n#&gt; function (x) \n#&gt; {\n#&gt;     x^0.5\n#&gt; }\n\nnew_function()的另一个用法是用于类似graphics::curve()这样的函数，它能够绘制数学表达式，而无需创建函数。\n\ncurve(sin(exp(4 * x)), n = 1000)\n\n\n\n\n\n\n\n\n在这段代码中，x是一个代词：它不表示单一的具体值，而是一个在图形范围内变化的占位符。实现curve()的一种方法是将该表达式转换为一个带有单个参数x的函数，然后调用该函数。\n\ncurve2 &lt;- function(expr, xlim = c(0, 1), n = 100) {\n  expr &lt;- enexpr(expr)\n  f &lt;- new_function(exprs(x = ), expr)\n\n  x &lt;- seq(xlim[1], xlim[2], length = n)\n  y &lt;- f(x)\n\n  plot(x, y, type = \"l\", ylab = expr_text(expr))\n}\ncurve2(sin(exp(4 * x)), n = 1000)\n\n\n\n\n\n\n\n\nf &lt;- new_function(exprs(x = ), expr(sin(exp(4 * x))))\n\n像curve()这样使用包含代词的表达式的函数被称为指代函数（anaphoric functions）。",
    "crumbs": [
      "19 Quasiquotation"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html",
    "href": "Books/Advanced R(2e)/17 Big picture.html",
    "title": "17 Big picture",
    "section": "",
    "text": "元编程（metaprogramming）可以说是本书中最难的章节，因为它需要你结果之前不相关的章节来处理你之前从未想过的问题，并且你会接触到大量的抽象概念。即使你是一个使用另一种语言的经验丰富的程序员，你现有的技能也不太可能提供太多帮助，因为现代流行语言很少有能够有R所提供的元编程水平。所以，如果你一开始感到沮丧或困惑，不要惊讶；这是这个过程中每个人都会经历的自然过程！\n但在过去几年中，元编程的理论和实践已经大幅成熟，为解决常见问题提供了坚实的基础和工具，学习元编程比以往任何时候都更容易。在本章中，你将了解所有主要元素的大致情况，以及它们是如何组合在一起的。\n\n\n\n17.2节：code is tree，使用抽象语法树（abstract syntax tree）将code转换为树。\n17.3节：code is data，创建或修改捕获的code字符串（使用树结构）。\n17.4节：如何以编程方式创建新表达式。\n17.5节：如何通过在环境中评估表达式来执行它们。\n17.6节：如何通过在新环境中提供自定义功能来自定义评估。\n17.7节：将这种自定义扩展到数据掩码，使环境和数据框之间的界限变得模糊。\n17.8节：引入一种新的数据结构，称为 Quoseure, 它使这一切变得更加简单和正确。\n\n\n\n\n我们使用“rlang”包来介绍上述内容，再后续章节中介绍base R中的等价物。同时使用“lobstr”包来查看code的树结构。\n\nlibrary(rlang)\nlibrary(lobstr)\n\n请确保你对环境和dataframe的数据结构十分了解。",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#introduction",
    "href": "Books/Advanced R(2e)/17 Big picture.html#introduction",
    "title": "17 Big picture",
    "section": "",
    "text": "元编程（metaprogramming）可以说是本书中最难的章节，因为它需要你结果之前不相关的章节来处理你之前从未想过的问题，并且你会接触到大量的抽象概念。即使你是一个使用另一种语言的经验丰富的程序员，你现有的技能也不太可能提供太多帮助，因为现代流行语言很少有能够有R所提供的元编程水平。所以，如果你一开始感到沮丧或困惑，不要惊讶；这是这个过程中每个人都会经历的自然过程！\n但在过去几年中，元编程的理论和实践已经大幅成熟，为解决常见问题提供了坚实的基础和工具，学习元编程比以往任何时候都更容易。在本章中，你将了解所有主要元素的大致情况，以及它们是如何组合在一起的。\n\n\n\n17.2节：code is tree，使用抽象语法树（abstract syntax tree）将code转换为树。\n17.3节：code is data，创建或修改捕获的code字符串（使用树结构）。\n17.4节：如何以编程方式创建新表达式。\n17.5节：如何通过在环境中评估表达式来执行它们。\n17.6节：如何通过在新环境中提供自定义功能来自定义评估。\n17.7节：将这种自定义扩展到数据掩码，使环境和数据框之间的界限变得模糊。\n17.8节：引入一种新的数据结构，称为 Quoseure, 它使这一切变得更加简单和正确。\n\n\n\n\n我们使用“rlang”包来介绍上述内容，再后续章节中介绍base R中的等价物。同时使用“lobstr”包来查看code的树结构。\n\nlibrary(rlang)\nlibrary(lobstr)\n\n请确保你对环境和dataframe的数据结构十分了解。",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#code-is-tree",
    "href": "Books/Advanced R(2e)/17 Big picture.html#code-is-tree",
    "title": "17 Big picture",
    "section": "Code is tree",
    "text": "Code is tree\n抽象语法树（abstract syntax tree）几乎是每个编程语言的底层逻辑。R不同的地方在于，可以真实得查看和修改树结构。\n使用lobstr::ast()函数可以直观地感受到代码就是一个树形结构。其中“函数”形成树地分支点，其他常量或变量形成树的叶子。\n\nlobstr::ast(f(a, \"b\"))\n#&gt; █─f \n#&gt; ├─a \n#&gt; └─\"b\"\n\n嵌套函数会创建更深的分支树：\n\nlobstr::ast(f1(f2(a, b), f3(1, f4(2))))\n#&gt; █─f1 \n#&gt; ├─█─f2 \n#&gt; │ ├─a \n#&gt; │ └─b \n#&gt; └─█─f3 \n#&gt;   ├─1 \n#&gt;   └─█─f4 \n#&gt;     └─2\n\n因为R中所有的函数都可以写成标准形式（function(x) x），所以所有的代码都会生成树形结构：\n\nlobstr::ast(1 + 2 * 3)\n#&gt; █─`+` \n#&gt; ├─1 \n#&gt; └─█─`*` \n#&gt;   ├─2 \n#&gt;   └─3",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#code-is-data",
    "href": "Books/Advanced R(2e)/17 Big picture.html#code-is-data",
    "title": "17 Big picture",
    "section": "Code is data",
    "text": "Code is data\n正因为R代码的底层是树形结构，所以R代码可以作为数据处理。lobstr::ast()只是直接展示了树形结构，我们需要使用rlang::expr()将这种树形结构数据类型提取出来。\n\nexpr(mean(x, na.rm = TRUE))\n#&gt; mean(x, na.rm = TRUE)\ntest &lt;- expr(10 + 100 + 1000)\ntest\n#&gt; 10 + 100 + 1000\nstr(test)\n#&gt;  language 10 + 100 + 1000\nclass(test)\n#&gt; [1] \"call\"\ntypeof(test)\n#&gt; [1] \"language\"\n\n通常称被捕获的代码为表达式（expression）。但表达式不仅只有“代码”（call），还有：变量（symbol），常量（constant），成对列表（pairlist）。\n当你在函数中使用expr()时会失效，它不会捕捉传入的参数，而是直接捕获它的输入：\n\ncapture_it &lt;- function(x) {\n  expr(x)\n}\ncapture_it(a + b + c)\n#&gt; x\n\n需要使用enexpr()函数来捕获传入的参数：\n\ncapture_it &lt;- function(x) {\n  enexpr(x)\n}\ncapture_it(a + b + c)\n#&gt; a + b + c\n\n一旦捕获了表达式，我们就可以修改它了。可以把树形结构数据当作列表处理，使用[[和$获取“元素”：\n\nast(f(x = 1, y = 2))\n#&gt; █─f \n#&gt; ├─x = 1 \n#&gt; └─y = 2\n\nf &lt;- expr(f(x = 1, y = 2))\n\n# Add a new argument\nf$z &lt;- 3\nf\n#&gt; f(x = 1, y = 2, z = 3)\n\n# Or remove an argument:\nf[[2]] &lt;- NULL\nf\n#&gt; f(y = 2, z = 3)\n\n需要注意：列表的第一个元素是函数本身f，修改第一个参数x使用[[2]]。",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#code-can-generate-code",
    "href": "Books/Advanced R(2e)/17 Big picture.html#code-can-generate-code",
    "title": "17 Big picture",
    "section": "Code can generate code",
    "text": "Code can generate code\n捕获代码后可以创建代码对应的树，相应地，我们可以将提取创建好地树转换为代码。\nrlang::call2()函数可以构建一个“调用函数代码”：第一个参数是函数名，后续是参数。\n\ncall2(\"f\", 1, 2, 3)\n#&gt; f(1, 2, 3)\ncall2(\"+\", 1, call2(\"*\", 2, 3))\n#&gt; 1 + 2 * 3\n\n除了call2()函数，我们也可以使用rlang::expr()和解引操作符（unquote operator）来生成代码，这种方式特别适合构建复杂的嵌套代码。\n\nxx &lt;- expr(x + x)\nyy &lt;- expr(y + y)\n\nexpr(!!xx / !!yy)\n#&gt; (x + x)/(y + y)\n\n注意：生成的是正确的语法(x + x) / (y + y)，不是x + x / y + y，也不是x + (x / y) + y。\n解引操作符在构建函数是会更有用：首先使用enexpr()捕获参数，然后使用!!和expr()创建新的代码。下面是一个生成计算变异系数代码的示例：\n\ncv &lt;- function(var) {\n  var &lt;- enexpr(var)\n  expr(sd(!!var) / mean(!!var))\n}\n\ncv(x)\n#&gt; sd(x)/mean(x)\ncv(x + y)\n#&gt; sd(x + y)/mean(x + y)\n\n即使输入是奇怪的字符，函数也可以正常运行：\n\ncv(`)`)\n#&gt; sd(`)`)/mean(`)`)",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#evaluation-runs-code",
    "href": "Books/Advanced R(2e)/17 Big picture.html#evaluation-runs-code",
    "title": "17 Big picture",
    "section": "Evaluation runs code",
    "text": "Evaluation runs code\n当捕获并修改代码后，我们可以提供一个运行环境，然后重新运行（evaluate）代码。\nbase::eval()函数有两个参数：1. 代码；2. 运行环境。\n\neval(expr(x + y), env(x = 1, y = 10))\n#&gt; [1] 11\neval(expr(x + y), env(x = 2, y = 100))\n#&gt; [1] 102\n\n如果环境参数缺失，会自动使用当前环境：\n\nx &lt;- 1\ny &lt;- 10\neval(expr(x + y))\n#&gt; [1] 11\n\n重新评估代码的一大优势是可以调整环境。这样做主要有两个原因：\n\n临时重写函数以实现特定于领域的语言。\n添加数据掩码，以便可以像引用环境中的变量一样引用数据框中的变量。",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#customising-evaluation-with-functions",
    "href": "Books/Advanced R(2e)/17 Big picture.html#customising-evaluation-with-functions",
    "title": "17 Big picture",
    "section": "Customising evaluation with functions",
    "text": "Customising evaluation with functions\n在重新评估代码时使用的环境中，我们不仅可以添加变量，还可以添加函数。这样我们就可以在特定的环境中重新命名已有的函数，例如：我们可以构建一个字符串的+和*函数。\n\nstring_math &lt;- function(x) {\n  e &lt;- env(\n    caller_env(),\n    `+` = function(x, y) paste0(x, y),\n    `*` = function(x, y) strrep(x, y)\n  )\n\n  eval(enexpr(x), e)\n}\n\nname &lt;- \"Hadley\"\nstring_math(\"Hello \" + name)\n#&gt; [1] \"Hello Hadley\"\nstring_math((\"x\" * 2 + \"-y\") * 3)\n#&gt; [1] \"xx-yxx-yxx-y\"\n\ndplyr将这个想法发挥到极致，在一个生成SQL并能在远程数据库中执行的环境中运行代码：\n\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), filename = \":memory:\")\nmtcars_db &lt;- copy_to(con, mtcars)\n\nmtcars_db %&gt;%\n  filter(cyl &gt; 2) %&gt;%\n  select(mpg:hp) %&gt;%\n  head(10) %&gt;%\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT `mpg`, `cyl`, `disp`, `hp`\n#&gt; FROM `mtcars`\n#&gt; WHERE (`cyl` &gt; 2.0)\n#&gt; LIMIT 10\n\nDBI::dbDisconnect(con)",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#customising-evaluation-with-data",
    "href": "Books/Advanced R(2e)/17 Big picture.html#customising-evaluation-with-data",
    "title": "17 Big picture",
    "section": "Customising evaluation with data",
    "text": "Customising evaluation with data\n重定义函数是一种极其强大的工具，但是它要大量的投入。相反，数据掩码是一种即时快速的应用—-直接引用数据框中的变量而不是从环境中获取。数据掩码技术在很多函数中应用：subset()、transform()、aes()、summarise()等等。虽然eval()函数能够实现数据掩码技术，但存在潜在的风险（20.6节），本节使用rlang::eval_tidy()函数来完成。\neval_tidy()函数同样接受代码和运行环境两个参数，但此处的环境特指数据框。\n\ndf &lt;- data.frame(x = 1:5, y = sample(5))\neval_tidy(expr(x + y), df)\n#&gt; [1]  5  5  4  6 10\n\n虽然数据掩码技术使得直接调用x + y成为可能，方便了交互使用，但同时也会造成歧义。在20.4节中我们会介绍使用.data，.env来避免歧义。\n搭配enexpr()函数，我们开一个创建适配数据掩码的“with”函数——with2()：\n\n# Unfortunately, this function has a subtle bug and we need a new data structure to help deal with it.\nwith2 &lt;- function(df, expr) {\n  eval_tidy(enexpr(expr), df)\n}\n\nwith2(df, x + y)\n#&gt; [1]  5  5  4  6 10",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/17 Big picture.html#quosures",
    "href": "Books/Advanced R(2e)/17 Big picture.html#quosures",
    "title": "17 Big picture",
    "section": "Quosures",
    "text": "Quosures\n上面的with2()函数存在潜在风险，它会优先调用函数运行环境中的变量：\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enexpr(expr), df)\n}\n\ndf &lt;- data.frame(x = 1:3)\na &lt;- 10\nwith2(df, x + a)\n#&gt; [1] 1001 1002 1003\n\n幸运地是，我们可以使用新的数据结构——quosure——来解决这个问题。eval_tidy()函数知道如何处理quosure，所以我们只需要使用enquo()函数来替换enexpr()函数：\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enquo(expr), df)\n}\n\nwith2(df, x + a)\n#&gt; [1] 11 12 13\n\n无论何时使用数据掩码，都必须始终使用enquo()而不是enexpr()。",
    "crumbs": [
      "17 Big picture"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html",
    "href": "Books/Advanced R(2e)/15 S4.html",
    "title": "15 S4",
    "section": "",
    "text": "S4 是一种更规范的基于泛型函数的面向对象系统。其底层思想与S3类似，但是有更严格的函数去创建类、泛型函数、方法——setClass()，setGeneric()，setMethod()。此外，S4 OOP 提供更多的继承和方法派发。\nS4 与S3 相比，还多了一个新的内容——slot（槽），使用特殊操作符@提取。\n\n\n\n15.2节：简要概述 S4 的主要组成部分：类、泛型函数和方法\n15.3节：深入 S4 “类”的细节，包括原型、构造函数、辅助函数和验证函数。\n15.4节：介绍如何创建新的 S4 “泛型函数”，以及如何为这些泛型函数提供方法；介绍访问器函数——旨在允许用户安全地检查和修改对象槽。\n15.5节：深入 S4 中“方法多分派”的全部细节。\n15.6节：讨论 S4 和 S3 之间的相互作用，及如何一起使用它们。\n\n\n\n\n与前几章类似，我们只关注S4 OOP如何工作，如何高效的使用它不在次范围内。在实践中，你需要额外注意：\n\n没有一本书可以回答你所有关于S4的问题。\nR的内置文档有时会与社区最佳实践发生冲突。\n\n在实践中，你需要仔细阅读文档，不断提问，多次实验。\n\n\n\n所有S4 OOP相关的函数都由“methods”包提供。虽然该包总是会在终端交互时加载，但当使用Rscritp时则不会，所以当我们使用S4 OOP时，最好加载它。\n\nlibrary(methods)",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#introduction",
    "href": "Books/Advanced R(2e)/15 S4.html#introduction",
    "title": "15 S4",
    "section": "",
    "text": "S4 是一种更规范的基于泛型函数的面向对象系统。其底层思想与S3类似，但是有更严格的函数去创建类、泛型函数、方法——setClass()，setGeneric()，setMethod()。此外，S4 OOP 提供更多的继承和方法派发。\nS4 与S3 相比，还多了一个新的内容——slot（槽），使用特殊操作符@提取。\n\n\n\n15.2节：简要概述 S4 的主要组成部分：类、泛型函数和方法\n15.3节：深入 S4 “类”的细节，包括原型、构造函数、辅助函数和验证函数。\n15.4节：介绍如何创建新的 S4 “泛型函数”，以及如何为这些泛型函数提供方法；介绍访问器函数——旨在允许用户安全地检查和修改对象槽。\n15.5节：深入 S4 中“方法多分派”的全部细节。\n15.6节：讨论 S4 和 S3 之间的相互作用，及如何一起使用它们。\n\n\n\n\n与前几章类似，我们只关注S4 OOP如何工作，如何高效的使用它不在次范围内。在实践中，你需要额外注意：\n\n没有一本书可以回答你所有关于S4的问题。\nR的内置文档有时会与社区最佳实践发生冲突。\n\n在实践中，你需要仔细阅读文档，不断提问，多次实验。\n\n\n\n所有S4 OOP相关的函数都由“methods”包提供。虽然该包总是会在终端交互时加载，但当使用Rscritp时则不会，所以当我们使用S4 OOP时，最好加载它。\n\nlibrary(methods)",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#basics",
    "href": "Books/Advanced R(2e)/15 S4.html#basics",
    "title": "15 S4",
    "section": "Basics",
    "text": "Basics\n\nsetClass()\nsetClass()可以定义一个类——类名和槽。在数据槽中可以规定数据类型和名字。\n\nsetClass(\n  \"Person\",\n  slots = c(\n    name = \"character\",\n    age = \"numeric\"\n  )\n)\n\n类定义完毕后，可以使用new()函数实例化一个对象。\n\njohn &lt;- new(\"Person\", name = \"John Smith\", age = NA_real_)\n\n使用is()函数检查对象的类名；使用@或slot()函数获取对象中的槽值。\n\nis(john)\n#&gt; [1] \"Person\"\njohn@name\n#&gt; [1] \"John Smith\"\nslot(john, \"age\")\n#&gt; [1] NA\n\n\n\nsetGeneric()\nsetGeneric()可以定义一个泛型函数。我们可以创建一个提取年龄信息的泛型函数。\n\nsetGeneric(\"age\", function(x) standardGeneric(\"age\"))\n#&gt; [1] \"age\"\nsetGeneric(\"age&lt;-\", function(x, value) standardGeneric(\"age&lt;-\"))\n#&gt; [1] \"age&lt;-\"\n\n\n\nsetMethod()\nsetMethod()可以定义一个方法。我们根据上面的泛型方法，定义属于Person类的age方法。\n\nsetMethod(\"age\", \"Person\", function(x) x@age)\nsetMethod(\"age&lt;-\", \"Person\", function(x, value) {\n  x@age &lt;- value\n  x\n})\n\nage(john) &lt;- 18\nage(john)\n#&gt; [1] 18\n\n假设Person类属于某个R包，你可以使用class?Person来获取该类的帮助文档。在方法调用前添加?，如?age(john)，可以获取方法的帮助文档。\n最后，你可以使用“sloop”包中的函数检查类和泛型函数。\n\nsloop::otype(john)\n#&gt; [1] \"S4\"\nsloop::ftype(age)\n#&gt; [1] \"S4\"      \"generic\"",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#classes",
    "href": "Books/Advanced R(2e)/15 S4.html#classes",
    "title": "15 S4",
    "section": "Classes",
    "text": "Classes\n定义S4类时，通常要设定setClass()中的三个参数：\n\nclass name：类名，通常类名使用UpperCamelCase。\nslots：一个有name属性的字符串向量，描述了槽的名字和数据类型。ANY可以表示任意类型数据。\nprototype：一个有name属性的列表，记录了每个槽中的默认值。\n\n\nsetClass(\n  \"Person\",\n  slots = c(\n    name = \"character\",\n    age = \"numeric\"\n  ),\n  prototype = c(\n    name = NA_character_,\n    age = NA_real_\n  )\n)\n\nme &lt;- new(\"Person\", name = \"James\")\nstr(me)\n#&gt; Formal class 'Person' [package \".GlobalEnv\"] with 2 slots\n#&gt;   ..@ name: chr \"James\"\n#&gt;   ..@ age : num(0) \n#&gt;   ..$ names: chr [1:2] \"name\" \"age\"\n\n\nInheritance\nsetClass()中的contains参数可以指定一个类，表示当前类继承自该类。例如，我们可以创建一个Employee类，继承自Person类，添加额外槽boss。\n\nsetClass(\n  \"Employee\",\n  contains = \"Person\",\n  slots = c(\n    boss = \"Person\"\n  ),\n  prototype = list(\n    boss = new(\"Person\")\n  )\n)\n\nstr(new(\"Employee\"))\n#&gt; Formal class 'Employee' [package \".GlobalEnv\"] with 3 slots\n#&gt;   ..@ boss:Formal class 'Person' [package \".GlobalEnv\"] with 2 slots\n#&gt;   .. .. ..@ name: chr(0) \n#&gt;   .. .. ..@ age : num(0) \n#&gt;   .. .. ..$ names: chr [1:2] \"name\" \"age\"\n#&gt;   ..@ name: chr(0) \n#&gt;   ..@ age : num(0)\n\nSetClass()还有9个其他参数，但这些参数要么已被弃用，要么不推荐使用。\n\n\nIntrospection\nis()可以查看对象所属的类及其继承类。\n\nis(new(\"Person\"))\n#&gt; [1] \"Person\"\nis(new(\"Employee\"))\n#&gt; [1] \"Employee\" \"Person\"\n\n此外，还可以用作判断函数。\n\nis(john, \"Person\")\n#&gt; [1] TRUE\n\n\n\nRedefinition\n因为类与对象的定义都是环境中的一个变量，所以在实例化一个类的对象后，依然可以重新定义这个类，此时要注意已经实例化的对象可能无效。\n\nsetClass(\"A\", slots = c(x = \"numeric\"))\na &lt;- new(\"A\", x = 10)\n\nsetClass(\"A\", slots = c(a_different_slot = \"numeric\"))\na\n#&gt; An object of class \"A\"\n#&gt; Slot \"a_different_slot\":\n#&gt; Error in slot(object, what): no slot of name \"a_different_slot\" for this object of class \"A\"\n\n\n\nHelper\nnew()函数是一种非常原始的实例化函数，很适合开发者；但对于使用者来说，new()函数不太友好，我们需要一个面向用户的辅助函数，更好的实例化对象。辅助函数应始终：\n\n使用与类相同的名称，例如myclass()。\n精心设计的用户界面，包含精心选择的默认值和有用的转换。\n为最终用户创建精心设计的错误消息。\n最后调用ethods::new()。\n\n我们可以创建为“Person”类创建一个简单的辅助函数：\n\nPerson &lt;- function(name, age = NA) {\n  age &lt;- as.double(age)\n\n  new(\"Person\", name = name, age = age)\n}\n\nPerson(\"Hadley\")\n#&gt; An object of class \"Person\"\n#&gt; Slot \"name\":\n#&gt; [1] \"Hadley\"\n#&gt; \n#&gt; Slot \"age\":\n#&gt; [1] NA\n\n\n\nValidator\nS4类的构造函数会根据slots中的类型进行验证。\n\nPerson(mtcars)\n#&gt; Error in validObject(.Object): invalid class \"Person\" object: invalid object for slot \"name\" in class \"Person\": got class \"data.frame\", should be or extend class \"character\"\n\n但有时我们需要做出更多的判断，例如假设你想让“Person”类可以接受多个“人”的名字和年龄，那么名字和年龄的长度应该一致。\n\nPerson(\"Hadley\", age = c(30, 37))\n#&gt; An object of class \"Person\"\n#&gt; Slot \"name\":\n#&gt; [1] \"Hadley\"\n#&gt; \n#&gt; Slot \"age\":\n#&gt; [1] 30 37\n\n此时我们需要使用setValidity()方法来定义额外的验证逻辑。setValidity()方法接收两个参数——类名和验证函数，最终返回一个字符串信息（未通过）或TRUE（通过）。\n\nsetValidity(\"Person\", function(object) {\n  if (length(object@name) != length(object@age)) {\n    \"@name and @age must be same length\"\n  } else {\n    TRUE\n  }\n})\n#&gt; Class \"Person\" [in \".GlobalEnv\"]\n#&gt; \n#&gt; Slots:\n#&gt;                           \n#&gt; Name:       name       age\n#&gt; Class: character   numeric\n#&gt; \n#&gt; Known Subclasses: \"Employee\"\n\n现在，Person()构造函数会返回错误信息。\n\nPerson(\"Hadley\", age = c(30, 37))\n#&gt; Error in validObject(.Object): invalid class \"Person\" object: @name and @age must be same length\n\n但是要注意，此时验证函数依然只能通过函数new()来触发，在修改对象槽值时，验证函数不会被触发。只能使用validObject()函数来检查对象。\n\nalex &lt;- Person(\"Alex\", age = 30)\nalex@age &lt;- 1:10\n\nvalidObject(alex)\n#&gt; Error in validObject(alex): invalid class \"Person\" object: @name and @age must be same length",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#generics-and-methods",
    "href": "Books/Advanced R(2e)/15 S4.html#generics-and-methods",
    "title": "15 S4",
    "section": "Generics and methods",
    "text": "Generics and methods\n创建新的S4类的泛型函数的方式为：在setGeneric()函数中调用standardGeneric()函数。\nsetGeneric(\"myGeneric\", function(x) standardGeneric(\"myGeneric\"))\n通常规定，新的泛型函数名采用lowerCamelCase命名方式。\n同时，不要在泛型函数中使用{}，因为这会触发一种特殊情况，只要必要时才会这样做。\n# Don't do this!\nsetGeneric(\"myGeneric\", function(x) {\n  standardGeneric(\"myGeneric\")\n})\n\nSignature\n与setClass()函数类似，setGeneric()函数同样拥有很多参数，但只需了解signature参数即可，该参数控制了方法派发。该参数的值来源只能是函数参数，若未指定，则会使用除...外所有的函数参数。在派发过程中，使用的是参数的类信息，如果你有类似verbose = TRUE或quiet = FALSE等无需参与方法派发的参数，最好为signature参数提供具体值。\nsetGeneric(\"myGeneric\",\n  function(x, ..., verbose = TRUE) standardGeneric(\"myGeneric\"),\n  signature = \"x\"\n)\n\n\nMethods\n使用setMethod()函数可以创建S4类的方法，该函数有三个参数：泛型函数、singnature、函数定义，其余参数无需了解。此处的“signature”用来规定函数中的参数x的类（可以是多个）。与S3不同，S4的方法派发根据signature会有多种形式。我们将在下一节中详细介绍方法派发。\nsetMethod(\"myGeneric\", \"Person\", function(x) {\n  # method implementation\n})\n要列出属于某个泛型函数或与类关联的所有方法，可以使用methods(\"generic\")或methods(class = \"class\")；要查找特定方法的实现，可以使用selectMethod(\"generic\", \"class\")。\n\n\nShow method\nS4类有一些通用的泛型函数，如show，用于控制对象打印的信息。可以查看文档或使用args(getGeneric(\"show\"))查看参数。\n\nargs(getGeneric(\"show\"))\n#&gt; function (object) \n#&gt; NULL\n\n可以看到“show”方法接受唯一的参数——对象本身。\n\nsetMethod(\"show\", \"Person\", function(object) {\n  cat(is(object)[[1]], \"\\n\",\n    \"  Name: \", object@name, \"\\n\",\n    \"  Age:  \", object@age, \"\\n\",\n    sep = \"\"\n  )\n})\njohn\n#&gt; Person\n#&gt;   Name: John Smith\n#&gt;   Age:  18\n\n\n\nAccessors\n始终应该为slot中的值提供访问器函数，方便访问与修改。\n如果只有一个槽，访问器函数可以是：\nperson_name &lt;- function(x) {x@name}\n也可以通过泛型函数定义，使得多个类可以同时使用：\n\nsetGeneric(\"name\", function(x) standardGeneric(\"name\"))\n#&gt; [1] \"name\"\nsetMethod(\"name\", \"Person\", function(x) x@name)\n\nname(john)\n#&gt; [1] \"John Smith\"\n\n如果需要修改槽的值，还需要定义一个能修改值的访问器函数，函数在返回对象前执行validObject()检查。\n\nsetGeneric(\"name&lt;-\", function(x, value) standardGeneric(\"name&lt;-\"))\n#&gt; [1] \"name&lt;-\"\nsetMethod(\"name&lt;-\", \"Person\", function(x, value) {\n  x@name &lt;- value\n  validObject(x)\n  x\n})\n\nname(john) &lt;- \"Jon Smythe\"\nname(john)\n#&gt; [1] \"Jon Smythe\"\n\nname(john) &lt;- letters\n#&gt; Error in validObject(x): invalid class \"Person\" object: @name and @age must be same length\n\n\n\n\n\n\n\nWarning\n\n\n\n直接使用class@slot &lt;- value的方式也可以修改槽的值，但会忽略validObject()检查。\n\njohn@name &lt;- letters\nname(john)\n#&gt;  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\"\n#&gt; [19] \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#method-dispatch",
    "href": "Books/Advanced R(2e)/15 S4.html#method-dispatch",
    "title": "15 S4",
    "section": "Method dispatch",
    "text": "Method dispatch\nS4 OOP中的方法派发极其复杂，因为：\n\n多种继承：例如一个类可以继承多个类。\n多种派发方法：例如一个泛型函数可以根据多个参数来派发方法。\n\nS4类的特点使得其很强大，但也使其很难理解。在实践中，除非必要，我们要尽可能避免使用多种继承和多种派发方法。\n我们将使用一个基于表情符号的虚拟类图，从单一的继承与派发到多个继承与派发，逐步介绍S4类中的方法派发。\n\n😜继承自😉，😉继承自😶；😎继承自🕶和🙂。\n\nSingle dispatch\n让我们以基于某个继承单一的类为例，介绍方法派发的过程。\n\n上面的模式图由两部分构成：\n\n顶部，f(...)中的参数类型，定义了派发的逻辑图。图中的参数只有一个且单一继承了其他类，深度是2。\n底部，展示了所有可能被定义的方法间的关系。我们假定如果方法被定义，则背景色为灰色。\n\n方法派发的实际过程就是从实际参数中最具体的类开始，然后跟随箭头，直到找到一个存在的方法。例如你想找到😜的方法，跟随箭头最终找到的是它的父类😶的方法。如果没有找到方法，R会抛出错误，表示方法派发失败。这意味着，你必须为最终的类定义一个方法。\nS4类中有两个“伪类”（pseudo-classes）用来定义方法。被称为“伪类”是因为它们不是真实存在，但你可以为它们定义方法。\n第一个伪类是ANY，它表示任何类，同时某个类与ANY的距离比其与其他类的距离都要大，用来准确地派发方法。\n\n第二个伪类是MISSING。如果你为该类定义了某个方法，那么这个方法可以匹配任何没有参数的泛函。它在“single dispatch”模式下没有用，主要用于类似+,-等“double dispatch”模式下。\n\n\nMultiple dispatch\n当类的继承有多个时，方法派发会变得复杂起来。\n\n派发的原理与单一继承类似——从初始类开始，沿着箭头向右寻找，直到找到一个方法；区别在于，可能找到多个方法，此时会选择最近的一个方法（例如，箭头数最少）。需要注意的是，真实的方法派发逻辑不是我们图中“箭头”组成的“图”。\n假如出现距离相同，如下：🕶和🙂都有定义的方法而且😶没有方法，此时，R 会按照字母顺序选择方法。图中红色双线框表示没有方法，虚线框表示方法模糊。\n\n当这种情况发生时，应当为该类提供一个准确的方法。\n\n如果最终找到的是ANY类的方法，因为ANY类与其他类的距离相比真实类间的距离更远，所以永远不会产生歧义。\n\n使用多重继承时，很难防止歧义，需要谨慎使用，尽可能减少方法数量，并根据图进行规划。例如下图中，只有两种符合数量尽可能少（左上，右上？），而且只有一种没有问题（右上？）。\n\n\n\nMultiple dispatch\n一旦你理解了多重继承，多重派发就会很直观，只是每个方法都需要两个类去调用。\n\n下面是一个方法歧义的示意图。\n\n实际上，多重派发要比多重继承更容易处理，因为它的终端类组合数量更少。\n\n\nMultiple dispatch and multiple inheritance\n当然，我们也可以将多重派发与多重继承组合起来使用。\n\n一个更复杂的案例分发到两个类，这两个类都具有多重继承：\n\n随着方法图变得越来越复杂，在给定输入组合的情况下，预测哪个方法将被调用变得越来越困难，同时也越来越难以确保你没有引入歧义。如果你不得不绘制图表来确定实际将被调用的方法，这强烈表明你应该回过头来简化你的设计。",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/15 S4.html#s4-and-s3",
    "href": "Books/Advanced R(2e)/15 S4.html#s4-and-s3",
    "title": "15 S4",
    "section": "S4 and S3",
    "text": "S4 and S3\nS4类可以与已存在的S3类进行交互。\n\nClasses\nsetClass()函数的参数slots,contains可以使用S4类或S3类，甚至implicit类。\n在使用S3类前，必须使用setOldClass()函数来重定义S3类。定义声明只需一次即可，例如下面是被base R重定义的S3类：\nsetOldClass(\"data.frame\")\nsetOldClass(c(\"ordered\", \"factor\"))\nsetOldClass(c(\"glm\", \"lm\"))\n你也可以使用setClass()函数来重定义S3类，提供更具体地描述。\nsetClass(\"factor\",\n  contains = \"integer\",\n  slots = c(\n    levels = \"character\"\n  ),\n  prototype = structure(\n    integer(),\n    levels = character()\n  )\n)\nsetOldClass(\"factor\", S4Class = \"factor\")\n注意：对S3类的重定义行为通常由S3类的拥有者提供。例如，如果你想在某个S3类的基础上定义S4类，你需要让S3类的拥有者提供重定义行为。\n如果S4类继承于某个S3类或base type，那么该S4类的实例化对象中会自动添加.Data槽，用来存放原始数据。\n\nRangedNumeric &lt;- setClass(\n  \"RangedNumeric\",\n  contains = \"numeric\",\n  slots = c(min = \"numeric\", max = \"numeric\"),\n  prototype = structure(numeric(), min = NA_real_, max = NA_real_)\n)\nrn &lt;- RangedNumeric(1:10, min = 1, max = 10)\nrn@min\n#&gt; [1] 1\nrn@.Data\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\n你也可以为S4类定义S3类型的方法，反之亦然（使用setOldClass()）。阅读?Methods_for_S3了解更多信息。\n\n\nGenerics\n使用setGeneric()还可以将S3类的泛型函数转换为S4类的泛型函数。\n\nsetGeneric(\"mean\")\n#&gt; [1] \"mean\"\n\n这种情况下，mean()函数会变为ANY类的默认方法。\n\nselectMethod(\"mean\", \"ANY\")\n#&gt; Method Definition (Class \"derivedDefaultMethod\"):\n#&gt; \n#&gt; function (x, ...) \n#&gt; UseMethod(\"mean\")\n#&gt; &lt;bytecode: 0x00000279bb99a6f8&gt;\n#&gt; &lt;environment: namespace:base&gt;\n#&gt; \n#&gt; Signatures:\n#&gt;         x    \n#&gt; target  \"ANY\"\n#&gt; defined \"ANY\"\n\n注意：如何setMethod()的首个参数不是已经存在的泛型函数，它会自动调用setGeneric()。应该避免将常规函数转换为S4类的泛型函数。",
    "crumbs": [
      "15 S4"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html",
    "href": "Books/Advanced R(2e)/13 S3.html",
    "title": "13 S3",
    "section": "",
    "text": "R 的第一个OOP是S3面向对象，它的唯一优点是极其简单——无法再扔掉它的任何一部分来实现OOP。S3面向对象是R base和stats包中唯一使用的OOP。\nS3面向对象不像其他语言中那么严格，赋予了R编程人员很大的自由，这是危险的；为了能更好的使用S3面向对象，本章会介绍使用S3面向对象时约定的惯例。\n本章的目的也仅是对S3面向对象如何工作进行介绍，不涉及如何有效地创建新的S3类。\n\n\n\n13.2节：总览S3类——类，泛型函数，方法\n13.3节：创建新的S3类时的细节，三个函数：constructor，helper，validator\n13.4节：介绍类与方法如何工作（方法派发），method dispatch\n13.5节：S3类的四种主要风格：vector，record，data frame，scalar\n13.6节：介绍S3的继承及如何添加类的关系\n13.7节：详细介绍类方法分派\n\n\n\n\nS3类与class属性密不可分，我们需要使用sloop包来查看S3类的内部结构。\n\nlibrary(sloop)",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#introduction",
    "href": "Books/Advanced R(2e)/13 S3.html#introduction",
    "title": "13 S3",
    "section": "",
    "text": "R 的第一个OOP是S3面向对象，它的唯一优点是极其简单——无法再扔掉它的任何一部分来实现OOP。S3面向对象是R base和stats包中唯一使用的OOP。\nS3面向对象不像其他语言中那么严格，赋予了R编程人员很大的自由，这是危险的；为了能更好的使用S3面向对象，本章会介绍使用S3面向对象时约定的惯例。\n本章的目的也仅是对S3面向对象如何工作进行介绍，不涉及如何有效地创建新的S3类。\n\n\n\n13.2节：总览S3类——类，泛型函数，方法\n13.3节：创建新的S3类时的细节，三个函数：constructor，helper，validator\n13.4节：介绍类与方法如何工作（方法派发），method dispatch\n13.5节：S3类的四种主要风格：vector，record，data frame，scalar\n13.6节：介绍S3的继承及如何添加类的关系\n13.7节：详细介绍类方法分派\n\n\n\n\nS3类与class属性密不可分，我们需要使用sloop包来查看S3类的内部结构。\n\nlibrary(sloop)",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#basics",
    "href": "Books/Advanced R(2e)/13 S3.html#basics",
    "title": "13 S3",
    "section": "Basics",
    "text": "Basics\n\n类，属性\nS3对象是一个至少有1个class属性的base type。例如factor，它的base type是integer，除此之外还有class属性和levels属性。\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"))\n\ntypeof(f)\n#&gt; [1] \"integer\"\nattributes(f)\n#&gt; $levels\n#&gt; [1] \"a\" \"b\" \"c\"\n#&gt; \n#&gt; $class\n#&gt; [1] \"factor\"\n\n使用unclass()可以去除S3对象的class属性，暴露出其底层的base type，同时失去对象的特有行为。\n\nunclass(f)\n#&gt; [1] 1 2 3\n#&gt; attr(,\"levels\")\n#&gt; [1] \"a\" \"b\" \"c\"\n\n\n\n方法\nS3类方法的实现是基于“泛型函数”的。识别泛型函数的一种简单方法是使用sloop::ftype()，如果结果中有”generic”字样，就是泛型函数。\n\nftype(print)\n#&gt; [1] \"S3\"      \"generic\"\nftype(str)\n#&gt; [1] \"S3\"      \"generic\"\nftype(unclass)\n#&gt; [1] \"primitive\"\n\n不同S3对象输入到同一个泛型函数时会产生不同的结果（多态）。底层逻辑是，根据对象class属性，选择对应的方法。许多R函数属于泛型函数，例如print()。\n\nprint(f)\n#&gt; [1] a b c\n#&gt; Levels: a b c\n\n# stripping class reverts to integer behaviour\nprint(unclass(f))\n#&gt; [1] 1 2 3\n#&gt; attr(,\"levels\")\n#&gt; [1] \"a\" \"b\" \"c\"\n\n我们知道面向对象系统有一个重要特征——“封装”。S3系统的泛型函数同样具有封装的特性，会隐藏对象中的细节。例如常用的泛型函数str()，虽然是用来查看对象的结构，但你所看到的是对象想让你看到的，没有展示对象内部详细的细节。POSIXlt类用来表示时间，使用str()只能看到年月日，但其底层是一个list。\n\ntime &lt;- strptime(c(\"2017-01-01\", \"2020-05-04 03:21\"), \"%Y-%m-%d\")\nstr(time)\n#&gt;  POSIXlt[1:2], format: \"2017-01-01\" \"2020-05-04\"\n\nstr(unclass(time))\n#&gt; List of 11\n#&gt;  $ sec   : num [1:2] 0 0\n#&gt;  $ min   : int [1:2] 0 0\n#&gt;  $ hour  : int [1:2] 0 0\n#&gt;  $ mday  : int [1:2] 1 4\n#&gt;  $ mon   : int [1:2] 0 4\n#&gt;  $ year  : int [1:2] 117 120\n#&gt;  $ wday  : int [1:2] 0 1\n#&gt;  $ yday  : int [1:2] 0 124\n#&gt;  $ isdst : int [1:2] 0 0\n#&gt;  $ zone  : chr [1:2] \"CST\" \"CST\"\n#&gt;  $ gmtoff: int [1:2] NA NA\n#&gt;  - attr(*, \"tzone\")= chr [1:3] \"\" \"CST\" \"CDT\"\n#&gt;  - attr(*, \"balanced\")= logi TRUE\n\n\n\n方法派发\n泛型函数会为不同类找到正确的实现，类的特定实现称为方法（method），泛型函数通过执行方法分派（method dispatch）来找到对应的方法。\n可以使用sloop::s3_dispatch()函数查看方法派发的过程。如下面所示，找到了当前print的多个方法print.factor和print.default，最终选择了print.factor。\n\ns3_dispatch(print(f))\n#&gt; =&gt; print.factor\n#&gt;  * print.default\n\n从结果中可以看到，方法的命名规则是generic.class()。通常你无需使用generic.class()形式的函数，只需使用generic()，它会根据输入对象的class属性自动识别。因为某些函数的定义早于S3系统，所以类型&lt;&gt;.&lt;&gt;的函数不都是泛函，可以使用sloop::ftype()进行识别。\n\nftype(t.test)\n#&gt; [1] \"S3\"      \"generic\"\nftype(t.data.frame)\n#&gt; [1] \"S3\"     \"method\"\n\nS3 系统的泛函通常无法看到源码，但是可以使用sloop::s3_get_method()查看。\n\nweighted.mean.Date\n#&gt; Error: object 'weighted.mean.Date' not found\n\ns3_get_method(weighted.mean.Date)\n#&gt; function (x, w, ...) \n#&gt; .Date(weighted.mean(unclass(x), w, ...))\n#&gt; &lt;bytecode: 0x000001e1d2610048&gt;\n#&gt; &lt;environment: namespace:stats&gt;",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#classes",
    "href": "Books/Advanced R(2e)/13 S3.html#classes",
    "title": "13 S3",
    "section": "Classes",
    "text": "Classes\nS3面向对象实例化一个对象只能使用structure()或calss&lt;-()函数——通过设置class属性的方法实现。\n\n# Create and assign class in one step\nx &lt;- structure(list(), class = \"my_class\")\n\n# Create, then set class\nx &lt;- list()\nclass(x) &lt;- \"my_class\"\n\n使用class()查看一个对象的类型，使用inherits(x, \"classname\")来判断一个对象是否属于某类。\n\nclass(x)\n#&gt; [1] \"my_class\"\ninherits(x, \"my_class\")\n#&gt; [1] TRUE\ninherits(x, \"your_class\")\n#&gt; [1] FALSE\n\nclassname 可以是任意字符串，但是推荐使用字母和_，杜绝使用.。在package中使用class时，推荐加上包名。\nS3面向对象没有类正确性检查，意味着可以随意更改一个对象的类：\n\n# Create a linear model\nmod &lt;- lm(log(mpg) ~ log(disp), data = mtcars)\nclass(mod)\n#&gt; [1] \"lm\"\nprint(mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(mpg) ~ log(disp), data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    log(disp)  \n#&gt;      5.3810      -0.4586\n\n# Turn it into a date (?!)\nclass(mod) &lt;- \"Date\"\n\n# Unsurprisingly this doesn't work very well\nprint(mod)\n#&gt; Error in as.POSIXlt(.Internal(Date2POSIXlt(x, tz)), tz = tz): 'list' object cannot be coerced to type 'double'\n\n为了避免上述问题发生，推荐创建三个函数：\n\n低级构造函数(Constructor)，new_myclass()：高效地创建结构正确的类。\n验证函数(validator)，validate_myclass()：验证类的合法性。\n辅助函数(helper)，myclass()：用户友好的创建类。\n\n\nConstructors\nS3 面向对象没有提供正式的类定义函数，无法保证类的结构统一，我们需要自行创建一个构造函数，保证类的构造正确。构造函数应当遵循下面三条原则：\n\n函数命名为new_myclass()样式。\n一个参数服务于构成类的base type；每个属性一个参数。\n检查base type和每个参数的类型是否符合要求。\n\n下面是一个具有units属性，class属性为difftime的类构造函数：\n\nnew_difftime &lt;- function(x = double(), units = \"secs\") {\n  stopifnot(is.double(x))\n  units &lt;- match.arg(units, c(\"secs\", \"mins\", \"hours\", \"days\", \"weeks\"))\n\n  structure(x,\n    class = \"difftime\",\n    units = units\n  )\n}\n\nnew_difftime(c(1, 10, 3600), \"secs\")\n#&gt; Time differences in secs\n#&gt; [1]    1   10 3600\n\nnew_difftime(52, \"weeks\")\n#&gt; Time difference of 52 weeks\n\n构造函数面向的是开发者，这意味着，有些时候，我们可以牺牲一些安全性来换取高效性，例如避免耗时项的检查。\n\n\nValidators\n越复杂的类越需要细致地检查。如下例factor类，虽然有对输入类型检查，但仍然创建了错误的类。\n\nnew_factor &lt;- function(x = integer(), levels = character()) {\n  stopifnot(is.integer(x))\n  stopifnot(is.character(levels))\n\n  structure(\n    x,\n    levels = levels,\n    class = \"factor\"\n  )\n}\n\nnew_factor(1:5, \"a\")\n#&gt; Error in as.character.factor(x): malformed factor\nnew_factor(0:1, \"a\")\n#&gt; Error in as.character.factor(x): malformed factor\n\n想比于构造函数中对base type和参数类型的检查，对类输入的其他检查应该分离为单独的函数。这样当你知道输入是正确时，可以以更低的成本创建类，并且检查方法可以用在其他地方。\n\nvalidate_factor &lt;- function(x) {\n  values &lt;- unclass(x)\n  levels &lt;- attr(x, \"levels\")\n\n  if (!all(!is.na(values) & values &gt; 0)) {\n    stop(\n      \"All `x` values must be non-missing and greater than zero\",\n      call. = FALSE\n    )\n  }\n\n  if (length(levels) &lt; max(values)) {\n    stop(\n      \"There must be at least as many `levels` as possible values in `x`\",\n      call. = FALSE\n    )\n  }\n\n  x\n}\n\nvalidate_factor(new_factor(1:5, \"a\"))\n#&gt; Error: There must be at least as many `levels` as possible values in `x`\nvalidate_factor(new_factor(0:1, \"a\"))\n#&gt; Error: All `x` values must be non-missing and greater than zero\n\n\n\nHelpers\n为了方便使用者创建类对象，我们需要提供一个辅助函数。其遵循以下原则：\n\n函数名和类名相同，myclass()。\n如果存在constructor和validator，函数要使用它们。\n为终端用户创建精心设计的错误消息。。\n有一个精心设计的用户界面，精心选择的默认值和有用的转换\n\n下面是三个常见的示例：\n\n输入参数类型强制转换\n例如上面的new_difftime()对输入十分严格，当输入是integer时报错。\n\nnew_difftime(1:10)\n#&gt; Error in new_difftime(1:10): is.double(x) is not TRUE\n\n可以在helper函数中添加类型强制转换：\n\ndifftime &lt;- function(x = double(), units = \"secs\") {\n  x &lt;- as.double(x)\n  new_difftime(x, units = units)\n}\n\ndifftime(1:10)\n#&gt; Time differences in secs\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n提供有用的默认值\n在生成factor时，提供默认的levels。\n\nfactor &lt;- function(x = character(), levels = unique(x)) {\n  ind &lt;- match(x, levels)\n  validate_factor(new_factor(ind, levels))\n}\n\nfactor(c(\"a\", \"a\", \"b\"))\n#&gt; [1] a a b\n#&gt; Levels: a b\n\n\n\n使用简单成分组成用户界面\n下面是一个构建POSIXct类的函数，函数的输入都是简单的整数。\n\nPOSIXct &lt;- function(year = integer(),\n                    month = integer(),\n                    day = integer(),\n                    hour = 0L,\n                    minute = 0L,\n                    sec = 0,\n                    tzone = \"\") {\n  ISOdatetime(year, month, day, hour, minute, sec, tz = tzone)\n}\n\nPOSIXct(2020, 1, 1, tzone = \"America/New_York\")\n#&gt; [1] \"2020-01-01 EST\"",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#generics-and-methods",
    "href": "Books/Advanced R(2e)/13 S3.html#generics-and-methods",
    "title": "13 S3",
    "section": "Generics and methods",
    "text": "Generics and methods\n如上所述，S3系统的泛型函数会执行方法分派——找到类能使用的方法，该过程由UseMethod()函数实现。UseMethod()函数有两个参数：\n\ngeneric：字符串类型的泛型函数名称。\nobject：要分派的对象。\n\n通常只需要第一个参数，例如mean():\n\nmean\n#&gt; function (x, ...) \n#&gt; UseMethod(\"mean\")\n#&gt; &lt;bytecode: 0x000001e1d1097580&gt;\n#&gt; &lt;environment: namespace:base&gt;\n\n创建自己的泛型函数，在创建时，泛型函数的参数不需要传递到UseMethod()中。\nmy_new_generic &lt;- function(x) {\n  UseMethod(\"my_new_generic\")\n}\n泛型函数内不要添加额外的处理逻辑，这会导致不可预的意外，例如下面的泛型函数g()，你无法修改变量y的值。\n\ng &lt;- function(x) {\n  x &lt;- 10\n  y &lt;- 10\n  UseMethod(\"g\")\n}\ng.default &lt;- function(x) c(x = x, y = y)\n\nx &lt;- 1\ny &lt;- 1\ng(x)\n#&gt; x y \n#&gt; 1 1\n\n\nMethod dispath\nUseMethod()执行方法派发的原理：创建泛型函数拥有的方法向量（generic.class），寻找潜在可用的方法。使用sloop::s3_dispath()可以查看这一过程。\n\nx &lt;- Sys.Date()\ns3_dispatch(print(x))\n#&gt; =&gt; print.Date\n#&gt;  * print.default\n\n\n=&gt;表示该方法被调用\n*表示该方法被定义但未被调用\n\n“default” class是特殊的伪类，它不是真实存在的类；包含它是为了使定义一个标准回退成为可能，这个回退在特定类的方法不可用时可以找到。例如下面的test类，在执行t()时，会自动为其分配t.default()方法，而不是R 4.0.0版本前的将t.test()当作方法进行派发。\n\nx &lt;- structure(1:10, class = \"test\")\ns3_dispatch(t(x))\n#&gt;    t.test\n#&gt; =&gt; t.default\n\n# Output in R version 3.6.2\nx &lt;- structure(1:10, class = \"test\")\nt(x)\n#&gt;\n#&gt;  One Sample t-test\n#&gt;\n#&gt; data:  x\n#&gt; t = 5.7446, df = 9, p-value = 0.0002782\n#&gt; alternative hypothesis: true mean is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  3.334149 7.665851\n#&gt; sample estimates:\n#&gt; mean of x\n#&gt;       5.5\n方法派发的本质相当简单，但随着本章的深入，你会发现它在包含继承、基类型、内部泛型和组泛型方面变得越来越复杂。下面的代码展示了几个更复杂的情况，我们将在后续13.7节和14章中详细介绍。\n\nx &lt;- matrix(1:10, nrow = 2)\ns3_dispatch(mean(x))\n#&gt;    mean.matrix\n#&gt;    mean.integer\n#&gt;    mean.numeric\n#&gt; =&gt; mean.default\n\ns3_dispatch(sum(Sys.time()))\n#&gt;    sum.POSIXct\n#&gt;    sum.POSIXt\n#&gt;    sum.default\n#&gt; =&gt; Summary.POSIXct\n#&gt;    Summary.POSIXt\n#&gt;    Summary.default\n#&gt; -&gt; sum (internal)\n\n\n\nFinding methods\nsloop::s3_methods_generic()函数可以查看一个generic函数的所有方法。sloop::s3_methods_class()函数可以查看一个class的所有方法。\n\ns3_methods_generic(\"mean\")\n#&gt; # A tibble: 7 × 4\n#&gt;   generic class    visible source             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;lgl&gt;   &lt;chr&gt;              \n#&gt; 1 mean    Date     TRUE    base               \n#&gt; 2 mean    default  TRUE    base               \n#&gt; 3 mean    difftime TRUE    base               \n#&gt; 4 mean    POSIXct  TRUE    base               \n#&gt; 5 mean    POSIXlt  TRUE    base               \n#&gt; 6 mean    quosure  FALSE   registered S3method\n#&gt; # ℹ 1 more row\n\ns3_methods_class(\"ordered\")\n#&gt; # A tibble: 4 × 4\n#&gt;   generic       class   visible source             \n#&gt;   &lt;chr&gt;         &lt;chr&gt;   &lt;lgl&gt;   &lt;chr&gt;              \n#&gt; 1 as.data.frame ordered TRUE    base               \n#&gt; 2 Ops           ordered TRUE    base               \n#&gt; 3 relevel       ordered FALSE   registered S3method\n#&gt; 4 Summary       ordered TRUE    base\n\n\n\nCreating methods\n创建自己类的方法格式为：\ngeneric_name.class_name &lt;- function(generic_params) {\n  # do something\n}\n在创建新方法时要注意：\n\n首先，只有当你拥有泛型函数或类时，才应该编写方法。虽然即使你不拥有某个类或泛型函数,R 也会允许你定义方法，但这是极其不礼貌的行为。相反，请与泛型函数或类的作者合作，在他们的代码中添加方法。\n一个方法必须具有与其泛型相同的参数。这是通过 R CMD 检查在包中强制执行的，但即使不创建包，这也是一个很好的实践。",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#object-styles",
    "href": "Books/Advanced R(2e)/13 S3.html#object-styles",
    "title": "13 S3",
    "section": "Object styles",
    "text": "Object styles\n这里用length()函数介绍一下不同类的方法风格。当length()函数作用于向量时，返回向量的长度。当作用在其他如dataframe类时，返回的是list的长度。\n\nRecord style object：list中的元素等长。例如POSIXlt类：\n\n\nx &lt;- as.POSIXlt(ISOdatetime(2020, 1, 1, 0, 0, 1:3))\nx\n#&gt; [1] \"2020-01-01 00:00:01 CST\" \"2020-01-01 00:00:02 CST\"\n#&gt; [3] \"2020-01-01 00:00:03 CST\"\n\nlength(x)\n#&gt; [1] 3\nlength(unclass(x))\n#&gt; [1] 11\n\nx[[1]] # the first date time\n#&gt; [1] \"2020-01-01 00:00:01 CST\"\nunclass(x)[[1]] # the first component, the number of seconds\n#&gt; [1] 1 2 3\n\n\nData frame\n\n\nx &lt;- data.frame(x = 1:100, y = 1:100)\nlength(x)\n#&gt; [1] 2\nnrow(x)\n#&gt; [1] 100\n\n\nScalar object: 使用一个list表示一个单一的对象。例如：lm对象\n\n\nmod &lt;- lm(mpg ~ wt, data = mtcars)\nlength(mod)\n#&gt; [1] 12\n\n更多有关Object style2的内容可参考vctrs包。",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#inheritance",
    "href": "Books/Advanced R(2e)/13 S3.html#inheritance",
    "title": "13 S3",
    "section": "Inheritance",
    "text": "Inheritance\nS3类的方法“继承”有由下面三种方式实现：\n\nclass属性可以是一个向量。\n\n\nclass(ordered(\"x\"))\n#&gt; [1] \"ordered\" \"factor\"\nclass(Sys.time())\n#&gt; [1] \"POSIXct\" \"POSIXt\"\n\n\n如果第一个类（class向量）没有方法定义，R 会自动向下寻找。\n\n\ns3_dispatch(print(ordered(\"x\")))\n#&gt;    print.ordered\n#&gt; =&gt; print.factor\n#&gt;  * print.default\ns3_dispatch(print(Sys.time()))\n#&gt; =&gt; print.POSIXct\n#&gt;    print.POSIXt\n#&gt;  * print.default\n\n\n使用NextMethod()可以访委托方法给已存在的方法。\n\n\n# 注意`-&gt;`\ns3_dispatch(ordered(\"x\")[1])\n#&gt;    [.ordered\n#&gt; =&gt; [.factor\n#&gt;    [.default\n#&gt; -&gt; [ (internal)\ns3_dispatch(Sys.time()[1])\n#&gt; =&gt; [.POSIXct\n#&gt;    [.POSIXt\n#&gt;    [.default\n#&gt; -&gt; [ (internal)\n\nS3面向对象没有对子类和父类之间的关系施加限制，在创建一个子类时，推荐：\n\n子类的base type与父类保持一致\n子类的属性包含（继承）父类的属性\n\n\nNextMethod()\nNextMethod ()是继承中最难理解的部分，所以我们从一个最常用的[的具体示例开始。我们首先创建一个简单的类：一个在打印时隐藏其输出的秘密类：\n\nnew_secret &lt;- function(x = double()) {\n  stopifnot(is.double(x))\n  structure(x, class = \"secret\")\n}\n\nprint.secret &lt;- function(x, ...) {\n  print(strrep(\"x\", nchar(x)))\n  invisible(x)\n}\n\nx &lt;- new_secret(c(15, 1, 456))\nx\n#&gt; [1] \"xx\"  \"x\"   \"xxx\"\n\n上面代码运行成功，但secret类没有定义[方法。\n\ns3_dispatch(x[1])\n#&gt;    [.secret\n#&gt;    [.default\n#&gt; =&gt; [ (internal)\nx[1]\n#&gt; [1] 15\n\n下面直接定义的[.secret方法会陷入无限循环：\n`[.secret` &lt;- function(x, i) {\n  new_secret(x[i])\n}\n我们需要额外的步骤，先去掉x的类属性，取完值后再重新定义类：\n\n`[.secret` &lt;- function(x, i) {\n  x &lt;- unclass(x)\n  new_secret(x[i])\n}\nx[1]\n#&gt; [1] \"xx\"\n\n上面的方法虽然有效，但会额外的创建x对象，造成资源浪费。更好的方法是使用NextMethod()方法。\n\n`[.secret` &lt;- function(x, i) {\n  new_secret(NextMethod())\n}\nx[1]\n#&gt; [1] \"xx\"\n\n\ns3_dispatch(x[1])\n#&gt; =&gt; [.secret\n#&gt;    [.default\n#&gt; -&gt; [ (internal)\n\n=&gt;表示调用了[.secret, 但NextMethod()将方法委托于底层的内部[方法，如-&gt;所示。\nNextMethod()在寻找委托方法的类时，会根据全局变量.Class来判断，内部的类型改变无效，例如：下面的结果返回的是a2。\n\ngeneric2 &lt;- function(x) UseMethod(\"generic2\")\ngeneric2.a1 &lt;- function(x) \"a1\"\ngeneric2.a2 &lt;- function(x) \"a2\"\ngeneric2.b &lt;- function(x) {\n  class(x) &lt;- \"a1\"\n  NextMethod()\n}\n\ngeneric2(structure(list(), class = c(\"b\", \"a2\")))\n#&gt; [1] \"a2\"\n\n\n\nAllowing subclassing\n当你创建一个类时，你需要决定是否允许可以存在一个子类，因为允许子类会要求你再创建构造函数和方法时作出一些额外的工作。\n例如，允许子类后，父类的构造函数要添加额外的参数——...,class。\n\nnew_secret &lt;- function(x, ..., class = character()) {\n  stopifnot(is.double(x))\n\n  structure(\n    x,\n    ...,\n    class = c(class, \"secret\")\n  )\n}\n\n然后，子类的构造函数可以直接调用父类的构造函数，并添加额外的参数。\n\nnew_supersecret &lt;- function(x) {\n  new_secret(x, class = \"supersecret\")\n}\n\nprint.supersecret &lt;- function(x, ...) {\n  print(rep(\"xxxxx\", length(x)))\n  invisible(x)\n}\n\nx2 &lt;- new_supersecret(c(15, 1, 456))\nx2\n#&gt; [1] \"xxxxx\" \"xxxxx\" \"xxxxx\"\n\n如果允许添加子类，方法同样需要额外的工作，否则方法始终返回父类。\n\n`[.secret` &lt;- function(x, ...) {\n  new_secret(NextMethod())\n}\nx2[1:3]\n#&gt; [1] \"xx\"  \"x\"   \"xxx\"\n\n\n`[.secret` &lt;- function(x, ...) {\n  new_secret(NextMethod(), class = class(x))\n}\nx2[1:3]\n#&gt; [1] \"xxxxx\" \"xxxxx\" \"xxxxx\"\n\n我们也可以使用vctrs::vec_restore()泛型函数，为secret类添加不同类时的返回结果。\n\n# `to` 参数用来传递类的其他属性。\nvec_restore.secret &lt;- function(x, to, ...) new_secret(x)\nvec_restore.supersecret &lt;- function(x, to, ...) new_supersecret(x)\n\n`[.secret` &lt;- function(x, ...) {\n  vctrs::vec_restore(NextMethod(), x)\n}\nx2[1:3]\n#&gt; [1] \"xxxxx\" \"xxxxx\" \"xxxxx\"",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/13 S3.html#dispatch-details",
    "href": "Books/Advanced R(2e)/13 S3.html#dispatch-details",
    "title": "13 S3",
    "section": "Dispatch details",
    "text": "Dispatch details\n\nS3 and base types\nS3面向对象系统中的泛型函数在作用于 base type 时，不会根据calss()返回的类进行方法派发。\n\nclass(matrix(1:5))\n#&gt; [1] \"matrix\" \"array\"\ns3_dispatch(print(matrix(1:5)))\n#&gt;    print.matrix\n#&gt;    print.integer\n#&gt;    print.numeric\n#&gt; =&gt; print.default\n\nx1 &lt;- 1:5\nclass(x1)\n#&gt; [1] \"integer\"\ns3_dispatch(mean(x1))\n#&gt;    mean.integer\n#&gt;    mean.numeric\n#&gt; =&gt; mean.default\n\nx2 &lt;- structure(x1, class = \"integer\")\nclass(x2)\n#&gt; [1] \"integer\"\ns3_dispatch(mean(x2))\n#&gt;    mean.integer\n#&gt; =&gt; mean.default\n\nbase type 的方法派发根据是其implicit class。implicit class 有三种组成：\n\n当对象有维度时，“array”, “matrix”。\ntypeof()的结果。\n当对象是”integer”或”double”时，“numeric”。\n\nimplicit class 只能由sloop::s3_class()获取。\n\ns3_class(matrix(1:5))\n#&gt; [1] \"matrix\"  \"integer\" \"numeric\"\n\n\n\nInternal generics\nbase R 中的一些函数，如[,sum(),cbind()等被称为internal generics。因为它们不使用UseMethod()，而是C语言中的DispatchGroup()或DispatchOrEval()。使用s3_dispatch()时，会显示为(internal)。\n\ns3_dispatch(Sys.time()[1])\n#&gt; =&gt; [.POSIXct\n#&gt;    [.POSIXt\n#&gt;    [.default\n#&gt; -&gt; [ (internal)\n\n\n\nGroup generics\nGroup generics 与 internal generics 类似，只存在于 base R 中，你自己无法构建。\nbase R 中存在4大类 Group generics：\n\nMath：abs(), sign(), sqrt(), floor(), cos(), sin(), log(), and more (see ?Math for the complete list).\nOps：+, -, *, /, ^, %%, %/%, &, |, !, ==, !=, &lt;, &lt;=, &gt;=, and &gt;.\nSummary：all(), any(), sum(), prod(), min(), max(), and range().\nComplex：Arg(), Conj(), Im(), Mod(), Re().\n\n当你为你的类定义了某个Group generic，这个Group generic内的所有方法都会被覆盖。当某个方法不存在时，会从组内寻找。\n\ns3_dispatch(sum(Sys.time()))\n#&gt;    sum.POSIXct\n#&gt;    sum.POSIXt\n#&gt;    sum.default\n#&gt; =&gt; Summary.POSIXct\n#&gt;    Summary.POSIXt\n#&gt;    Summary.default\n#&gt; -&gt; sum (internal)\n\n很多 Group generics 中都使用了NextMethod()。例如期，我们用abs()来计算一个difftime对象。\n\ny &lt;- as.difftime(10, units = \"mins\")\ns3_dispatch(abs(y))\n#&gt;    abs.difftime\n#&gt;    abs.default\n#&gt; =&gt; Math.difftime\n#&gt;    Math.default\n#&gt; -&gt; abs (internal)\n\nMath.difftime()大致如下：\nMath.difftime &lt;- function(x, ...) {\n  new_difftime(NextMethod(), units = attr(x, \"units\"))\n}\n\n\nDouble dispatch\nOps Group generics 使用了 double dispatch。这保证了a + b和b + a的计算结果一致。例如：\n\ndate &lt;- as.Date(\"2017-01-01\")\ninteger &lt;- 1L\n\ndate + integer\n#&gt; [1] \"2017-01-02\"\ninteger + date\n#&gt; [1] \"2017-01-02\"\n\n因为要同时为两种类进行方法派发，所以就会出现下面三种情况：\n\n如果方法相同，无所谓使用哪个方法。\n如果方法不同，R 最终回归到内部方法，并附带一个警告。\n如果有一个方法是“internal”，则R使用另外一种方法。",
    "crumbs": [
      "13 S3"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/11 Function operators.html",
    "href": "Books/Advanced R(2e)/11 Function operators.html",
    "title": "11 Function operators",
    "section": "",
    "text": "function operators 本质也是一个function factories，只是规定输入是一个函数。下面的简单示例——chatty()函数，接受一个函数f，返回一个能打印f的输入的函数。\n\nchatty &lt;- function(f) {\n  force(f)\n\n  function(x, ...) {\n    message(\"Processing \", x)\n    f(x, ...)\n  }\n}\nf &lt;- function(x) x^2\ns &lt;- c(3, 2, 1)\n\npurrr::map_dbl(s, chatty(f))\n#&gt; Processing 3\n#&gt; Processing 2\n#&gt; Processing 1\n#&gt; [1] 9 4 1\n\nfunction operators 与 python 中的装饰器相同，遵循开放封闭原则，即对扩展开放，对修改封闭。它允许我们在不修改原有函数代码的情况下增加额外的功能，例如：为函数添加日志、权限检查、参数检查等多种功能，这使得代码更加模块化，易于维护和扩展。\n\n\n\n11.2节介绍一些极其有用的 function operators 函数。\n11.2节介绍如何根据实际问题，创建自己的 function operators 函数。\n\n\n\n\nfunction operators 本质是function factories，请先了解 function factory 函数。\n本章会用到purrr包中的泛函和其提供的function operators函数，及 memoise 包中的memoise()函数。\n\nlibrary(purrr)\nlibrary(memoise)",
    "crumbs": [
      "11 Function operators"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/11 Function operators.html#introduction",
    "href": "Books/Advanced R(2e)/11 Function operators.html#introduction",
    "title": "11 Function operators",
    "section": "",
    "text": "function operators 本质也是一个function factories，只是规定输入是一个函数。下面的简单示例——chatty()函数，接受一个函数f，返回一个能打印f的输入的函数。\n\nchatty &lt;- function(f) {\n  force(f)\n\n  function(x, ...) {\n    message(\"Processing \", x)\n    f(x, ...)\n  }\n}\nf &lt;- function(x) x^2\ns &lt;- c(3, 2, 1)\n\npurrr::map_dbl(s, chatty(f))\n#&gt; Processing 3\n#&gt; Processing 2\n#&gt; Processing 1\n#&gt; [1] 9 4 1\n\nfunction operators 与 python 中的装饰器相同，遵循开放封闭原则，即对扩展开放，对修改封闭。它允许我们在不修改原有函数代码的情况下增加额外的功能，例如：为函数添加日志、权限检查、参数检查等多种功能，这使得代码更加模块化，易于维护和扩展。\n\n\n\n11.2节介绍一些极其有用的 function operators 函数。\n11.2节介绍如何根据实际问题，创建自己的 function operators 函数。\n\n\n\n\nfunction operators 本质是function factories，请先了解 function factory 函数。\n本章会用到purrr包中的泛函和其提供的function operators函数，及 memoise 包中的memoise()函数。\n\nlibrary(purrr)\nlibrary(memoise)",
    "crumbs": [
      "11 Function operators"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/11 Function operators.html#existing-function-operators",
    "href": "Books/Advanced R(2e)/11 Function operators.html#existing-function-operators",
    "title": "11 Function operators",
    "section": "Existing function operators",
    "text": "Existing function operators\n\nCapturing errors with purrr::safely()\n在使用map()等函数替代for-loop时，我们通常会困扰于：如果函数执行过程中发生错误，那么map()函数会直接停止，不会返回已运行成功的部分结果，而for-loop则会保留部分结果。\n\nx &lt;- list(\n  c(0.512, 0.165, 0.717),\n  c(0.064, 0.781, 0.427),\n  \"oops\",\n  c(0.890, 0.785, 0.495)\n)\n\nout &lt;- rep(NA_real_, length(x))\nfor (i in seq_along(x)) {\n  out[[i]] &lt;- sum(x[[i]])\n}\n#&gt; Error in sum(x[[i]]): invalid 'type' (character) of argument\n\nout\n#&gt; [1] 1.394 1.272    NA    NA\n\nmap_dbl(x, sum)\n#&gt; Error in `map_dbl()`:\n#&gt; ℹ In index: 3.\n#&gt; Caused by error:\n#&gt; ! invalid 'type' (character) of argument\n\n上面的例子中，虽然最后会失败，但out会保留前面成功的结果，但map_dbl()则不会。如果我们使用safely()修改sum()，就会始终返回一个同时包含正确结果和错误信息的list。仔细观察结果，会进一步发现：for-loop在第三个循环失败后不再允许，map则会继续执行，它返回了第四个结果。\n\nout &lt;- map(x, safely(sum))\nstr(out)\n#&gt; List of 4\n#&gt;  $ :List of 2\n#&gt;   ..$ result: num 1.39\n#&gt;   ..$ error : NULL\n#&gt;  $ :List of 2\n#&gt;   ..$ result: num 1.27\n#&gt;   ..$ error : NULL\n#&gt;  $ :List of 2\n#&gt;   ..$ result: NULL\n#&gt;   ..$ error :List of 2\n#&gt;   .. ..$ message: chr \"invalid 'type' (character) of argument\"\n#&gt;   .. ..$ call   : language .Primitive(\"sum\")(..., na.rm = na.rm)\n#&gt;   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n#&gt;  $ :List of 2\n#&gt;   ..$ result: num 2.17\n#&gt;   ..$ error : NULL\n\n那么，safely()函数做了什么？打印safe_sum()，我们会发现它调用了capture_error()函数，捕获错误信息并返回。\n\nsafe_sum &lt;- safely(sum)\nsafe_sum\n#&gt; function (...) \n#&gt; capture_error(.f(...), otherwise, quiet)\n#&gt; &lt;bytecode: 0x000002c8bb963f60&gt;\n#&gt; &lt;environment: 0x000002c8bc128710&gt;\n\nstr(safe_sum(x[[1]]))\n#&gt; List of 2\n#&gt;  $ result: num 1.39\n#&gt;  $ error : NULL\nstr(safe_sum(x[[3]]))\n#&gt; List of 2\n#&gt;  $ result: NULL\n#&gt;  $ error :List of 2\n#&gt;   ..$ message: chr \"invalid 'type' (character) of argument\"\n#&gt;   ..$ call   : language .Primitive(\"sum\")(..., na.rm = na.rm)\n#&gt;   ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\n由于safely()后的函数始终返回一个list——包含两个元素：result，error，我们可以使用purrr::transpose()函数，将map()的输出结果转置，得到一个包含两个元素的list，第一个元素是正常结果，第二个元素是错误信息。\n\nout &lt;- transpose(map(x, safely(sum)))\nstr(out)\n#&gt; List of 2\n#&gt;  $ result:List of 4\n#&gt;   ..$ : num 1.39\n#&gt;   ..$ : num 1.27\n#&gt;   ..$ : NULL\n#&gt;   ..$ : num 2.17\n#&gt;  $ error :List of 4\n#&gt;   ..$ : NULL\n#&gt;   ..$ : NULL\n#&gt;   ..$ :List of 2\n#&gt;   .. ..$ message: chr \"invalid 'type' (character) of argument\"\n#&gt;   .. ..$ call   : language .Primitive(\"sum\")(..., na.rm = na.rm)\n#&gt;   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n#&gt;   ..$ : NULL\n\n现在我们可以轻易地找到结果和错误原因。\n\nok &lt;- map_lgl(out$error, is.null)\nok\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n\nx[!ok]\n#&gt; [[1]]\n#&gt; [1] \"oops\"\n\nout$result[ok]\n#&gt; [[1]]\n#&gt; [1] 1.394\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1.272\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 2.17\n\nsafely()函数的使用场景有很多，我们可以总结出下面的使用规律：\nf &lt;- fcuntion (x, ...) {\n  # do something\n}\n\nout &lt;- transpose(map(x, safely(f)))\nok &lt;- map_lgl(out$error, is.null)\n\n# which data failed to converge?\nx[!ok]\n\n# which models were successful?\nout$result[ok]\n\n\nOther function operators in purrr\n\npossibly()：当函数报错时，返回默认值，无法判断是否发生了错误。\nquietly()：返回函数中除报错的其他信息。\n\n\nf &lt;- function() {\n  print(\"Hi!\")\n  message(\"Hello\")\n  warning(\"How are ya?\")\n  \"Gidday\"\n}\nf()\n#&gt; [1] \"Hi!\"\n#&gt; Hello\n#&gt; Warning in f(): How are ya?\n#&gt; [1] \"Gidday\"\n\nf_quiet &lt;- quietly(f)\nstr(f_quiet())\n#&gt; List of 4\n#&gt;  $ result  : chr \"Gidday\"\n#&gt;  $ output  : chr \"[1] \\\"Hi!\\\"\"\n#&gt;  $ warnings: chr \"How are ya?\"\n#&gt;  $ messages: chr \"Hello\\n\"\n\n\nas_browse()：当函数报错时，进入断点调试模式。\n\n\n\nCaching computations with memoise::memoise()\nmemoises 使函数可以缓存之前的输入和输出。这种缓存能力势必会增加内存的消耗，但却会提高计算的速度。\n\nslow_function &lt;- function(x) {\n  Sys.sleep(1)\n  x * 10 * runif(1)\n}\nsystem.time(print(slow_function(1)))\n#&gt; [1] 4.170535\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    1.02\n\nsystem.time(print(slow_function(1)))\n#&gt; [1] 5.910717\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    1.01\n\n上面的例子中，每次运行结果都会不同，但是当被memoises后，第一次的结果就会被缓存，当输入相同时，就会直接返回缓存的结果。\n\nfast_function &lt;- memoise::memoise(slow_function)\nsystem.time(print(fast_function(1)))\n#&gt; [1] 7.387216\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    1.09\n\nsystem.time(print(fast_function(1)))\n#&gt; [1] 7.387216\n#&gt;    user  system elapsed \n#&gt;    0.02    0.00    0.01\n\n另外一个例子是计算斐波那契数列（f(0) = 0, f(1) = 1, f(n) = f(n-1) + f(n-2)）。\n\nfib &lt;- function(n) {\n  if (n &lt; 2) {\n    return(n)\n  }\n  fib(n - 2) + fib(n - 1)\n}\nsystem.time(fib(23))\n#&gt;    user  system elapsed \n#&gt;    0.03    0.00    0.03\nsystem.time(fib(24))\n#&gt;    user  system elapsed \n#&gt;    0.04    0.00    0.05\n\n将fib()memoises化后， 当计算完fib2(23)后，fib2(24)的计算速度会非常快。\n\nfib2 &lt;- memoise::memoise(function(n) {\n  if (n &lt; 2) {\n    return(n)\n  }\n  fib2(n - 2) + fib2(n - 1)\n})\nsystem.time(fib2(23))\n#&gt;    user  system elapsed \n#&gt;       0       0       0\nsystem.time(fib2(24))\n#&gt;    user  system elapsed \n#&gt;       0       0       0\n\n在动态规划中（dynamic programming），memoises更加常见。\n但在memoises化函数之前，要检查函数是否是pure的。",
    "crumbs": [
      "11 Function operators"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/11 Function operators.html#case-study-creating-your-own-function-operators",
    "href": "Books/Advanced R(2e)/11 Function operators.html#case-study-creating-your-own-function-operators",
    "title": "11 Function operators",
    "section": "Case study: Creating your own function operators",
    "text": "Case study: Creating your own function operators\n下面我们以一个下载数据的例子，介绍如何编写自己的function operator。\n假设你有很多书籍的网址，你想要下载这些书籍。使用前面章节中的walk2()和file.download()，可以简单地写为：\nurls &lt;- c(\n  \"adv-r\" = \"https://adv-r.hadley.nz\",\n  \"r4ds\" = \"http://r4ds.had.co.nz/\"\n  # and many many more\n)\npath &lt;- paste0(tempdir(), names(urls), \".html\")\n\nwalk2(urls, path, download.file, quiet = TRUE)\n上面的方法在urls不是很长时，确实足够。但当urls变得很长时，你就需要考虑：\n\n每本书下载后要添加一个延时，避免阻塞服务器。\n显示下载的进度。\n\n使用for-loop可以轻松解决上面两点，但for-loop将“下载”、“延时”，“显示进度”三个不同目的的东西都放在了一起，会让代码难于阅读。\nfor (i in seq_along(urls)) {\n  Sys.sleep(0.1)\n  if (i %% 10 == 0) cat(\".\")\n  download.file(urls[[i]], path[[i]], quiet = TRUE)\n}\n我们使用function operators来将这三个目的分开。首先创建“延时”函数delay_by()：接受两个参数——函数，延时时长\n\ndelay_by &lt;- function(f, amount) {\n  force(f)\n  force(amount)\n\n  function(...) {\n    Sys.sleep(amount)\n    f(...)\n  }\n}\nsystem.time(runif(100))\n#&gt;    user  system elapsed \n#&gt;       0       0       0\nsystem.time(delay_by(runif, 0.1)(100))\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    0.11\n\n将delay_by()应用到download.file()中：\nwalk2(urls, path, delay_by(download.file, 0.1), quiet = TRUE)\n接下来创建“显示进度”函数dot_every()：接受两个参数——函数，显示点的间隔\n\ndot_every &lt;- function(f, n) {\n  force(f)\n  force(n)\n\n  i &lt;- 0\n  function(...) {\n    i &lt;&lt;- i + 1\n    if (i %% n == 0) cat(\".\")\n    f(...)\n  }\n}\nwalk(1:100, runif)\nwalk(1:100, dot_every(runif, 10))\n#&gt; ..........\n\n将dot_every()应用到download.file()中：\nwalk2(\n  urls, path,\n  dot_every(delay_by(download.file, 0.1), 10),\n  quiet = TRUE\n)\n我们也可以使用管道符%&gt;%将函数串起来写：\nwalk2(\n  urls, path,\n  download.file %&gt;% delay_by(0.1) %&gt;% dot_every(10),\n  quiet = TRUE\n)",
    "crumbs": [
      "11 Function operators"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html",
    "href": "Books/Advanced R(2e)/1 Introduction.html",
    "title": "1 Introduction",
    "section": "",
    "text": "一千个人有一千个哈姆雷特。对于我而言，我因为生物信息了解R，因为tidyverse的编程哲学喜欢上了R。但R不是一个很严谨的语言，需要不断学习尝试，遵循一定惯例。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#why-r",
    "href": "Books/Advanced R(2e)/1 Introduction.html#why-r",
    "title": "1 Introduction",
    "section": "",
    "text": "一千个人有一千个哈姆雷特。对于我而言，我因为生物信息了解R，因为tidyverse的编程哲学喜欢上了R。但R不是一个很严谨的语言，需要不断学习尝试，遵循一定惯例。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#who-should-read-this-book",
    "href": "Books/Advanced R(2e)/1 Introduction.html#who-should-read-this-book",
    "title": "1 Introduction",
    "section": "Who should read this book",
    "text": "Who should read this book\n本书适合两类人：\n\n中级 R 程序员，他们希望更深入地研究 R 语言，理解语言的工作原理，并学习解决各种问题的新策略。\n来自其他语言的程序员，他们正在学习 R，并且想要理解为什么 R 是这样工作的。\n\n为了阅读这本书之前，你需要用 R 或其他编程语言编写大量的代码，熟悉数据分析的基础知识 (即数据导入、操作和可视化) ，熟悉 CRAN 包的安装和使用。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#what-you-will-get-out-of-this-book",
    "href": "Books/Advanced R(2e)/1 Introduction.html#what-you-will-get-out-of-this-book",
    "title": "1 Introduction",
    "section": "What you will get out of this book",
    "text": "What you will get out of this book\n阅读本书，你可以：\n\n熟悉 R 的基础知识。你将了解复杂数据类型以及对它们执行操作的最佳方法，对函数如何工作有深入的了解，知道什么是环境，以及如何使用条件系统。\n理解函数式编程的含义，以及为什么它是数据科学的有用工具。你将能够快速学习如何使用现有的工具，并且具备在需要时创建自己的功能性工具的知识。\n了解 R 的各种面向对象系统。你将最熟悉 S3，并且知道 S4 和 R6，以及在需要时在哪里查找更多信息。\n欣赏元编程的双刃剑。你将能够创建使用整洁计算、保存类型和创建优雅代码来表示重要操作的函数，也会明白什么是危险，什么时候应该避免。\n有一个良好的直觉，在 R 中的操作是缓慢的或使用大量的内存。你将知道如何使用分析来查明性能瓶颈，并且知道足够多的 C++ 来将慢速 R 函数转换为快速 C++ 等价物。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#what-you-will-not-learn",
    "href": "Books/Advanced R(2e)/1 Introduction.html#what-you-will-not-learn",
    "title": "1 Introduction",
    "section": "What you will not learn",
    "text": "What you will not learn\n本书不涉及：\n\n如何进行数据分析，数据分析推荐R for Data Science。\n如何生成R包，构建R包推荐R packages。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#meta-techniques",
    "href": "Books/Advanced R(2e)/1 Introduction.html#meta-techniques",
    "title": "1 Introduction",
    "section": "Meta-techniques",
    "text": "Meta-techniques\n提升编程能力的两个途径：\n\n阅读源代码。\n采用科学思维",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#recommended-reading",
    "href": "Books/Advanced R(2e)/1 Introduction.html#recommended-reading",
    "title": "1 Introduction",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nThe Structure and Interpretation of Computer Programs：提升R面向对象的理解。\nConcepts, Techniques and Models of Computer Programming：理解R与其他语言的差异。\nThe Pragmatic Programmer",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#getting-help",
    "href": "Books/Advanced R(2e)/1 Introduction.html#getting-help",
    "title": "1 Introduction",
    "section": "Getting help",
    "text": "Getting help\n在遇到困难寻求帮助时，下面是一些问答网站：\n\nRstudio Community\nStackOverflow\nR-help mailing list\n\n在寻求帮助前，你要：\n\n确定你的R版本和R包版本，及环境。\n给出一个复现问题的最小示例（reprex）。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#acknowledgments",
    "href": "Books/Advanced R(2e)/1 Introduction.html#acknowledgments",
    "title": "1 Introduction",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n…",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#conventions",
    "href": "Books/Advanced R(2e)/1 Introduction.html#conventions",
    "title": "1 Introduction",
    "section": "Conventions",
    "text": "Conventions\n对本书中的一些符号进行说明：\n\nf() 表示函数\ng 表示变量和函数参数\nh/ 表示路径\n#&gt; 表示终端运行结果\nset.seed(1014) 每章固定随机数种子",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/1 Introduction.html#colophon",
    "href": "Books/Advanced R(2e)/1 Introduction.html#colophon",
    "title": "1 Introduction",
    "section": "Colophon",
    "text": "Colophon\n书中的R 和 R包版本会实时更新，请阅读原文了解详细信息。",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "Blog/相关性计算.html",
    "href": "Blog/相关性计算.html",
    "title": "相关性计算",
    "section": "",
    "text": "近日，就相关性及其显著性的计算发生了一些讨论，现记录总结如下。"
  },
  {
    "objectID": "Blog/相关性计算.html#计算相关性及显著性",
    "href": "Blog/相关性计算.html#计算相关性及显著性",
    "title": "相关性计算",
    "section": "计算相关性及显著性",
    "text": "计算相关性及显著性\n共分为四步：\n\nstats::cor()计算相关性r。\n计算自由度n。\n构建正态分布统计量t。\n计算相关性显著性p。\n\n# x is matrix\n# 代码源于 psych::corr.test\nr &lt;- cor(x, use = use, method = method)\nn &lt;- t(!is.na(x)) %*% (!is.na(x))\nt &lt;- (r * sqrt(n - 2)) / sqrt(1 - r^2)\np &lt;- -2 * expm1(pt(abs(t), (n - 2), log.p = TRUE))\nse &lt;- sqrt((1 - r * r) / (n - 2))"
  },
  {
    "objectID": "Blog/相关性计算.html#cor函数参数use",
    "href": "Blog/相关性计算.html#cor函数参数use",
    "title": "相关性计算",
    "section": "cor函数参数use",
    "text": "cor函数参数use\n在上面四个步骤中，use参数往往会被忽略，但是它及其重要。use参数用来处理输入数据中的NA值，是大型数据处理时的主要限速步骤。\nuse参数的取值有：\n\n\"everything\": 忽略NA值，数据中的NA值导致相关性为NA值。\n\"all.obs\": 数据中有NA值时报错，只计算有完整数据对之间的相关性。\n\"complete.obs\": 允许数据中有NA值，计算前会整体地删除有NA值的行，删除后无法计算相关性时报错。\n\"na.or.complete\": 允许数据中有NA值，计算前会整体地删除有NA值的行，删除后无法计算相关性时输出NA值。\n\"pairwise.complete.obs\": 允许数据中有NA值，计算前会按变量单独地删除有NA值的行，删除后无法计算相关性时输出NA值。\n\n\nx &lt;- c(1, 2, NA, NA, NA)\ny &lt;- c(NA, 2, 5, 6, 6)\nz &lt;- c(NA, NA, 4, 7, 8)\n\ndat &lt;- data.frame(x, y, z)\n\ncor(dat, use = \"everything\")\n#&gt;    x  y  z\n#&gt; x  1 NA NA\n#&gt; y NA  1 NA\n#&gt; z NA NA  1\ncor(dat, use = \"all.obs\")\n#&gt; Error in cor(dat, use = \"all.obs\"): missing observations in cov/cor\ncor(dat, use = \"complete.obs\")\n#&gt; Error in cor(dat, use = \"complete.obs\"): no complete element pairs\ncor(dat, use = \"na.or.complete\")\n#&gt;    x  y  z\n#&gt; x NA NA NA\n#&gt; y NA NA NA\n#&gt; z NA NA NA\ncor(dat, use = \"pairwise.complete.obs\")\n#&gt;    x         y         z\n#&gt; x  1        NA        NA\n#&gt; y NA 1.0000000 0.9707253\n#&gt; z NA 0.9707253 1.0000000"
  },
  {
    "objectID": "Blog/相关性计算.html#显著性计算",
    "href": "Blog/相关性计算.html#显著性计算",
    "title": "相关性计算",
    "section": "显著性计算",
    "text": "显著性计算\n显著性的计算基于pt函数，该函数依据数据构建的统计量和自由度，计算该数据点出现的概率。上面的代码使用了对数化操作，能够提高计算的精度。\n考虑到use = \"pairwise\"时，每对相关性数据的自由度会不同，所以上面的代码采取了矩阵的计算方法。生成每对相关性数据的自由度和统计量。\n虽然当use = \"everything\"时，每对数据点之间自由度相同，可以使用标量进行计算，但是实际上矩阵 * 标量 = 矩阵，矩阵 * 矩阵 = 矩阵，并不会造成计算浪费。"
  },
  {
    "objectID": "Blog/相关性计算.html#测试",
    "href": "Blog/相关性计算.html#测试",
    "title": "相关性计算",
    "section": "测试",
    "text": "测试\n\n# 生成一个100行，10000列的随机矩阵，用来测试相关性\nx &lt;- matrix(rnorm(100 * 10000), nrow = 100)\n\nsystem.time({\n  r &lt;- cor(x, use = \"pairwise\", method = \"pearson\")\n})\n#&gt;    user  system elapsed \n#&gt;   27.90    0.20   28.11\nsystem.time({\n  r &lt;- cor(x, use = \"everything\", method = \"pearson\")\n})\n#&gt;    user  system elapsed \n#&gt;    6.28    0.18    6.45\nsystem.time({\n  n &lt;- t(!is.na(x)) %*% (!is.na(x))\n})\n#&gt;    user  system elapsed \n#&gt;    5.73    0.28    6.02\nsystem.time({\n  t &lt;- (r * sqrt(n - 2)) / sqrt(1 - r^2)\n})\n#&gt;    user  system elapsed \n#&gt;    1.33    0.36    1.69\nsystem.time({\n  p &lt;- -2 * expm1(pt(abs(t), (n - 2), log.p = TRUE))\n})\n#&gt;    user  system elapsed \n#&gt;   40.84    1.95   42.80\nsystem.time({\n  p &lt;- -2 * expm1(pt(abs(t), (dim(x)[1] - 2), log.p = TRUE))\n})\n#&gt;    user  system elapsed \n#&gt;   39.53    0.36   39.91"
  },
  {
    "objectID": "Blog/双环网络图.html",
    "href": "Blog/双环网络图.html",
    "title": "双环网络图",
    "section": "",
    "text": "双环网络图的使用情景：图是二分图，即：A组与B组之间元素有关联，但组内元素没有关联；组间元素的关联呈现出聚集状。可以使用igraph::is_bipartite(g)进行二分图的判断。"
  },
  {
    "objectID": "Blog/双环网络图.html#生成示例数据",
    "href": "Blog/双环网络图.html#生成示例数据",
    "title": "双环网络图",
    "section": "生成示例数据",
    "text": "生成示例数据\n\nA &lt;- paste0(\"groupA\", 1:30)\nB &lt;- paste0(\"groupB\", 1:10)\n\nnode &lt;- tibble::tibble(\n  name = c(A, B),\n  type = c(rep(FALSE, length(A)), rep(TRUE, length(B))),\n  Class = c(rep(\"A\", length(A)), rep(\"B\", length(B)))\n)\n\nset.seed(123)\nconnnectNum &lt;- vapply(\n  seq_along(A),\n  function(x) {\n    sample(1:7, 1)\n  },\n  FUN.VALUE = 1\n)\n\ngetB &lt;- function(a, b, size) {\n  tibble::tibble(\n    from = a,\n    to = sample(b, min(size, length(B)), replace = TRUE),\n    cor = sample(60:100, min(size, length(B)), replace = TRUE) / 100\n  )\n}\n\nedge &lt;- purrr::imap(\n  A, function(x, i) {\n    if (i &lt; 15) {\n      getB(x, B[1:5], connnectNum[i])\n    } else if (i &lt; 25) {\n      getB(x, B[5:8], connnectNum[i])\n    } else {\n      getB(x, B[9:10], connnectNum[i])\n    }\n  }\n) |&gt;\n  purrr::list_rbind()\n\n在生成node时，添加了额外的列type，这是识别二分图的标志，我们可以手动添加，也可以使用igraph::V(g)$type &lt;- bipartite_mapping(g)$type来添加。"
  },
  {
    "objectID": "Blog/双环网络图.html#创建网络图对象并添加度信息",
    "href": "Blog/双环网络图.html#创建网络图对象并添加度信息",
    "title": "双环网络图",
    "section": "创建网络图对象，并添加度信息",
    "text": "创建网络图对象，并添加度信息\n\ng &lt;- igraph::graph_from_data_frame(edge, vertices = node, directed = FALSE) |&gt; # igraph 生成对象\n  igraph::simplify(remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = \"first\") |&gt; # 使用igraph函数去除重复边，去除自环，边属性保留第一个，因为都一样\n  tidygraph::as_tbl_graph() |&gt; # tidygraph 生成对象\n  tidygraph::activate(\"nodes\") |&gt;\n  dplyr::mutate(Degree = tidygraph::centrality_degree()) # 生成节点度信息"
  },
  {
    "objectID": "Blog/双环网络图.html#检查是否是二分图",
    "href": "Blog/双环网络图.html#检查是否是二分图",
    "title": "双环网络图",
    "section": "检查是否是二分图",
    "text": "检查是否是二分图\n\nigraph::is_bipartite(g)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "Blog/双环网络图.html#绘制二分图",
    "href": "Blog/双环网络图.html#绘制二分图",
    "title": "双环网络图",
    "section": "绘制二分图",
    "text": "绘制二分图\n注意：在设置type列时，值为TRUE的节点位于下方，值为FALSE的节点位于上方。这在转换极坐标系时用于区分内外圈，TRUE的节点位于内圈，值为FALSE的节点位于外圈。\n\np &lt;- ggraph::ggraph(g, layout = \"bipartite\") +\n  ggraph::geom_edge_link() +\n  ggraph::geom_node_point(\n    ggplot2::aes(color = Class, size = Degree),\n    shape = 19,\n    alpha = 1\n  ) +\n  ggplot2::theme_void()\np"
  },
  {
    "objectID": "Blog/双环网络图.html#转换成极坐标系",
    "href": "Blog/双环网络图.html#转换成极坐标系",
    "title": "双环网络图",
    "section": "转换成极坐标系",
    "text": "转换成极坐标系\n\ncoords &lt;- g |&gt;\n  ggraph::create_layout(layout = \"bipartite\") |&gt;\n  dplyr::select(x, y) |&gt;\n  dplyr::mutate(\n    y = ifelse(y == 0, 0.3, y),\n    theta = x / (max(x) + 1) * 2 * pi,\n    r = y + 1,\n    x = r * cos(theta),\n    y = r * sin(theta),\n  )"
  },
  {
    "objectID": "Blog/双环网络图.html#绘制双环网络图",
    "href": "Blog/双环网络图.html#绘制双环网络图",
    "title": "双环网络图",
    "section": "绘制双环网络图",
    "text": "绘制双环网络图\n\np &lt;- ggraph::ggraph(g, layout = \"manual\", x = coords$x, y = coords$y) +\n  ggraph::geom_edge_link() +\n  ggraph::geom_node_point(\n    ggplot2::aes(color = Class, size = Degree),\n    shape = 19,\n    alpha = 1\n  ) +\n  ggplot2::theme_void()\np\n\n\n\n\n\n\n\n稍稍美化一下图\n\npal &lt;- colorRampPalette(c(\"#3a9ab2\", \"#ddc744\", \"#f11b00\"))(100)\n\np &lt;- ggraph::ggraph(g, layout = \"manual\", x = coords$x, y = coords$y) +\n  ggraph::geom_edge_diagonal(\n    ggplot2::aes(color = cor, edge_width = cor)\n  ) +\n  ggraph::geom_node_point(\n    ggplot2::aes(color = Class, size = Degree),\n    shape = 19,\n    alpha = 1\n  ) +\n  ggraph::geom_node_text(\n    ggplot2::aes(\n      x = ifelse(Class == \"B\", x * 0.95, x * 1.03),\n      y = ifelse(Class == \"B\", y * 0.95, y * 1.03),\n      label = name,\n      hjust = ifelse(Class == \"B\", \"inward\", \"outward\"),\n      angle = -((-ggraph::node_angle(x, y) + 90) %% 180) + 90,\n      colour = Class\n    ),\n    repel = FALSE,\n    size = 8 / ggplot2::.pt\n  ) +\n  ggplot2::scale_color_manual(values = c(\"A\" = \"#f8766d\", \"B\" = \"#00bfc4\")) +\n  ggplot2::scale_size_continuous(range = c(1, 5)) +\n  ggraph::scale_edge_color_gradientn(colours = pal) +\n  ggraph::scale_edge_width_continuous(range = c(0.1, 1)) +\n  ggplot2::scale_x_continuous(expand = ggplot2::expansion(mult = .1)) +\n  ggplot2::scale_y_continuous(expand = ggplot2::expansion(mult = .1)) +\n  ggplot2::coord_fixed() +\n  ggplot2::guides(\n    edge_color = ggraph::guide_edge_colorbar(order = 1),\n    edge_width = ggplot2::guide_legend(order = 2, override.aes = list(color = \"black\")),\n    color = ggplot2::guide_legend(order = 3, override.aes = list(size = 5)),\n    size = ggplot2::guide_legend(order = 4)\n  ) +\n  ggplot2::theme_void() +\n  ggplot2::theme(\n    plot.margin = ggplot2::margin(1, 1, 1, 1, \"cm\"),\n    legend.text = ggplot2::element_text(size = 8),\n    legend.title = ggplot2::element_text(size = 10)\n    # legend.position = c(0.5, 0.5),\n    # legend.box = \"horizontal\"\n  )\np"
  },
  {
    "objectID": "Blog/Object in R1.html#成为面对对象的五个条件",
    "href": "Blog/Object in R1.html#成为面对对象的五个条件",
    "title": "R 中的面向对象(1)",
    "section": "成为面对对象的五个条件",
    "text": "成为面对对象的五个条件\n\n类\n对象\n继承\n封装\n多态\n\n定义一个类，这个类有一些属性和方法，方法被封装为接口和实现；类可以实例化一个对象；类之间具有父子关系，子类可以继承父类的属性和方法；不同类的方法名是重复的，可以根据类选择对应的实现，这称之为多态。"
  },
  {
    "objectID": "Blog/Object in R1.html#为什么使用面向对象",
    "href": "Blog/Object in R1.html#为什么使用面向对象",
    "title": "R 中的面向对象(1)",
    "section": "为什么使用面向对象：",
    "text": "为什么使用面向对象：\n\n封装提供了标准的使用接口，能够降低系统的耦合度。\n继承允许子类继承父类的特性，提高了代码的重用性。\n多态允许相同函数对于不同的类有不同的行为，提高了代码的灵活性。\n\n想象一下，当你想更新一个函数的使用方法，但是又得保持和以前的兼容性，你只需要再创建一个子类，然后重新实现父类的方法，就可以了。"
  },
  {
    "objectID": "Blog/Object in R1.html#r-中的面向对象",
    "href": "Blog/Object in R1.html#r-中的面向对象",
    "title": "R 中的面向对象(1)",
    "section": "R 中的面向对象",
    "text": "R 中的面向对象\nR 语言中存在多种面向对象的编程，包括封装类的如RC，R6；泛函类的如S3，S4等。\n\n基于S3的面向对象编程基于泛型函数（generic function），不基于类层级结构,没有类的继承，没有严格的层级结构式继承。\n基于S4的面向对象编程，相对S3健全，实现了类的继承，具有严格的层级结构式类继承，但是是在函数封装过程中同S3一样，基于泛型函数。\n基于RC（也称S5）的面向对象编程则更加完善，符合上面的要求。\n基于R6包的面向对象编程，同样符合上面的要求。\n基于proto包的面向对象编程在ggplot2包中使用。\n基于R.oo包的面向对象编程类似S3。"
  },
  {
    "objectID": "Blog/Object in R1.html#sloop-包",
    "href": "Blog/Object in R1.html#sloop-包",
    "title": "R 中的面向对象(1)",
    "section": "sloop 包",
    "text": "sloop 包\n包中的函数sloop::otype()可以用来检查对象的面向对象编程类型。\n\nlibrary(sloop)\n\notype(1:10)\n#&gt; [1] \"base\"\n\notype(mtcars)\n#&gt; [1] \"S3\"\n\nmle_obj &lt;- stats4::mle(function(x = 1) (x - 2)^2)\notype(mle_obj)\n#&gt; [1] \"S4\""
  },
  {
    "objectID": "Blog/Data Masking.html",
    "href": "Blog/Data Masking.html",
    "title": "Data Masking(1)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rlang)"
  },
  {
    "objectID": "Blog/Data Masking.html#什么是data-masking",
    "href": "Blog/Data Masking.html#什么是data-masking",
    "title": "Data Masking(1)",
    "section": "什么是data-masking",
    "text": "什么是data-masking\nData-masking 是一种允许直接调用数据框中的列名作为一个正常环境变量的技术。例如下例，使用with函数实现该目的：\n\n# Unmasked programming\nmean(mtcars$cyl + mtcars$am)\n#&gt; [1] 6.59375\n\n# Referring to columns is an error - Where is the data?\nmean(cyl + am)\n#&gt; Error: object 'cyl' not found\n\n# Data-masking\nwith(mtcars, mean(cyl + am))\n#&gt; [1] 6.59375"
  },
  {
    "objectID": "Blog/Data Masking.html#data-masking-带来的问题",
    "href": "Blog/Data Masking.html#data-masking-带来的问题",
    "title": "Data Masking(1)",
    "section": "data-masking 带来的问题",
    "text": "data-masking 带来的问题\n虽然 data-masking 技术使得操作数据框十分方便，但会增加创造函数的困难。例如下面例子中的var,var2在函数bodys中并不表示参数，而是被 data-masking 解释为数据data中的列。\n\nmy_mean &lt;- function(data, var1, var2) {\n  dplyr::summarise(data, mean(var1 + var2))\n}\n\nmy_mean(mtcars, cyl, am)\n#&gt; Error in `dplyr::summarise()`:\n#&gt; ℹ In argument: `mean(var1 + var2)`.\n#&gt; Caused by error:\n#&gt; ! object 'cyl' not found\n\n使用{{可以避免 data-masking 带来的问题，因为它会把var1和var2解释为参数而不是数据data中的列。\n\nmy_mean &lt;- function(data, var1, var2) {\n  dplyr::summarise(data, mean({{ var1 }} + {{ var2 }}))\n}\n\nmy_mean(mtcars, cyl, am)\n#&gt;   mean(cyl + am)\n#&gt; 1        6.59375"
  },
  {
    "objectID": "Blog/Data Masking.html#masking-具体是什么意思",
    "href": "Blog/Data Masking.html#masking-具体是什么意思",
    "title": "Data Masking(1)",
    "section": "masking 具体是什么意思？",
    "text": "masking 具体是什么意思？\n从上面的例子中也可以看出，所谓的masking，就是词法作用域的优先级。相同变量名在data-masking中会被优先解释为数据框中的列，而非外部环境中的变量。rlang 包所构建的tidy eval框架提供了pronouns来声明变量的所属环境。\n\ncyl &lt;- 1000\n\nmtcars %&gt;%\n  dplyr::summarise(\n    mean_data = mean(.data$cyl),\n    mean_env = mean(.env$cyl)\n  )\n#&gt;   mean_data mean_env\n#&gt; 1    6.1875     1000"
  },
  {
    "objectID": "Blog/Data Masking.html#data-masking-如何工作",
    "href": "Blog/Data Masking.html#data-masking-如何工作",
    "title": "Data Masking(1)",
    "section": "data-masking 如何工作？",
    "text": "data-masking 如何工作？\ndata-masking 依赖R语言的三个特点：\n\ndefuse 变量，如 base R 中的substitute()、rlang 中的enquo(),{{等。\nfirst class environment。环境在R中一个类似list的特殊对象，R 允许将list或dataframe转换为环境。\n\n\nas.environment(mtcars)\n#&gt; &lt;environment: 0x00000223c6c53da0&gt;\n\n\n评估函数——eval()(base)、eval_tidy()(rlang)。\n\n也即：先将变量名转换为defused状态，变得不可用，然后将dataframe转换为环境，最后在转换后的环境中重新评估变量。"
  },
  {
    "objectID": "Blog/Data Masking.html#data-masking-编程模式",
    "href": "Blog/Data Masking.html#data-masking-编程模式",
    "title": "Data Masking(1)",
    "section": "data-masking 编程模式",
    "text": "data-masking 编程模式\n诚如上述，在函数中使用 data-masking，需要特殊处理才能正确解析参数。在rlang官网上，有四种解决方案。\n\nforwarding pattern\n\n\n使用{{\n{{用来直接解析单个参数，并且不丢失原有的信息（观察下面例子列名）。\n\nmy_summarise &lt;- function(data, var) {\n  data %&gt;% dplyr::summarise({{ var }})\n}\n\nmtcars %&gt;% my_summarise(mean(cyl))\n#&gt;   mean(cyl)\n#&gt; 1    6.1875\n\nx &lt;- \"cyl\"\nmtcars %&gt;% my_summarise(mean(.data[[x]]))\n#&gt;   mean(.data[[\"cyl\"]])\n#&gt; 1               6.1875\n\n\n\n...\n... 不要求额外的语法设置，可以直接使用，用来解析多个参数。\n\nmy_group_by &lt;- function(.data, ...) {\n  .data %&gt;% dplyr::group_by(...)\n}\n\nmtcars %&gt;% my_group_by(cyl = cyl * 100, am)\n#&gt; # A tibble: 32 × 11\n#&gt; # Groups:   cyl, am [6]\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21     600   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 2  21     600   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 3  22.8   400   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 4  21.4   600   258   110  3.08  3.22  19.4     1     0     3     1\n#&gt; 5  18.7   800   360   175  3.15  3.44  17.0     0     0     3     2\n#&gt; 6  18.1   600   225   105  2.76  3.46  20.2     1     0     3     1\n#&gt; # ℹ 26 more rows\n\nmy_select &lt;- function(.data, ...) {\n  .data %&gt;% dplyr::select(...)\n}\n\nmtcars %&gt;% my_select(starts_with(\"c\"), vs:carb)\n#&gt;                     cyl carb vs am gear\n#&gt; Mazda RX4             6    4  0  1    4\n#&gt; Mazda RX4 Wag         6    4  0  1    4\n#&gt; Datsun 710            4    1  1  1    4\n#&gt; Hornet 4 Drive        6    1  1  0    3\n#&gt; Hornet Sportabout     8    2  0  0    3\n#&gt; Valiant               6    1  1  0    3\n#&gt; Duster 360            8    4  0  0    3\n#&gt; Merc 240D             4    2  1  0    4\n#&gt; Merc 230              4    2  1  0    4\n#&gt; Merc 280              6    4  1  0    4\n#&gt; Merc 280C             6    4  1  0    4\n#&gt; Merc 450SE            8    3  0  0    3\n#&gt; Merc 450SL            8    3  0  0    3\n#&gt; Merc 450SLC           8    3  0  0    3\n#&gt; Cadillac Fleetwood    8    4  0  0    3\n#&gt; Lincoln Continental   8    4  0  0    3\n#&gt; Chrysler Imperial     8    4  0  0    3\n#&gt; Fiat 128              4    1  1  1    4\n#&gt; Honda Civic           4    2  1  1    4\n#&gt; Toyota Corolla        4    1  1  1    4\n#&gt; Toyota Corona         4    1  1  0    3\n#&gt; Dodge Challenger      8    2  0  0    3\n#&gt; AMC Javelin           8    2  0  0    3\n#&gt; Camaro Z28            8    4  0  0    3\n#&gt; Pontiac Firebird      8    2  0  0    3\n#&gt; Fiat X1-9             4    1  1  1    4\n#&gt; Porsche 914-2         4    2  0  1    5\n#&gt; Lotus Europa          4    2  1  1    5\n#&gt; Ford Pantera L        8    4  0  1    5\n#&gt; Ferrari Dino          6    6  0  1    5\n#&gt; Maserati Bora         8    8  0  1    5\n#&gt; Volvo 142E            4    2  1  1    4\n\n有些函数会将多个参数同时传递给函数中的一个参数，如下例所示。此时c()生成的不是向量，而是tidy-select组合。\n\nmy_pivot_longer &lt;- function(.data, ...) {\n  .data %&gt;% tidyr::pivot_longer(c(...))\n}\n\nmtcars %&gt;% my_pivot_longer(starts_with(\"c\"), vs:carb)\n#&gt; # A tibble: 160 × 8\n#&gt;     mpg  disp    hp  drat    wt  qsec name  value\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1    21   160   110   3.9  2.62  16.5 cyl       6\n#&gt; 2    21   160   110   3.9  2.62  16.5 carb      4\n#&gt; 3    21   160   110   3.9  2.62  16.5 vs        0\n#&gt; 4    21   160   110   3.9  2.62  16.5 am        1\n#&gt; 5    21   160   110   3.9  2.62  16.5 gear      4\n#&gt; 6    21   160   110   3.9  2.88  17.0 cyl       6\n#&gt; # ℹ 154 more rows\n\n\n\nname pattern\n使用tidy eval框架提供的pronouns，可以直接使用参数。\n\nmy_mean &lt;- function(data, var) {\n  data %&gt;% dplyr::summarise(mean = mean(.data[[var]]))\n}\n\nmy_mean(mtcars, \"cyl\")\n#&gt;     mean\n#&gt; 1 6.1875\n\n遗憾的是，这种方法只能处理单个参数的情况。\n\nmtcars %&gt;% dplyr::summarise(.data[c(\"cyl\", \"am\")])\n#&gt; Error in `dplyr::summarise()`:\n#&gt; ℹ In argument: `.data[c(\"cyl\", \"am\")]`.\n#&gt; Caused by error in `.data[c(\"cyl\", \"am\")]`:\n#&gt; ! `[` is not supported by the `.data` pronoun, use `[[` or $ instead.\n\n\n\nbridge pattern\n使用中间桥梁函数解析参数，如across()、transmute()等\n\nacross()\n\nmy_group_by &lt;- function(data, var) {\n  data %&gt;% dplyr::group_by(across({{ var }}))\n}\n\nmtcars %&gt;% my_group_by(starts_with(\"c\"))\n#&gt; # A tibble: 32 × 11\n#&gt; # Groups:   cyl, carb [9]\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n#&gt; 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n#&gt; 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n#&gt; # ℹ 26 more rows\n\n\nmy_group_by &lt;- function(.data, ...) {\n  .data %&gt;% dplyr::group_by(across(c(...)))\n}\n\nmtcars %&gt;% my_group_by(starts_with(\"c\"), vs:gear)\n#&gt; # A tibble: 32 × 11\n#&gt; # Groups:   cyl, carb, vs, am, gear [15]\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n#&gt; 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n#&gt; 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n#&gt; # ℹ 26 more rows\n\n\nmy_group_by &lt;- function(data, vars) {\n  data %&gt;% dplyr::group_by(across(all_of(vars)))\n}\n\nmtcars %&gt;% my_group_by(c(\"cyl\", \"am\"))\n#&gt; # A tibble: 32 × 11\n#&gt; # Groups:   cyl, am [6]\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n#&gt; 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n#&gt; 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n#&gt; # ℹ 26 more rows\n\n\n\n\ntransmute()\n\nmy_pivot_longer &lt;- function(data, ...) {\n  # Forward `...` in data-mask context with `transmute()`\n  # and save the inputs names\n  inputs &lt;- dplyr::transmute(data, ...)\n  names &lt;- names(inputs)\n\n  # Update the data with the inputs\n  data &lt;- dplyr::mutate(data, !!!inputs)\n\n  # Select the inputs by name with `all_of()`\n  tidyr::pivot_longer(data, cols = all_of(names))\n}\n\nmtcars %&gt;% my_pivot_longer(cyl, am = am * 100)\n#&gt; # A tibble: 64 × 11\n#&gt;     mpg  disp    hp  drat    wt  qsec    vs  gear  carb name  value\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1  21     160   110  3.9   2.62  16.5     0     4     4 cyl       6\n#&gt; 2  21     160   110  3.9   2.62  16.5     0     4     4 am      100\n#&gt; 3  21     160   110  3.9   2.88  17.0     0     4     4 cyl       6\n#&gt; 4  21     160   110  3.9   2.88  17.0     0     4     4 am      100\n#&gt; 5  22.8   108    93  3.85  2.32  18.6     1     4     1 cyl       4\n#&gt; 6  22.8   108    93  3.85  2.32  18.6     1     4     1 am      100\n#&gt; # ℹ 58 more rows\n\n使用transmute()创建新的数据框，然后提取name，最后更新数据框。\n\n\nTransformation patterns\n对多个参数执行相同的操作，有下面两种类型：\n\nTransforming inputs with across()\n\nmy_mean &lt;- function(data, ...) {\n  data %&gt;% dplyr::summarise(across(c(...), ~ mean(.x, na.rm = TRUE)))\n}\n\nmtcars %&gt;% my_mean(cyl, carb)\n#&gt;      cyl   carb\n#&gt; 1 6.1875 2.8125\n\nmtcars %&gt;% my_mean(foo = cyl, bar = carb)\n#&gt;      foo    bar\n#&gt; 1 6.1875 2.8125\n\nmtcars %&gt;% my_mean(starts_with(\"c\"), mpg:disp)\n#&gt;      cyl   carb      mpg     disp\n#&gt; 1 6.1875 2.8125 20.09062 230.7219\n\n\n\nTransforming inputs with if_all() and if_any()\n\nfilter_non_baseline &lt;- function(.data, ...) {\n  .data %&gt;% dplyr::filter(if_all(c(...), ~ .x != min(.x, na.rm = TRUE)))\n}\n\nmtcars %&gt;% filter_non_baseline(vs, am, gear)\n#&gt;                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#&gt; Datsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n#&gt; Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n#&gt; Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n#&gt; Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n#&gt; Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#&gt; Volvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "Blog/Code chunks.html",
    "href": "Blog/Code chunks.html",
    "title": "quarto 中R代码块的设置",
    "section": "",
    "text": "每个代码块可以设置一个label，用来标记代码块和代码块的结果，可以用于交叉引用。\n```{r}\n#| label: simple-addition\n\n1 + 1\n```"
  },
  {
    "objectID": "Blog/Code chunks.html#label",
    "href": "Blog/Code chunks.html#label",
    "title": "quarto 中R代码块的设置",
    "section": "",
    "text": "每个代码块可以设置一个label，用来标记代码块和代码块的结果，可以用于交叉引用。\n```{r}\n#| label: simple-addition\n\n1 + 1\n```"
  },
  {
    "objectID": "Blog/Code chunks.html#execute",
    "href": "Blog/Code chunks.html#execute",
    "title": "quarto 中R代码块的设置",
    "section": "execute",
    "text": "execute\n可以使用execute选项来控制代码块；下面是对可用参数的总结。"
  },
  {
    "objectID": "Blog/Code chunks.html#knitr",
    "href": "Blog/Code chunks.html#knitr",
    "title": "quarto 中R代码块的设置",
    "section": "knitr",
    "text": "knitr\n因为quarto除对R代码块支持外，还支持python、Julia，所以除通用设置execute，quarto 还有R语言特有的设置——knitr。\nknitr:\n  opts_chunk:\n    comment: \"#&gt;\"\n    collapse: true\n上面的设置可以自动将R代码的运行结果于代码拼接起来，并使用#&gt;作为注释，达到下面的效果。\n\n1 + 1\n#&gt; 2"
  },
  {
    "objectID": "Blog/Code chunks.html#行代码",
    "href": "Blog/Code chunks.html#行代码",
    "title": "quarto 中R代码块的设置",
    "section": "行代码",
    "text": "行代码\n处理上面使用```{r}的代码块，quarto还使用`r的行代码。"
  },
  {
    "objectID": "Blog/ExWAS.html",
    "href": "Blog/ExWAS.html",
    "title": "暴露组学关联分析",
    "section": "",
    "text": "本篇是对omicRexposome包中“association”分析流程的拆解，其源代码为MultiDataSet-association.R，omicRexposome包的官方分析流程可以参考教程，该包相关文献为Multi-omics signatures of the human earlylife exposome。"
  },
  {
    "objectID": "Blog/ExWAS.html#安装相关r包",
    "href": "Blog/ExWAS.html#安装相关r包",
    "title": "暴露组学关联分析",
    "section": "安装相关R包",
    "text": "安装相关R包\n我们的测试数据来自于brgedata包，同时需要安装rexposome和omicRexposome包。\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\npackages &lt;- c(\n  \"Biobase\", \"mice\", \"MultiDataSet\", \"lsr\", \"FactoMineR\",\n  \"stringr\", \"circlize\", \"corrplot\", \"ggplot2\", \"reshape2\", \"pryr\",\n  \"scales\", \"imputeLCMD\", \"scatterplot3d\", \"glmnet\", \"gridExtra\",\n  \"grid\", \"Hmisc\", \"gplots\", \"gtools\", \"S4Vectors\",\n  \"Biobase\", \"methods\", \"snpStats\", \"limma\", \"sva\", \"ggplot2\",\n  \"ggrepel\", \"PMA\", \"omicade4\", \"ggplot2\", \"qqman\", \"gridExtra\"\n)\n\nfor (pkg in packages) {\n  if (!pkg %in% rownames(installed.packages())) {\n    message(\"Installing \", pkg)\n    BiocManager::install(pkg)\n  }\n}\n\ndevtools::install_github(\"isglobal-brge/rexposome\")\n#&gt; Skipping install of 'rexposome' from a github remote, the SHA1 (0bb431c5) has not changed since last install.\n#&gt;   Use `force = TRUE` to force installation\ndevtools::install_github(\"isglobal-brge/omicRexposome\")\n#&gt; Skipping install of 'omicRexposome' from a github remote, the SHA1 (c8716b17) has not changed since last install.\n#&gt;   Use `force = TRUE` to force installation\ndevtools::install_github(\"isglobal-brge/brgedata\")\n#&gt; Skipping install of 'brgedata' from a github remote, the SHA1 (e649549b) has not changed since last install.\n#&gt;   Use `force = TRUE` to force installation"
  },
  {
    "objectID": "Blog/ExWAS.html#关联分析思路",
    "href": "Blog/ExWAS.html#关联分析思路",
    "title": "暴露组学关联分析",
    "section": "关联分析思路",
    "text": "关联分析思路\n暴露组学与单组学的关联分析思路大致为：单组学设置为响应变量Y，暴露组学中的单个暴露因素设置为X，添加表型数据中的某些表型为协变量Z，使用limma进行建模分析，循环每个暴露因素，得到每个暴露因素与响应变量的回归系数和显著度。除此之外，源码MultiDataSet-association.R文件中还添加了检查输入数据和进行SVA分析的功能。\n输入检查\n\n\n单组学，暴露组，表型数据，它们的样本数必须一致，删除不一致的样本。\nif (warnings | verbose) {\n  warning(\"Sets from 'MultiDataSet' will be reduced to common samples\")\n}\n\nl1 &lt;- vapply(Biobase::sampleNames(object)[c(omicset, expset)], length, FUN.VALUE = numeric(1))\nobject &lt;- MultiDataSet::commonSamples(object)\nl2 &lt;- sapply(Biobase::sampleNames(object)[c(omicset, expset)], length)\nl3 &lt;- mapply(\"-\", l1, l2, SIMPLIFY = FALSE)\n\nif (verbose) {\n  message(paste(unlist(l3), names(l3),\n    sep = \" samples were reduced from \", collapse = \", \"\n  ))\n}\n\n\n暴露因素和表型数据构成了设计矩阵，所以它们的数据必须没有NA值，\nna.loc &lt;- rowSums(apply(exp.dt, 2, is.na))\nna.loc &lt;- which(na.loc != 0)\nif (length(na.loc) != 0) {\n  if (warnings | verbose) {\n    warning(\n      \"There are missing values. \", length(na.loc),\n      \" samples will be removed.\"\n    )\n  }\n  exp.dt &lt;- exp.dt[-na.loc, , drop = FALSE]\n}\n且不是常量。\ntbl &lt;- sapply(all.vars(design), function(x) length(table(exp.dt[, x, drop = FALSE])))\nif (sum(!sapply(tbl, \"&gt;\", 1)) != 0) {\n  warning(\n    \"When testing for '\", ex, \"', at last one covariate \",\n    \"is constant (\",\n    paste(paste(names(tbl), tbl, sep = \": \"), collapse = \", \"),\n    \")\"\n  )\n}\n\n替代变量分析（SVA）\n当数据的异质性很大时，即使我们已经测量并尝试校正了一些已知的协变量（如年龄、性别、处理组等），可能仍然存在未被观测到的系统性变异来源（例如实验批次、样本处理时间、技术噪声等），这些因素会影响分析结果，导致假阳性或降低检验效能。\nSVA的核心思想是：识别并建模数据中主要的隐性变异模式（即“替代变量”），然后在后续的关联分析中将这些变量作为协变量加以调整，从而提高结果的准确性和可重复性。\ndesign.mm &lt;- model.matrix(formula(design), data = exp.dt)\n# If required, apply SVA\nn.sv &lt;- NA\nif (sva == \"fast\") {\n  ## Determine number of surrogate variables\n  Y.r &lt;- t(stats::resid(stats::lm(t(omic) ~ exp.dt[, ex])))\n  # ,data=exp.dt)))\n  n.sv &lt;- isva::EstDimRMT(Y.r, FALSE)$dim + 1\n\n  if (n.sv &gt; 0) {\n    sv.obj &lt;- SmartSVA::smartsva.cpp(omic,\n      design.mm,\n      mod0 = NULL, n.sv = n.sv\n    )\n    design.mm &lt;- cbind(design.mm, sv.obj$sv)\n  }\n} else if (sva == \"slow\") {\n  if (verbose | warnings) {\n    message(\"Computing SVA. This step can be very time consuming.\")\n    if (is.null(vfilter)) {\n      message(\"Consider using argument 'vfilter'.\")\n    }\n  }\n\n  ## Determine number of surrogate variables\n  n.sv &lt;- sva::num.sv(omic,\n    design.mm,\n    vfilter = vfilter\n  )\n  if (n.sv &gt; 0) {\n    sv.obj &lt;- sva::sva(omic, design.mm,\n      # design.mm[ , -1, drop=FALSE],\n      n.sv = n.sv, vfilter = vfilter\n    )\n    design.mm &lt;- cbind(design.mm, sv.obj$sv)\n  }\n}"
  },
  {
    "objectID": "Blog/ExWAS.html#整理上面的思路并总结出自己的分析流程",
    "href": "Blog/ExWAS.html#整理上面的思路并总结出自己的分析流程",
    "title": "暴露组学关联分析",
    "section": "整理上面的思路并总结出自己的分析流程",
    "text": "整理上面的思路并总结出自己的分析流程\n提取分析数据\n\n# 表型数据\ndata(\"brge_expo\", package = \"brgedata\")\npDat &lt;- Biobase::pData(brge_expo) |&gt;\n  tibble::rownames_to_column(var = \"SampleID\") |&gt;\n  tibble::as_tibble()\n#&gt; Loading required package: rexposome\n#&gt; Loading required package: Biobase\n#&gt; Loading required package: BiocGenerics\n#&gt; \n#&gt; Attaching package: 'BiocGenerics'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     IQR, mad, sd, var, xtabs\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n#&gt;     colnames, dirname, do.call, duplicated, eval, evalq, Filter,\n#&gt;     Find, get, grep, grepl, intersect, is.unsorted, lapply, Map,\n#&gt;     mapply, match, mget, order, paste, pmax, pmax.int, pmin,\n#&gt;     pmin.int, Position, rank, rbind, Reduce, rownames, sapply,\n#&gt;     saveRDS, setdiff, table, tapply, union, unique, unsplit,\n#&gt;     which.max, which.min\n#&gt; Welcome to Bioconductor\n#&gt; \n#&gt;     Vignettes contain introductory material; view with\n#&gt;     'browseVignettes()'. To cite Bioconductor, see\n#&gt;     'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n#&gt; \n#&gt; Attaching package: 'rexposome'\n#&gt; The following object is masked from 'package:BiocGenerics':\n#&gt; \n#&gt;     plotPCA\n\n# 暴露组学数据\ndata(\"brge_expo\", package = \"brgedata\")\nexDat &lt;- rexposome::expos(brge_expo) |&gt;\n  tibble::as_tibble(rownames = \"SampleID\") |&gt;\n  tibble::as_tibble()\n\n# 转录组学数据\ndata(\"brge_gexp\", package = \"brgedata\")\ngDat &lt;- Biobase::exprs(brge_gexp) |&gt;\n  tibble::as_tibble(rownames = \"geneID\") |&gt;\n  tibble::as_tibble()\n\n组学数据预处理\n不同组学数据需要根据要解释的效应水平进行预处理。例如，甲基化数据需要解释为变化范围，不做任何处理；其他如转录组，需要解释为变化倍数，要进行log转换：\n\nScaleOmic &lt;- function(data, omics) {\n  if (omics == \"methylation\") {\n    data &lt;- data\n  } else {\n    # data &lt;- log2(data)\n    data &lt;- data # 示例数据已经log2处理\n  }\n}\n\nSVA分析函数\n\n# SVA &lt;- function(omic, design.omic, y) {\n#   n.sv &lt;- sva::num.sv(\n#     omic,\n#     design.omic\n#   )\n\n#   if (n.sv &gt; 0) {\n#     sv.obj &lt;- sva::sva(as.matrix(omic), design.omic, n.sv = n.sv)\n#     design.omic &lt;- cbind(design.omic, sv.obj$sv)\n#   }\n\n#   return(design.omic)\n# }\n\n#' 快速计算SVA\n#'\n#' @param omic {matrix} 组学数据，行是样本，列是特征\n#' @param design.omic {matrix} 设计矩阵\n#'\n#' @return 增加了SVA的设计矩阵\n#'\nSVA &lt;- function(omic, design.omic, y) {\n  omic &lt;- as.matrix(omic)\n\n  Y.r &lt;- t(stats::resid(stats::lm(t(omic) ~ design.omic[, 2])))\n  n.sv &lt;- isva::EstDimRMT(Y.r, FALSE)$dim + 1\n  if (n.sv &gt; 0) {\n    sv.obj &lt;- SmartSVA::smartsva.cpp(\n      omic,\n      design.omic,\n      mod0 = NULL, n.sv = n.sv\n    )\n    design.omic &lt;- cbind(design.omic, sv.obj$sv)\n  }\n  return(design.omic)\n}\n\nlimma 建模函数\n\n#' 单个暴露因素与组学的关联分析\n#'\n#' @param ex {string} 暴露因素\n#' @param covars {string-vector} 协变量\n#' @param exDat {tibble} 暴露因素数据\n#' @param pDat {tibble} 表型数据\n#' @param gDat {tibble} 组学数据\n#'\n#' @return {tibble} \\code{limma::topTable}结果，并更新了logFC和SE\n#'\nexwas &lt;- function(ex, covars, exDat, pDat, gDat) {\n  # ex &lt;- \"Ben_p\"\n  X &lt;- dplyr::left_join(pDat, exDat, by = \"SampleID\") |&gt;\n    dplyr::select(SampleID, dplyr::all_of(c(ex, covars)))\n  design.omic &lt;- stats::model.matrix(\n    as.formula(paste(\"~ \", paste(c(ex, covars), collapse = \"+\"))),\n    data = X\n  )\n\n  gDat &lt;- gDat |&gt;\n    tibble::column_to_rownames(var = \"geneID\") |&gt;\n    dplyr::select(dplyr::all_of(X$SampleID[as.numeric(rownames(design.omic))])) |&gt;\n    as.data.frame()\n\n  gDat[] &lt;- ScaleOmic(gDat, omics = \"transcriptome\")\n\n  design.omic &lt;- SVA(gDat, design.omic)\n\n  fit &lt;- limma::lmFit(gDat, design.omic)\n  fit &lt;- limma::eBayes(fit)\n  res &lt;- limma::topTable(fit, coef = ex, number = Inf, adjust.method = \"BH\", sort.by = \"B\") |&gt;\n    tibble::rownames_to_column(var = \"geneID\") |&gt;\n    tibble::as_tibble() |&gt;\n    # design.omic，stdev.unscaled 是matrix，只能用[, ex]索引\n    dplyr::mutate(\n      logFC = logFC * IQR(design.omic[, ex]),\n      SE = (sqrt(fit$s2.post) * fit$stdev.unscaled[, ex])\n    )\n\n  return(res)\n}\n\n循环暴露因素分析\n\nsampleP &lt;- pDat$SampleID\nsampleE &lt;- exDat$SampleID\nsampleG &lt;- colnames(gDat)[-1]\n\nsamples &lt;- purrr::reduce(list(sampleP, sampleE, sampleG), intersect)\n\nexDat &lt;- dplyr::filter(exDat, SampleID %in% samples)\npDat &lt;- dplyr::filter(pDat, SampleID %in% samples)\ngDat &lt;- dplyr::select(gDat, geneID, dplyr::all_of(samples))\n\ncovars &lt;- c(\"Asthma\", \"Sex\", \"Age\")\nexpos &lt;- colnames(exDat)[-1]\nres &lt;- purrr::map(\n  expos,\n  ~ exwas(.x, covars, exDat, pDat, gDat)\n) |&gt;\n  purrr::set_names(expos)\n\nres[[1]]\n#&gt; # A tibble: 67,528 × 8\n#&gt;   geneID            logFC AveExpr     t   P.Value adj.P.Val        B     SE\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 TC13000973.hg.1  0.161     2.72  4.62 0.0000244     0.564  0.311   0.301 \n#&gt; 2 TC13000069.hg.1  0.165     2.68  4.60 0.0000268     0.564  0.260   0.0893\n#&gt; 3 TC08000518.hg.1 -0.0657    3.78 -4.55 0.0000311     0.564  0.181   0.161 \n#&gt; 4 TC04002007.hg.1  0.151     1.55  4.53 0.0000334     0.564  0.142   0.184 \n#&gt; 5 TC13001656.hg.1 -0.174     3.54 -4.46 0.0000428     0.578  0.00807 0.0933\n#&gt; 6 TC04000954.hg.1 -0.0673    6.46 -4.38 0.0000557     0.579 -0.134   0.0562\n#&gt; # ℹ 67,522 more rows\n\nlamda Score\n因为我们执行的是多重假设检验，需要对检验结果的可靠性进行评价，常见的是绘制QQ图，这里也可以使用lambda score。\n\nlambdaClayton &lt;- function(x, trim = 0.5) {\n  xx &lt;- qnorm(1 - x)^2\n  N &lt;- length(xx)\n  obsvd &lt;- sort(xx, na.last = NA)\n  expctd &lt;- qchisq(p = (1:N) / (N + 1), 1)\n  Nu &lt;- floor(trim * N)\n  lambda &lt;- mean(obsvd[1:Nu]) / mean(expctd[1:Nu])\n  lambda\n}\n\n\nlambdaClayton(res[[1]]$P.Value)\n#&gt; [1] 1.004892\n\n绘制曼哈顿图\n\n#' 绘制曼哈顿图\n#'\n#' @param exwasRes {list} exwas结果\n#' @param th {numeric} 显著性阈值，默认1e-5\n#'\n#' @return 曼哈顿图\n#'\nGetManhattan &lt;- function(exwasRes, th = 10e-5, exposureInfo = NULL) {\n  plotDat &lt;- purrr::list_rbind(exwasRes, names_to = \"Exposure\") |&gt;\n    dplyr::filter(`P.Value` &lt; th * 100) |&gt;\n    dplyr::mutate(\n      color = ifelse(logFC &gt; 0, \"up\", \"down\"),\n      color = ifelse(`P.Value` &lt; th, color, \"none\"),\n      color = factor(color, levels = c(\"up\", \"down\", \"none\"))\n    )\n\n  vlineDat &lt;- tibble::tibble(x = seq_len(length(exwasRes))[-1] - 0.5)\n\n  p1 &lt;- ggplot2::ggplot() +\n    ggplot2::geom_vline(data = vlineDat, ggplot2::aes(xintercept = x), color = \"lightgray\") +\n    ggplot2::geom_hline(yintercept = -log10(th), linetype = \"dashed\", color = \"darkgray\") +\n    ggplot2::geom_jitter(data = plotDat, ggplot2::aes(x = Exposure, y = -log10(`P.Value`), color = color), alpha = 0.6) +\n    ggplot2::scale_color_manual(values = c(\"up\" = \"red\", \"down\" = \"blue\", \"none\" = \"grey\")) +\n    # ggplot2::scale_x_discrete(expand = c(0, 0)) +\n    # ggplot2::scale_y_continuous(expand = c(0, 0)) +\n    ggplot2::theme_test() +\n    ggplot2::theme(\n      axis.title.x = ggplot2::element_blank(),\n      axis.text.x = ggplot2::element_blank(),\n      axis.ticks.x = ggplot2::element_blank()\n    ) +\n    ggplot2::labs(y = \"-log10(P.Value)\")\n\n  exDat &lt;- tibble::tibble(\n    Exposure = names(exwasRes),\n    group = names(exwasRes),\n    color = \"transparent\"\n  )\n\n  p2 &lt;- ggplot2::ggplot(exDat, ggplot2::aes(x = Exposure, y = \"Exposure\", fill = color)) +\n    ggplot2::geom_tile() +\n    ggplot2::geom_text(ggplot2::aes(label = Exposure), color = \"black\", angle = 90, size = 8 / ggplot2::.pt) +\n    ggplot2::scale_fill_identity() +\n    ggplot2::theme_void() +\n    ggplot2::theme(legend.position = \"none\")\n\n  p &lt;- aplot::insert_bottom(p1, p2, height = 0.2)\n\n  return(p)\n}\nGetManhattan(res)"
  },
  {
    "objectID": "Blog/替换多对字符串.html",
    "href": "Blog/替换多对字符串.html",
    "title": "替换多对字符串",
    "section": "",
    "text": "考虑下面的情景：你提取到每个物种特有的keggID号，为了节省空间，你将keggID号拼接了起来。与此同时，你需要将keggID号对应得代谢物名称追加到数据集中。\n\n# 示例数据\ndat &lt;- tibble::tribble(\n  ~species, ~keggID,\n  \"Homo sapiens\", \"C00001;C00002;C00003\",\n  \"Mus musculus\", \"C00002;C00004;C00005\",\n  \"Rattus norvegicus\", \"C00001;C00002;C00006\"\n)\n\nkeggID_metabolite &lt;- tibble::tribble(\n  ~keggID, ~metabolite,\n  \"C00001\", \"Amino acid\",\n  \"C00002\", \"Carbohydrate\",\n  \"C00003\", \"Lipid\",\n  \"C00004\", \"Carbohydrate\",\n  \"C00005\", \"Lipid\",\n  \"C00006\", \"Carbohydrate\"\n)\n\n我们可以按照常规做法：将dat中的keggID列拆分，然后将代谢物信息追加到数据中，最后再进行字符串拼接。\n\nres1 &lt;- dat |&gt;\n  tidyr::separate_rows(keggID, sep = \";\") |&gt;\n  dplyr::left_join(keggID_metabolite, by = \"keggID\") |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::summarise(\n    keggID = paste(keggID, collapse = \";\"),\n    metabolite = paste(metabolite, collapse = \";\")\n  )\nres1\n#&gt; # A tibble: 3 × 3\n#&gt;   species           keggID               metabolite                          \n#&gt;   &lt;chr&gt;             &lt;chr&gt;                &lt;chr&gt;                               \n#&gt; 1 Homo sapiens      C00001;C00002;C00003 Amino acid;Carbohydrate;Lipid       \n#&gt; 2 Mus musculus      C00002;C00004;C00005 Carbohydrate;Carbohydrate;Lipid     \n#&gt; 3 Rattus norvegicus C00001;C00002;C00006 Amino acid;Carbohydrate;Carbohydrate\n\n我们也可以使用泛函map() + reduce2() + stringr::str_replace_all()直接替换字符串。\n\nStrReplace &lt;- function(string, patterns, replacements) {\n  purrr::reduce2(\n    patterns,\n    replacements,\n    .f = stringr::str_replace_all,\n    .init = string\n  )\n}\n\nres2 &lt;- dat |&gt;\n  dplyr::mutate(\n    metabolite = StrReplace(\n      keggID,\n      patterns = keggID_metabolite$keggID,\n      replacements = keggID_metabolite$metabolite\n    )\n  )\nres2\n#&gt; # A tibble: 3 × 3\n#&gt;   species           keggID               metabolite                          \n#&gt;   &lt;chr&gt;             &lt;chr&gt;                &lt;chr&gt;                               \n#&gt; 1 Homo sapiens      C00001;C00002;C00003 Amino acid;Carbohydrate;Lipid       \n#&gt; 2 Mus musculus      C00002;C00004;C00005 Carbohydrate;Carbohydrate;Lipid     \n#&gt; 3 Rattus norvegicus C00001;C00002;C00006 Amino acid;Carbohydrate;Carbohydrate\n\n进行两种方法的耗时对比：\n\nmethod1 &lt;- function(data, keggID_metabolite) {\n  res &lt;- data |&gt;\n    tidyr::separate_rows(keggID, sep = \";\") |&gt;\n    dplyr::left_join(keggID_metabolite, by = \"keggID\") |&gt;\n    dplyr::group_by(species) |&gt;\n    dplyr::summarise(\n      keggID = paste(keggID, collapse = \";\"),\n      metabolite = paste(metabolite, collapse = \";\")\n    )\n  return(res)\n}\n\nmethod2 &lt;- function(data, keggID_metabolite) {\n  res &lt;- data |&gt;\n    dplyr::mutate(\n      metabolite = StrReplace(\n        keggID,\n        patterns = keggID_metabolite$keggID,\n        replacements = keggID_metabolite$metabolite\n      )\n    )\n  return(res)\n}\n\nbench_dat &lt;- function(dat, n) {\n  data &lt;- dplyr::bind_rows(replicate(n, dat, simplify = FALSE)) |&gt;\n    dplyr::mutate(species = dplyr::row_number())\n\n  bench::mark(\n    M1 = method1(data, keggID_metabolite),\n    M2 = method2(data, keggID_metabolite),\n    time_unit = \"ms\"\n  )\n}\n\nperformances &lt;- purrr::map_dfr(10^(1:5), ~ bench_dat(dat, .x))\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n\ndf_perf &lt;- tibble::tibble(\n  n = rep(10^(1:5), each = 2),\n  method = attr(performances$expression, \"description\"),\n  `time(s)` = performances$median / 100,\n  `memory(KB)` = as.numeric(bench::as_bench_bytes(performances$mem_alloc)) / 1024 / 1024\n) |&gt;\n  tidyr::pivot_longer(cols = c(`time(s)`, `memory(KB)`), names_to = \"type\", values_to = \"value\")\n\nlibrary(ggplot2)\nggplot(df_perf, aes(n, value, col = method)) +\n  geom_point(size = 2) +\n  geom_line(linetype = 2) +\n  scale_x_log10() +\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    x = \"Length of x\",\n    y = \"\",\n    color = \"Method\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n从结果中我们可以看到，随着要替换字符串的向量增加，无论是耗时还是内存占用，方法二都具有明显优势。\n\n\n\n Back to top"
  },
  {
    "objectID": "Blog/绘制点脊图.html",
    "href": "Blog/绘制点脊图.html",
    "title": "绘制点脊图",
    "section": "",
    "text": "最近在绘制气泡图时，发现点太密集了，想再绘制一个核密度图，来展示数据的分布。\n最先考虑的方案是：将geom_density()和geom_point()合并，创建一个新的geom。由于没有创建过自定义的geom，AI生成的结果不易修改，最终放弃。\n最后想到了ggridges包中可以分组单独绘制密度图，稍加修改得到最总的绘制结果。\n\nlibrary(ggplot2)\nlibrary(ggridges)\n\nset.seed(123)\ndf &lt;- data.frame(\n  category = factor(rep(LETTERS[1:3], each = 100)),\n  value = c(rnorm(100, 0), rnorm(100, 1), rnorm(100, 2))\n)\n\n# 调整 density_ridges 的 scale 参数，并设置点的位置\nggplot(df, aes(x = value, y = category)) +\n  geom_density_ridges(\n    aes(fill = category),\n    scale = 0.4, # 控制密度图的高度（较小值会使密度图更扁平）\n    rel_min_height = 0.01 # 设置最小高度以避免尾部过长\n  ) +\n  geom_point(aes(color = category), size = 5, position = position_nudge(y = -0.2))\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html",
    "href": "Books/Advanced R(2e)/10 Function factories.html",
    "title": "10 Function factories",
    "section": "",
    "text": "function factories就是能创建函数的函数。下面是一个示例：使用power1()函数生成square()和cube()函数。\n\npower1 &lt;- function(exp) {\n  function(x) {\n    x^exp\n  }\n}\n\nsquare &lt;- power1(2)\ncube &lt;- power1(3)\n\nsquare()和cube()函数被称为manufactured functions，与function factories相对，这种叫称呼的唯一意义就是区分函数的来源，本质都是函数，没有区别。\n\nsquare(3)\n#&gt; [1] 9\ncube(3)\n#&gt; [1] 27\n\n我们在之前章节中分别介绍了能够实现function factories的三大基石：\n\n6.2.3节，介绍了R中的函数都是第一类函数（first-class functions），使用&lt;-直接将function()创建的函数赋值给变量。\n7.4.2节，介绍了在创建函数时会绑定创建时的环境，形成闭包。\n7.4.4节，介绍了函数每次运行时都会创建临时运行环境——最终变成manufunction factories的闭包环境。\n\n本章，我们介绍如何使用上述的三个概念，创建function factories，并将其应用到可视化与统计处理中。\n在三种主要的泛函编程工具(functionals, function factories, and function operators)中，function factories是使用最少的。总的来说，它们并不倾向于降低整体代码复杂度，而是将复杂度分割成更容易理解的块。函数工厂也是非常有用的函数运算符的重要组成部分，你将在第 11 章中学习。\n\n\n\n10.2节：function factories如何工作和使用\n10.3节：在可视化中的应用\n10.4节：在统计分析中的应用\n10.5节：与泛函联用\n\n\n\n\n熟悉6.2.3，7.4.2，7.4.4章节中的内容。\n我们使用rlang包检视function factories；使用ggplot2，scale包创建可视化示例。\n\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(scales)",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html#introduction",
    "href": "Books/Advanced R(2e)/10 Function factories.html#introduction",
    "title": "10 Function factories",
    "section": "",
    "text": "function factories就是能创建函数的函数。下面是一个示例：使用power1()函数生成square()和cube()函数。\n\npower1 &lt;- function(exp) {\n  function(x) {\n    x^exp\n  }\n}\n\nsquare &lt;- power1(2)\ncube &lt;- power1(3)\n\nsquare()和cube()函数被称为manufactured functions，与function factories相对，这种叫称呼的唯一意义就是区分函数的来源，本质都是函数，没有区别。\n\nsquare(3)\n#&gt; [1] 9\ncube(3)\n#&gt; [1] 27\n\n我们在之前章节中分别介绍了能够实现function factories的三大基石：\n\n6.2.3节，介绍了R中的函数都是第一类函数（first-class functions），使用&lt;-直接将function()创建的函数赋值给变量。\n7.4.2节，介绍了在创建函数时会绑定创建时的环境，形成闭包。\n7.4.4节，介绍了函数每次运行时都会创建临时运行环境——最终变成manufunction factories的闭包环境。\n\n本章，我们介绍如何使用上述的三个概念，创建function factories，并将其应用到可视化与统计处理中。\n在三种主要的泛函编程工具(functionals, function factories, and function operators)中，function factories是使用最少的。总的来说，它们并不倾向于降低整体代码复杂度，而是将复杂度分割成更容易理解的块。函数工厂也是非常有用的函数运算符的重要组成部分，你将在第 11 章中学习。\n\n\n\n10.2节：function factories如何工作和使用\n10.3节：在可视化中的应用\n10.4节：在统计分析中的应用\n10.5节：与泛函联用\n\n\n\n\n熟悉6.2.3，7.4.2，7.4.4章节中的内容。\n我们使用rlang包检视function factories；使用ggplot2，scale包创建可视化示例。\n\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(scales)",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html#factory-fundamental",
    "href": "Books/Advanced R(2e)/10 Function factories.html#factory-fundamental",
    "title": "10 Function factories",
    "section": "Factory fundamental",
    "text": "Factory fundamental\n实现function factories的关键原理可以表述为：function factories的执行环境是manufactured functions的创建（闭包）环境。本节将通过互动探索和一些图表帮助你更深刻地理解这一原理。\n\nEnvironments\n首先查看一下square()和cube()函数。\n\nsquare\n#&gt; function (x) \n#&gt; {\n#&gt;     x^exp\n#&gt; }\n#&gt; &lt;environment: 0x000001acbf98c880&gt;\n\ncube\n#&gt; function (x) \n#&gt; {\n#&gt;     x^exp\n#&gt; }\n#&gt; &lt;bytecode: 0x000001acc0c0a458&gt;\n#&gt; &lt;environment: 0x000001acc1367208&gt;\n\n从两个函数的结构中，我们可以清晰地知道参数x的值来源，但参数exp的值来源呢？仔细观察会发现，两个函数的主体结构是相同的，它们绑定的创建环境不同。\n下面我们使用rlang::env_print()函数查看各自的创建环境。\n\nenv_print(square)\n#&gt; &lt;environment: 0x000001acbf98c880&gt;\n#&gt; Parent: &lt;environment: global&gt;\n#&gt; Bindings:\n#&gt; • exp: &lt;dbl&gt;\n\nenv_print(cube)\n#&gt; &lt;environment: 0x000001acc1367208&gt;\n#&gt; Parent: &lt;environment: global&gt;\n#&gt; Bindings:\n#&gt; • exp: &lt;dbl&gt;\n\n从结果中，我们可以看到两个函数的创建函数的父环境都是全局环境，同时都绑定了变量exp。\n我们使用rlang::fn_env()函数查看环境中变量的值。\n\nfn_env(square)$exp\n#&gt; [1] 2\n\nfn_env(cube)$exp\n#&gt; [1] 3\n\n终于，我们找到了square和cube函数的不同之处：square函数的创建环境绑定的变量exp的值为2，而cube函数的创建环境绑定的变量exp的值为3。\n\n\nDiagram conventions\npower1()，square()，cube()三个函数的关系可以用下面示意图表示：\n\n不考虑过多细节，我们可以抽象出下面两条规律：\n\n任何自由浮动的变量都存在于全局变量（function factories的创建环境）中。\n任何没有显式父级的环境都继承自全局环境（function factories的创建环境）。\n\n\n当执行square(10)时，我们可以预料到：x^exp中的x在函数的执行环境中，exp在函数的创建环境中。\n\n\n\nForcing evaluation\n在实际使用过程中，power1()函数会因为惰性评估造成bug。\n\nx &lt;- 2\nsquare &lt;- power1(x)\nx &lt;- 3\n\n此时运行square(2)，不会返回4，而是8。\n\nsquare(2)\n#&gt; [1] 8\n\n因为power1()中的x只有在square()被调用时才会被评估。这种由于惰性评估造成的bug，在由function factories生成manufactured functions时广泛存在。\n我们可以使用force()函数来强制在创建square()时，power1()的参数x就被评估。\n\npower2 &lt;- function(exp) {\n  force(exp)\n  function(x) {\n    x^exp\n  }\n}\n\nx &lt;- 2\nsquare &lt;- power2(x)\nx &lt;- 3\nsquare(2)\n#&gt; [1] 4\n\n如果输入的参数被调用，那么它就可以被视作“force”了。例如exp &lt;- exp + 1 - 1。\n\npower3 &lt;- function(exp) {\n  exp &lt;- exp + 1 - 1\n  function(x) {\n    x^exp\n  }\n}\nx &lt;- 2\nsquare &lt;- power3(x)\nx &lt;- 3\nsquare(2)\n#&gt; [1] 4\n\n\n\nStateful functions\n6.4.3节中讲到，函数每次执行都会创建执行环境，我们无法将函数的第一次调用与第二次调用进行关联。但是function factories可以允许我们进行关联，保持每次调用时的状态。\n\nmanufactured functions的执行环境是function factories的创建环境——唯一且固定。\nR 中的&lt;&lt;-允许修改创建环境中的变量。\n\n下面是一个记录函数被调用次数的状态函数例子：\n\nnew_counter &lt;- function() {\n  i &lt;- 0\n\n  function() {\n    i &lt;&lt;- i + 1\n    i\n  }\n}\n\ncounter_one &lt;- new_counter()\ncounter_two &lt;- new_counter()\n\ncounter_one()和counter_two()创建时，i的初始值是0。\n\n每被调用一次，i的值都会加1。\n\ncounter_one()\n#&gt; [1] 1\ncounter_one()\n#&gt; [1] 2\ncounter_two()\n#&gt; [1] 1\n\n\n状态函数最好用于调节。一旦函数开始管理多个变量的状态，最好切换到 R6 面向对象, 我们会在第14章中介绍。\n\n\nGarbage collection\n因为manufactured functions的执行环境是唯一且固定的，我们无法在全局环境中删除其创建的变量，当它内部创建了内存消耗过大的变量时，我们要手动定义删除这些变量。\n\nf1 &lt;- function(n) {\n  x &lt;- runif(n)\n  m &lt;- mean(x)\n  function() m\n}\n\ng1 &lt;- f1(1e6)\nlobstr::obj_size(g1)\n#&gt; 8.00 MB\n\nf2 &lt;- function(n) {\n  x &lt;- runif(n)\n  m &lt;- mean(x)\n  rm(x)\n  function() m\n}\n\ng2 &lt;- f2(1e6)\nlobstr::obj_size(g2)\n#&gt; 504 B",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html#graphical-factories",
    "href": "Books/Advanced R(2e)/10 Function factories.html#graphical-factories",
    "title": "10 Function factories",
    "section": "Graphical factories",
    "text": "Graphical factories\n本节我们给出一些应用function factories到ggplot2的例子。\n\nLabelling\nscales 包提供了许多function factories，例如其中的formatter函数：它根据参数返回一个函数，用来生成不同格式的标签。\n\ny &lt;- c(12345, 123456, 1234567)\ncomma_format()(y)\n#&gt; [1] \"12,345\"    \"123,456\"   \"1,234,567\"\n\nnumber_format(scale = 1e-3, suffix = \" K\")(y)\n#&gt; [1] \"12 K\"    \"123 K\"   \"1 235 K\"\n\nscales 包的这些function factories，可以说就是为了ggplot2服务的，例如laebls参数需要提供的值就是一个函数，用来对轴标签进行格式化。\n\ndf &lt;- data.frame(x = 1, y = y)\ncore &lt;- ggplot(df, aes(x, y)) +\n  geom_point() +\n  scale_x_continuous(breaks = 1, labels = NULL) +\n  labs(x = NULL, y = NULL)\n\ncore\n\n\n\n\n\n\n\ncore + scale_y_continuous(\n  labels = comma_format()\n)\n\n\n\n\n\n\n\ncore + scale_y_continuous(\n  labels = number_format(scale = 1e-3, suffix = \" K\")\n)\n\n\n\n\n\n\n\ncore + scale_y_continuous(\n  labels = scientific_format()\n)\n\n\n\n\n\n\n\n\n\n\nHistogram bins\ngeom_histogram()的binwidth参数除接受一个数值外，也可以接受一个函数。函数在分组绘制histogram时十分有用，因为它会按组别计算bin宽度，保持bin的数目一致。下面是一个例子：\n\n# construct some sample data with very different numbers in each cell\nsd &lt;- c(1, 5, 15)\nn &lt;- 100\n\ndf &lt;- data.frame(x = rnorm(3 * n, sd = sd), sd = rep(sd, n))\n\nggplot(df, aes(x)) +\n  geom_histogram(binwidth = 2) +\n  facet_wrap(~sd, scales = \"free_x\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\n我们在生成数据时，每个组的数据量是相同的，但区间不同；导致当设置binwidth参数为固定值时，不同组别的bin数目不同的，相反我们应该固定bin的数目，根据bin数据计算binwidth。\n\nbinwidth_bins &lt;- function(n) {\n  force(n)\n\n  function(x) {\n    (max(x) - min(x)) / n\n  }\n}\n\nggplot(df, aes(x)) +\n  geom_histogram(binwidth = binwidth_bins(20)) +\n  facet_wrap(~sd, scales = \"free_x\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\nbase R 提供了一些计算最优binwidth的函数，例如nclass.Sturges(),nclass.scott(),nclass.FD()，我们可以将其打包成一个function factories。\n\nbase_bins &lt;- function(type) {\n  fun &lt;- switch(type,\n    Sturges = nclass.Sturges,\n    scott = nclass.scott,\n    FD = nclass.FD,\n    stop(\"Unknown type\", call. = FALSE)\n  )\n\n  function(x) {\n    (max(x) - min(x)) / fun(x)\n  }\n}\n\nggplot(df, aes(x)) +\n  geom_histogram(binwidth = base_bins(\"FD\")) +\n  facet_wrap(~sd, scales = \"free_x\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\n\n\nggsave()\ngglot2 中的ggsave()函数，其内部使用了一个function factories——plot_dev()。 其根据文件后缀判断图片类型，选择合适的绘图设备。下面是plot_dev()的简化示例：\n\nplot_dev &lt;- function(ext, dpi = 96) {\n  force(dpi)\n\n  switch(ext,\n    eps = ,\n    ps = function(path, ...) {\n      grDevices::postscript(\n        file = filename, ..., onefile = FALSE,\n        horizontal = FALSE, paper = \"special\"\n      )\n    },\n    pdf = function(filename, ...) grDevices::pdf(file = filename, ...),\n    svg = function(filename, ...) svglite::svglite(file = filename, ...),\n    emf = ,\n    wmf = function(...) grDevices::win.metafile(...),\n    png = function(...) grDevices::png(..., res = dpi, units = \"in\"),\n    jpg = ,\n    jpeg = function(...) grDevices::jpeg(..., res = dpi, units = \"in\"),\n    bmp = function(...) grDevices::bmp(..., res = dpi, units = \"in\"),\n    tiff = function(...) grDevices::tiff(..., res = dpi, units = \"in\"),\n    stop(\"Unknown graphics extension: \", ext, call. = FALSE)\n  )\n}\n\nplot_dev(\"pdf\")\n#&gt; function (filename, ...) \n#&gt; grDevices::pdf(file = filename, ...)\n#&gt; &lt;bytecode: 0x000001acc69877b0&gt;\n#&gt; &lt;environment: 0x000001acc7194650&gt;\nplot_dev(\"png\")\n#&gt; function (...) \n#&gt; grDevices::png(..., res = dpi, units = \"in\")\n#&gt; &lt;bytecode: 0x000001acc61ad698&gt;\n#&gt; &lt;environment: 0x000001acc4e1cc70&gt;",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html#statistical-factories",
    "href": "Books/Advanced R(2e)/10 Function factories.html#statistical-factories",
    "title": "10 Function factories",
    "section": "Statistical factories",
    "text": "Statistical factories\n本节介绍一些统计分析中运用到的function factories例子。\n\nBox-Cox transformation\nBox-Cox变换是一种常用的数据变换方法，用于处理非正态数据正态化。唯一参数是λ，用来控制转换强度。Box-Cox变换可以用下面的函数实现：\n\nboxcox1 &lt;- function(x, lambda) {\n  stopifnot(length(lambda) == 1)\n\n  if (lambda == 0) {\n    log(x)\n  } else {\n    (x^lambda - 1) / lambda\n  }\n}\n\n我们将上面的函数转换为一个function factories，用其探索不同λ对数据的影响。\nboxcox2 &lt;- function(lambda) {\n  if (lambda == 0) {\n    function(x) log(x)\n  } else {\n    function(x) (x^lambda - 1) / lambda\n  }\n}\n\nstat_boxcox &lt;- function(lambda) {\n  stat_function(aes(color = lambda), fun = boxcox2(lambda), linewidth = 1)\n}\n\nggplot(data.frame(x = c(0, 5)), aes(x)) +\n  lapply(c(0.5, 1, 1.5), stat_boxcox) +\n  scale_colour_viridis_c(limits = c(0, 1.5))\n\nggplot(data.frame(x = c(0.01, 1)), aes(x)) +\n  lapply(c(0.5, 0.25, 0.1, 0), stat_boxcox) +\n  scale_colour_viridis_c(limits = c(0, 1.5))\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap generators\n在统计分析工作中，我们经常需要多个Bootstrap生成器，这个时候构建一个Bootsrap生成器的function factories就显得很有必要。\n下面是一个对数据框中某列进行随机抽样的例子：\n\nboot_sample &lt;- function(df, var) {\n  n &lt;- nrow(df)\n  force(var)\n\n  function() {\n    col &lt;- df[[var]]\n    col[sample(n, replace = TRUE)]\n  }\n}\n\nboot_mtcars1 &lt;- boot_sample(mtcars, \"mpg\")\nhead(boot_mtcars1())\n#&gt; [1] 15.0 18.7 15.8 17.8 21.4 15.8\nhead(boot_mtcars1())\n#&gt; [1] 19.7 21.4 13.3 13.3 21.5 30.4\n\n有时我们需要构建一个根据模型生成随机样本的Bootstrap生成器。\n\nboot_model &lt;- function(df, formula) {\n  model &lt;- lm(formula, data = df)\n  fitted &lt;- unname(fitted(model))\n  resid &lt;- unname(resid(model))\n  rm(model)\n\n  function() {\n    fitted + sample(resid)\n  }\n}\n\nboot_mtcars2 &lt;- boot_model(mtcars, mpg ~ wt + disp)\nhead(boot_mtcars2())\n#&gt; [1] 22.06680 27.93947 23.24319 22.34347 23.40125 16.51648\nhead(boot_mtcars2())\n#&gt; [1] 25.11314 20.01861 23.17180 17.40872 14.46308 19.10514\n\n\n\nMaximum likelihood estimation\n最大似然估计（MLE）用来找到某个分布的参数，使得观测数据在该分布下出现概率最大。下面我们使用泊松分布来演示如何通过function factories优雅的实现最大似然估计。\n泊松分布的公式如下，当已知参数λ后，我们可以计算观察数据\\(X\\)的概率：\n\\[\nP(\\lambda,{\\bf x})=\\prod_{i=1}^{n}\\frac{\\lambda^{x_{i}}e^{-\\lambda}}{x_{i}!}\n\\]\n在统计中，我们通常对数化累乘公式，将其转换为累加：\n\n累加的计算量小\n累乘在计算机中会导致浮点数精确度下降\n\n\\[\n\\log(P(\\lambda,{\\bf x})) = \\sum_{i=1}^{n}\\log\\left(\\frac{\\lambda^{x_i}e^{-\\lambda}}{x_i!}\\right)\n\\]\n\\[\n\\log(P(\\lambda,{\\bf x})) = \\sum_{i=1}^{n}\\left(x_i\\log(\\lambda) - \\lambda - \\log(x_i!)\\right)\n\\]\n\\[\n\\log(P(\\lambda,{\\bf x})) = \\sum_{i=1}^{n}x_i\\log(\\lambda) - \\sum_{i=1}^{n}\\lambda - \\sum_{i=1}^{n}\\log(x_i!)\n\\]\n\\[\n\\log(P(\\lambda,{\\bf x})) = \\log(\\lambda)\\sum_{i=1}^{n}x_i - n\\lambda - \\sum_{i=1}^{n}\\log(x_i!)\n\\]\n构造已知λ后计算观察数据\\(X\\)概率的函数：\n\nmle_poisson &lt;- function(lambda, x) {\n  # x 相对是固定的，提前计算，可以节省计算资源\n  sum_x &lt;- sum(x)\n  n &lt;- length(x)\n  c &lt;- sum(lfactorial(x))\n  # log(lambda) * sum(x) - n * lambda - sum(lfactorial(x))\n  log(lambda) * sum_x - n * lambda - c\n}\n\n使用base R 计算观察数据\\(X\\)的概率：\n\nx1 &lt;- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38)\nmle_poisson(10, x1)\n#&gt; [1] -183.6405\nmle_poisson(20, x1)\n#&gt; [1] -61.14028\nmle_poisson(30, x1)\n#&gt; [1] -30.98598\n\n虽然我们可以计算已知λ下的观察数据\\(X\\)的概率，但是最大似然估计要求我们找到一个λ，使得该概率最大。我们可以构造一个观察数据\\(X\\)的function factories，并使用optimize()函数来寻找最大概率的λ。\n\nmle_poisson2 &lt;- function(x) {\n  force(x)\n  function(lambda) {\n    mle_poisson(lambda, x)\n  }\n}\n\noptimise(mle_poisson2(x1), interval = c(0, 100), maximum = TRUE)\n#&gt; $maximum\n#&gt; [1] 32.09999\n#&gt; \n#&gt; $objective\n#&gt; [1] -30.26755\n\n借助optimize()函数的...参数，我们可以直接使用下面的代码：\n\noptimise(mle_poisson, interval = c(0, 100), x = x1, maximum = TRUE)\n#&gt; $maximum\n#&gt; [1] 32.09999\n#&gt; \n#&gt; $objective\n#&gt; [1] -30.26755",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/10 Function factories.html#function-factories-functionals",
    "href": "Books/Advanced R(2e)/10 Function factories.html#function-factories-functionals",
    "title": "10 Function factories",
    "section": "Function factories + functionals",
    "text": "Function factories + functionals\n组合使用function factories和functionals，可以通过map系函数传递参数，构造出一个函数集合。如果你的function factories需要多个参数，就使用相应的map系函数。\n\nnames &lt;- list(\n  square = 2,\n  cube = 3,\n  root = 1 / 2,\n  cuberoot = 1 / 3,\n  reciprocal = -1\n)\nfuns &lt;- purrr::map(names, power1)\n\nfuns$root(64)\n#&gt; [1] 8\nfuns$root\n#&gt; function (x) \n#&gt; {\n#&gt;     x^exp\n#&gt; }\n#&gt; &lt;bytecode: 0x000001acc0c0a458&gt;\n#&gt; &lt;environment: 0x000001acc191bd60&gt;\n\n在上面的例子中，你需要使用funs$来提取函数，下面有三种方法可以直接使用funs内部的函数：\n\n使用with()，临时提取函数\n\n\nwith(funs, root(64))\n#&gt; [1] 8\n\n\n使用attach()，将函数绑定到当前环境\n\n\nattach(funs)\n#&gt; The following objects are masked _by_ .GlobalEnv:\n#&gt; \n#&gt;     cube, square\nroot(100)\n#&gt; [1] 10\ndetach(funs)\n\n\n使用rlang::env_bind()，将函数绑定到当前环境\n\n\nrlang::env_bind(globalenv(), !!!funs)\nroot(100)\n#&gt; [1] 10\nrlang::env_unbind(globalenv(), names(funs))",
    "crumbs": [
      "10 Function factories"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/12 Base types.html",
    "href": "Books/Advanced R(2e)/12 Base types.html",
    "title": "12 Base types",
    "section": "",
    "text": "在R中，流传着这么一句话——R里的一切都是对象。但此“对象”与面向对象（OOP）中的“对象”不同，前者指得是来自于S语言的base object，它比面向对象出现得更早。它们的关系可以表示为：\n\n下面我们介绍如何区分base object和OO object及所有base object的类别。",
    "crumbs": [
      "12 Base types"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/12 Base types.html#introduction",
    "href": "Books/Advanced R(2e)/12 Base types.html#introduction",
    "title": "12 Base types",
    "section": "",
    "text": "在R中，流传着这么一句话——R里的一切都是对象。但此“对象”与面向对象（OOP）中的“对象”不同，前者指得是来自于S语言的base object，它比面向对象出现得更早。它们的关系可以表示为：\n\n下面我们介绍如何区分base object和OO object及所有base object的类别。",
    "crumbs": [
      "12 Base types"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/12 Base types.html#base-vs-oo-objects",
    "href": "Books/Advanced R(2e)/12 Base types.html#base-vs-oo-objects",
    "title": "12 Base types",
    "section": "Base VS OO objects",
    "text": "Base VS OO objects\n区分二者的三种方法：\n\nis.object()：base object返回FALSE，OO object返回TRUE。\nsloop::otype()：base object返回base，OO object返回其他，如S3。\nattr()：base object返回NULL，OO object返回class属性。\n\n\nis.object(1:10)\n#&gt; [1] FALSE\nis.object(mtcars)\n#&gt; [1] TRUE\n\nsloop::otype(1:10)\n#&gt; [1] \"base\"\nsloop::otype(mtcars)\n#&gt; [1] \"S3\"\n\nattr(1:10, \"class\")\n#&gt; NULL\nattr(mtcars, \"class\")\n#&gt; [1] \"data.frame\"\n\n从技术上讲，base object 与 OO object 的本质区别就是OO object具有class属性。但是仅限于attr()函数，class()`函数会返回结果。\n\nx &lt;- 1:10\nclass(x)\n#&gt; [1] \"integer\"\nsloop::s3_class(x)\n#&gt; [1] \"integer\" \"numeric\"",
    "crumbs": [
      "12 Base types"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/12 Base types.html#base-types",
    "href": "Books/Advanced R(2e)/12 Base types.html#base-types",
    "title": "12 Base types",
    "section": "Base types",
    "text": "Base types\n无论是OO object还是base object，都有一个base type，使用typeof()来查看；不要使用mode()或storage.mode()，它们只适配S语言。\n\ntypeof(1:10)\n#&gt; [1] \"integer\"\n\ntypeof(mtcars)\n#&gt; [1] \"list\"\n\ntypeof(mean)\n#&gt; [1] \"closure\"\n\nR 的底层使用了C语言中的switch语句来对不同base type执行不同处理。想要新增一个base type需要修改R-core，所以通常不会随意增加base type。截至目前一共有25种base type，下面按照本书中出现的顺序列举。\n\nVector\n\n\n\nbase Type\nC Type\n\n\n\n\nNULL\nNILSXP\n\n\nlogical\nLGLSXP\n\n\ninteger\nINTSXP\n\n\ndouble\nREALSXP\n\n\ncomplex\nCPLXSXP\n\n\ncharacter\nSTRSXP\n\n\nlist\nVECSXP\n\n\nraw\nRAWSXP\n\n\n\n\ntypeof(NULL)\n#&gt; [1] \"NULL\"\ntypeof(1L)\n#&gt; [1] \"integer\"\ntypeof(1i)\n#&gt; [1] \"complex\"\n\n\n\nFunctions\n\n\n\nbase Type\nC Type\nfunction type\n\n\n\n\nclosure\nCLOSXP\nregular R functions\n\n\nspecial\nSPECIALSXP\ninternal functions\n\n\nbuiltin\nBUILTINSXP\nprimitive functions\n\n\n\n\ntypeof(mean)\n#&gt; [1] \"closure\"\ntypeof(`[`)\n#&gt; [1] \"special\"\ntypeof(sum)\n#&gt; [1] \"builtin\"\n\n\n\nEnvironments\n\n\n\nbase Type\nC Type\n\n\n\n\nenvironment\nENVSXP\n\n\n\n\ntypeof(globalenv())\n#&gt; [1] \"environment\"\n\n\n\nS4\n\n\n\nbase Type\nC Type\n\n\n\n\nS4\nS4SXP\n\n\n\n\nmle_obj &lt;- stats4::mle(function(x = 1) (x - 2)^2)\ntypeof(mle_obj)\n#&gt; [1] \"S4\"\n\n\n\nLanguage components\n\n\n\nbase Type\nC Type\n\n\n\n\nsymbol\nSYMSXP\n\n\nlanguage\nLANGSXP\n\n\npairlist\nLISTSXP\n\n\nexpression\nEXPRSXP\n\n\n\n\ntypeof(quote(a))\n#&gt; [1] \"symbol\"\ntypeof(quote(a + 1))\n#&gt; [1] \"language\"\ntypeof(formals(mean))\n#&gt; [1] \"pairlist\"\ntypeof(expression(1 + 0:9))\n#&gt; [1] \"expression\"\n\n\n\nOthers\n\n\n\nbase Type\nC Type\n\n\n\n\nexternalptr\nEXTPTRSXP\n\n\nweakref\nWEAKREFSXP\n\n\nbytecode\nBCODESXP\n\n\npromise\nPROMSXP\n\n\n...\nDOTSXP\n\n\nany\nANYSXP\n\n\n\n\n\nNumeric type\n“numeric”在R中存在三种解读：\n\n某些地方，numeric是double的别名。例如as.numeric()和as.double()等价，numeric()和double()等价。\n在S3和S4系统中，“numeric”用作”integer type”或”double type”的缩写：\n\n\nsloop::s3_class(1)\n#&gt; [1] \"double\"  \"numeric\"\nsloop::s3_class(1L)\n#&gt; [1] \"integer\" \"numeric\"\n\n\nis.numeric()用来检测那些行为类似number的对象。例如，因子(factor)的本质是”integer type”，但是没有number的行为（求取一个因子的均值毫无意义）。\n\n\ntypeof(factor(\"x\"))\n#&gt; [1] \"integer\"\nis.numeric(factor(\"x\"))\n#&gt; [1] FALSE\n\n本书中的numeric表示integer或double。",
    "crumbs": [
      "12 Base types"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html",
    "href": "Books/Advanced R(2e)/14 R6.html",
    "title": "14 R6",
    "section": "",
    "text": "本章介绍R6 OOP系统，它有两大特点：\n\nR6使用了封装的OOP范式，意味着方法（method）属于对象（object）而不是泛型函数（generic），调用方法的范式为object$method()。\nR6对象是可改变的，意味着它们可以原地修改并具有引用语义。当你将一个R6对象赋值给另一个变量时，实际上是将指向该R6对象的引用来赋值给新变量。这样，任何对该对象所做的更改都会反映在所有引用它的变量中。\n\n虽然R6 OOP系统与其他语言中的OOP范式相同，使用起来更容易上手，但它缺点就是不符合R的使用习惯，我们将在第16章中讨论它们。\n\n\n\n14.2节：介绍使用R6::R6Class()创建R6类，使用构造器$new()创建新的R6对象。\n14.3节：讨论R6的访问机制：私有域和主动域。\n14.4节：探讨R6的引用语义的影响。学习如何使用终结器自动清理初始化器中执行的任何操作，以及如何在另一个R6对象中将一个R6对象作为字段使用。\n14.5节：对比R6系统和RC系统。\n\n\n\n\n\n# install.packages(\"R6\")\nlibrary(R6)",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html#introduction",
    "href": "Books/Advanced R(2e)/14 R6.html#introduction",
    "title": "14 R6",
    "section": "",
    "text": "本章介绍R6 OOP系统，它有两大特点：\n\nR6使用了封装的OOP范式，意味着方法（method）属于对象（object）而不是泛型函数（generic），调用方法的范式为object$method()。\nR6对象是可改变的，意味着它们可以原地修改并具有引用语义。当你将一个R6对象赋值给另一个变量时，实际上是将指向该R6对象的引用来赋值给新变量。这样，任何对该对象所做的更改都会反映在所有引用它的变量中。\n\n虽然R6 OOP系统与其他语言中的OOP范式相同，使用起来更容易上手，但它缺点就是不符合R的使用习惯，我们将在第16章中讨论它们。\n\n\n\n14.2节：介绍使用R6::R6Class()创建R6类，使用构造器$new()创建新的R6对象。\n14.3节：讨论R6的访问机制：私有域和主动域。\n14.4节：探讨R6的引用语义的影响。学习如何使用终结器自动清理初始化器中执行的任何操作，以及如何在另一个R6对象中将一个R6对象作为字段使用。\n14.5节：对比R6系统和RC系统。\n\n\n\n\n\n# install.packages(\"R6\")\nlibrary(R6)",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html#classes-and-methods",
    "href": "Books/Advanced R(2e)/14 R6.html#classes-and-methods",
    "title": "14 R6",
    "section": "Classes and methods",
    "text": "Classes and methods\nR6::R6Class()函数可以同时构建类（class）和方法（method），同时也是R6包中唯一需要使用的函数。\nR6Class()函数有两个极其重要的参数：\n\nclassname：类名，它不是必须的，但它改进了错误消息，并使得R6对象可以与S3类的泛型函数结合使用。R6 class名称通常使用UpperCamelCase命名法。\npublic：一个列表，包含类的属性（field）和方法，可以通过self$的方法获取。属性和方法通常使用snake_case命名法。\n\n\nAccumulator &lt;- R6Class(\n  classname = \"Accumulator\",\n  public = list(\n    sum = 0,\n    add = function(x = 1) {\n      self$sum &lt;- self$sum + x\n      invisible(self)\n    }\n  )\n)\n\n在使用R6Class()创建对象时，需要始终将创建的结果赋值给与类名相同的变量。\n\nAccumulator\n#&gt; &lt;Accumulator&gt; object generator\n#&gt;   Public:\n#&gt;     sum: 0\n#&gt;     add: function (x = 1) \n#&gt;     clone: function (deep = FALSE) \n#&gt;   Parent env: &lt;environment: R_GlobalEnv&gt;\n#&gt;   Locked objects: TRUE\n#&gt;   Locked class: FALSE\n#&gt;   Portable: TRUE\n\n可以使用object$new()的方法创建新对象。\n\nx &lt;- Accumulator$new()\n\n同样地，使用$获取对的属性和方法。\n\nx$add(4)\nx$sum\n#&gt; [1] 4\n\n后续我们以()区分$获取的时属性还是方法——$add()表示方法，$sum表示属性。\n\nMethod chaining\n当$add()方法返回的是self而不是$sum时，我们就可以使用方法链（method chaining），类似管道符。通常我们使用return()来返回，但鉴于self的隐私性，这里使用invisible()。\n\nx$add(10)$add(10)$sum\n#&gt; [1] 24\n\nx$\n  add(10)$\n  add(10)$\n  sum\n#&gt; [1] 44\n\n\n\nImportant methods\n对大多数R6对象，有两个重要的方法需要定义——$initialize()和$print()。它们非必须，但会提升对象的使用性。\n$initialize()方法会覆盖默认的$new()方法。例如下面的“Person”类，我在$initialize()方法中判断了$name属性只能是单一的字符串，$age属性只能是单一的数字。如果你有更多对输入的检查，将它们放在$validate()方法中更合适。\n\nPerson &lt;- R6Class(\"Person\", list(\n  name = NULL,\n  age = NA,\n  initialize = function(name, age = NA) {\n    stopifnot(is.character(name), length(name) == 1)\n    stopifnot(is.numeric(age), length(age) == 1)\n\n    self$name &lt;- name\n    self$age &lt;- age\n  }\n))\n\nhadley &lt;- Person$new(\"Hadley\", age = \"thirty-eight\")\n#&gt; Error in initialize(...): is.numeric(age) is not TRUE\n\nhadley &lt;- Person$new(\"Hadley\", age = 38)\n\n$print()方法会覆盖默认的print()方法，允许你自定义对象的打印输出。和其他R6对象的方法一样，最终使用invisible()来返回。\n\nPerson &lt;- R6Class(\"Person\", list(\n  name = NULL,\n  age = NA,\n  initialize = function(name, age = NA) {\n    self$name &lt;- name\n    self$age &lt;- age\n  },\n  print = function(...) {\n    cat(\"Person: \\n\")\n    cat(\"  Name: \", self$name, \"\\n\", sep = \"\")\n    cat(\"  Age:  \", self$age, \"\\n\", sep = \"\")\n    invisible(self)\n  }\n))\n\nhadley2 &lt;- Person$new(\"Hadley\")\nhadley2\n#&gt; Person: \n#&gt;   Name: Hadley\n#&gt;   Age:  NA\n\n\n\nAdding methods after creation\n可以使用$set()修改R6对象的属性和方法。\n\nAccumulator &lt;- R6Class(\"Accumulator\")\nAccumulator$set(\"public\", \"sum\", 0)\nAccumulator$set(\"public\", \"add\", function(x = 1) {\n  self$sum &lt;- self$sum + x\n  invisible(self)\n})\n\n需要注意：对象添加新的属性和方法后，只有用它创建新的对象时才会添加，已经创建好的对象不会添加新的属性和方法。\n\n\nInheritance\n参数inherit允许创建继承关系。\n\nAccumulatorChatty &lt;- R6Class(\n  \"AccumulatorChatty\",\n  inherit = Accumulator,\n  public = list(\n    add = function(x = 1) {\n      cat(\"Adding \", x, \"\\n\", sep = \"\")\n      super$add(x = x)\n    }\n  )\n)\n\nx2 &lt;- AccumulatorChatty$new()\nx2$add(10)$add(1)$sum\n#&gt; Adding 10\n#&gt; Adding 1\n#&gt; [1] 11\n\n拥有继承关系的子类可以使用父类的方法，但时如何名称相同发生覆盖，则需要使用suppe$来方法父类方法，这与上一章中的NextMethod()函数类似。\n\n\nIntrospection\n每一个R6对象中都含有一个S3类。这意味着我们可以对R6对象使用一些S3类常用的函数，上述提到的$print()方法，本质上是print.R6()函数。\nclass()可以确定是否属于R6类。\n\nclass(hadley2)\n#&gt; [1] \"Person\" \"R6\"\n\nnames()可以查看R6类的所有属性和方法名。下面的.__enclos_env__是R6内部的实现细节（R6 = S3 + env）。\n\nnames(hadley2)\n#&gt; [1] \".__enclos_env__\" \"age\"             \"name\"            \"clone\"          \n#&gt; [5] \"print\"           \"initialize\"\n\n\n\nExercises\n\nCreate a bank account R6 class that stores a balance and allows you to deposit and withdraw money. Create a subclass that throws an error if you attempt to go into overdraft. Create another subclass that allows you to go into overdraft, but charges you a fee.\n\n\n\nsolution\nBank &lt;- R6Class(\"Bank\", list(\n  name = \"\",\n  balance = 0,\n  initialize = function(name, balance = 0) {\n    stopifnot(is.character(name), length(name) == 1)\n    stopifnot(is.numeric(balance), length(balance) == 1)\n\n    self$name &lt;- name\n    self$balance &lt;- balance\n  },\n  print = function(...) {\n    cat(\"Bank: \\n\")\n    cat(\"  Name: \", self$name, \"\\n\", sep = \"\")\n    cat(\"  Balance:  \", self$balance, \"\\n\", sep = \"\")\n    invisible(self)\n  },\n  deposit = function(x) {\n    self$balance &lt;- self$balance + x\n    invisible(self)\n  },\n  withdraw = function(x) {\n    self$balance &lt;- self$balance - x\n    invisible(self)\n  }\n))\n\na &lt;- Bank$new(name = \"a\", balance = 1000)\na$deposit(500)$withdraw(2000)\na\n#&gt; Bank: \n#&gt;   Name: a\n#&gt;   Balance:  -500\n\nBank2 &lt;- R6Class(\"Bank2\", inherit = Bank, public = list(\n  withdraw = function(x) {\n    if (self$balance - x &lt; 0) {\n      stop(\"Insufficient funds\", call. = FALSE)\n    }\n  }\n))\n\nb &lt;- Bank2$new(name = \"b\", balance = 1000)\nb$deposit(500)$withdraw(2000)\n#&gt; Error: Insufficient funds\nb\n#&gt; Bank: \n#&gt;   Name: b\n#&gt;   Balance:  1500\n\nBank3 &lt;- R6Class(\"Bank3\", inherit = Bank, public = list(\n  withdraw = function(x) {\n    if (self$balance - x &lt; 0) {\n      message(\"charge of $5 applied\")\n      self$balance &lt;- self$balance - x - 5\n    }\n  }\n))\n\nc &lt;- Bank3$new(name = \"c\", balance = 1000)\nc$deposit(500)$withdraw(2000)\n#&gt; charge of $5 applied\nc\n#&gt; Bank: \n#&gt;   Name: c\n#&gt;   Balance:  -505\n\n\n\nCreate an R6 class that represents a shuffled deck of cards. You should be able to draw cards from the deck with $draw(n), and return all cards to the deck and reshuffle with $reshuffle(). Use the following code to make a vector of cards.\n\n\nsuit &lt;- c(\"♠\", \"♥\", \"♦\", \"♣\")\nvalue &lt;- c(\"A\", 2:10, \"J\", \"Q\", \"K\")\ncards &lt;- paste0(rep(value, 4), suit)\n\n\n\nsolution\nShuffledDeck &lt;- R6Class(\n  classname = \"ShuffledDeck\",\n  public = list(\n    deck = NULL,\n    initialize = function(deck = cards) {\n      self$deck &lt;- sample(deck)\n    },\n    reshuffle = function() {\n      self$deck &lt;- sample(cards)\n      invisible(self)\n    },\n    n = function() {\n      length(self$deck)\n    },\n    draw = function(n = 1) {\n      if (n &gt; self$n()) {\n        stop(\"Only \", self$n(), \" cards remaining.\", call. = FALSE)\n      }\n\n      output &lt;- self$deck[seq_len(n)]\n      self$deck &lt;- self$deck[-seq_len(n)]\n      output\n    }\n  )\n)\n\nmy_deck &lt;- ShuffledDeck$new()\nmy_deck$draw(52)\n#&gt;  [1] \"Q♦\"  \"5♥\"  \"K♦\"  \"6♣\"  \"K♠\"  \"2♠\"  \"Q♥\"  \"J♣\"  \"9♣\"  \"J♦\"  \"6♦\"  \"5♠\" \n#&gt; [13] \"K♣\"  \"2♦\"  \"8♥\"  \"A♥\"  \"10♠\" \"4♦\"  \"10♥\" \"7♦\"  \"9♦\"  \"3♠\"  \"3♥\"  \"7♥\" \n#&gt; [25] \"K♥\"  \"5♦\"  \"7♣\"  \"8♠\"  \"A♣\"  \"10♣\" \"9♠\"  \"6♥\"  \"J♠\"  \"2♥\"  \"9♥\"  \"A♦\" \n#&gt; [37] \"8♣\"  \"A♠\"  \"3♦\"  \"8♦\"  \"Q♠\"  \"4♠\"  \"4♣\"  \"3♣\"  \"4♥\"  \"6♠\"  \"J♥\"  \"Q♣\" \n#&gt; [49] \"5♣\"  \"7♠\"  \"10♦\" \"2♣\"\nmy_deck$draw(10)\n#&gt; Error: Only 0 cards remaining.\nmy_deck$reshuffle()$draw(5)\n#&gt; [1] \"10♣\" \"5♦\"  \"8♥\"  \"K♦\"  \"2♠\"\nmy_deck$reshuffle()$draw(5)\n#&gt; [1] \"K♦\" \"4♥\" \"9♣\" \"J♠\" \"K♥\"",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html#controlling-access",
    "href": "Books/Advanced R(2e)/14 R6.html#controlling-access",
    "title": "14 R6",
    "section": "Controlling access",
    "text": "Controlling access\nR6Class()函数有两个与public参数类似的参数：\n\nprivate：创建的R6对象的私有属性和方法，只允许对象内部访问。\nactive：创建的R6对象的动态属性，通过 accessor 函数访问。\n\n\nPrivacy\nprivate参数创建的私有属性和方法有两个特点：\n\n创建方式与public参数一样，都是一个带有name的list。\n在对象内部调用时，需要使用private$前缀。\n\n下面是一个私有属性示例：\n\nPerson &lt;- R6Class(\"Person\",\n  public = list(\n    initialize = function(name, age = NA) {\n      private$name &lt;- name\n      private$age &lt;- age\n    },\n    print = function(...) {\n      cat(\"Person: \\n\")\n      cat(\"  Name: \", private$name, \"\\n\", sep = \"\")\n      cat(\"  Age:  \", private$age, \"\\n\", sep = \"\")\n    }\n  ),\n  private = list(\n    age = NA,\n    name = NULL\n  )\n)\n\nhadley3 &lt;- Person$new(\"Hadley\")\nhadley3\n#&gt; Person: \n#&gt;   Name: Hadley\n#&gt;   Age:  NA\nhadley3$name\n#&gt; NULL\n\n相交于其他语言，私有方法在R语言中通常不是很重要。\n\n\nActive fields\n动态属性看起来像是公共属性，但实际是由一个active binding函数定义。active binding函数只有一个参数value，如果参数missing(), 则检索该值；否则，将对其进行修改。\n下例定义了动态属性random，每次访问时，会返回一个随机数。\n\nRando &lt;- R6::R6Class(\"Rando\", active = list(\n  random = function(value) {\n    if (missing(value)) {\n      runif(1)\n    } else {\n      stop(\"Can't set `$random`\", call. = FALSE)\n    }\n  }\n))\nx &lt;- Rando$new()\nx$random(3)\n#&gt; Error: attempt to apply non-function\nx$random\n#&gt; [1] 0.197574\nx$random &lt;- 31\n#&gt; Error: Can't set `$random`\n\n动态属性可以使静态属性看起来像公共属性。例如下例中，我们创建了只读的属性age和能确保字符串长度为1的属性name。\n\nPerson &lt;- R6Class(\"Person\",\n  private = list(\n    .age = NA,\n    .name = NULL\n  ),\n  active = list(\n    age = function(value) {\n      if (missing(value)) {\n        private$.age\n      } else {\n        stop(\"`$age` is read only\", call. = FALSE)\n      }\n    },\n    name = function(value) {\n      if (missing(value)) {\n        private$.name\n      } else {\n        stopifnot(is.character(value), length(value) == 1)\n        private$.name &lt;- value\n        self\n      }\n    }\n  ),\n  public = list(\n    initialize = function(name, age = NA) {\n      private$.name &lt;- name\n      private$.age &lt;- age\n    }\n  )\n)\n\nhadley4 &lt;- Person$new(\"Hadley\", age = 38)\nhadley4$name\n#&gt; [1] \"Hadley\"\nhadley4$name &lt;- \"Hadley2\"\nhadley4$name &lt;- 10\n#&gt; Error in (function (value) : is.character(value) is not TRUE\nhadley4$age &lt;- 20\n#&gt; Error: `$age` is read only\n\n子类无法访问到父类的私有属性，但是可以访问到父类的私有方法：\n\nA &lt;- R6Class(\n  classname = \"A\",\n  private = list(\n    field = \"foo\",\n    method = function() {\n      \"bar\"\n    }\n  )\n)\n\nB &lt;- R6Class(\n  classname = \"B\",\n  inherit = A,\n  public = list(\n    test = function() {\n      cat(\"Field:  \", super$field, \"\\n\", sep = \"\")\n      cat(\"Method: \", super$method(), \"\\n\", sep = \"\")\n    }\n  )\n)\n\nB$new()$test()\n#&gt; Field:  \n#&gt; Method: bar",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html#reference-semantics",
    "href": "Books/Advanced R(2e)/14 R6.html#reference-semantics",
    "title": "14 R6",
    "section": "Reference semantics",
    "text": "Reference semantics\nR6 OOP系统与其他系统的最大不同就是它的引用语义。引用语义意味着对象被修改时不会被复制。\n\ny1 &lt;- Accumulator$new()\ny2 &lt;- y1\n\ny1$add(10)\nc(y1 = y1$sum, y2 = y2$sum)\n#&gt; y1 y2 \n#&gt; 10 10\n\n如果你想要复制对象，你需要使用$clone()方法，添加参数deep = TRUE可以克隆嵌套的对象。\n\ny1 &lt;- Accumulator$new()\ny2 &lt;- y1$clone()\n\ny1$add(10)\nc(y1 = y1$sum, y2 = y2$sum)\n#&gt; y1 y2 \n#&gt; 10  0\n\n引用语义的使用同样会带来其他结果：\n\n需要更多的上下文才能理解R6对象。\n考虑何时删除 R6对象是有意义的，你可以编写$finalize()来补充$initialize()，。\n如果某个属性是R6对象，则必须在$initialize()中创建它，而不是在R6Class()中。\n\n\nReasoning\n通常，参考语义会导致代码更难推理。考虑下面的例子：\nx &lt;- list(a = 1)\ny &lt;- list(b = 2)\n\nz &lt;- f(x, y)\n因为函数f内部无法修改外部的x,y，所以我们知道函数f只修改了z。\n但是想象x,y是一个R6对象：\nx &lt;- List$new(a = 1)\ny &lt;- List$new(b = 2)\n\nz &lt;- f(x, y)\n函数f内部可以调用x和y内部的属性或方法，并对它们进行修改。我们无法仅从z &lt;- f(x, y)判断函数f是否修改了x,y，我们需要查看函数f内部的代码。\n\n\nFinalizer\n因为R6对象具有引用语义，所以删除一次就会完全删除对象（不发生修改即拷贝）。这意味着我们可以在R6对象被删除时，使用$finalize()执行某些清理工作（类似on.exit()），来补充$initialize()。如下例中，我们实例化一个创建临时文件对象，然后删除该实例，就会删除临时文件。\n\nTemporaryFile &lt;- R6Class(\n  \"TemporaryFile\",\n  public = list(\n    path = NULL,\n    initialize = function() {\n      self$path &lt;- tempfile()\n    }\n  ),\n  private = list(\n    finalize = function() {\n      message(\"Cleaning up \", self$path)\n      unlink(self$path)\n    }\n  )\n)\n\ntf &lt;- TemporaryFile$new()\nrm(tf)\ngc() # 使用gc()才会触发，书中好像是rm(tf)就会触发。\n#&gt;           used (Mb) gc trigger  (Mb) max used  (Mb)\n#&gt; Ncells  949190 50.7    1892770 101.1  1892770 101.1\n#&gt; Vcells 2058420 15.8    8388608  64.0  3451412  26.4\n\n\n\nR6 fields\n当使用R6类作为另外一个R6类的属性时，必须在$initialize方法中初始化属性。因为在外部定义的属性，表示该属性在定义R6类时已经创建，后续的所有实例都会继承这个属性。例如下面案例：我们想每次创建临时数据库时都创建一个临时文件，如果在外部定义属性file，实例db_a和db_b都会继承这个属性，这样db_a和db_b的属性file都指向同一个文件。\n\nTemporaryDatabase &lt;- R6Class(\n  \"TemporaryDatabase\",\n  public = list(\n    con = NULL,\n    file = TemporaryFile$new(),\n    initialize = function() {\n      self$con &lt;- DBI::dbConnect(RSQLite::SQLite(), path = file$path)\n    }\n  ),\n  private = list(\n    finalize = function() {\n      DBI::dbDisconnect(self$con)\n    }\n  )\n)\n\ndb_a &lt;- TemporaryDatabase$new()\ndb_b &lt;- TemporaryDatabase$new()\n\ndb_a$file$path == db_b$file$path\n#&gt; [1] TRUE\n\n相反，使用$initialize()方法，在创建实例时，始终会重新创建属性file。\n\nTemporaryDatabase &lt;- R6Class(\n  \"TemporaryDatabase\",\n  public = list(\n    con = NULL,\n    file = NULL,\n    initialize = function() {\n      self$file &lt;- TemporaryFile$new()\n      self$con &lt;- DBI::dbConnect(RSQLite::SQLite(), path = file$path)\n    }\n  ),\n  private = list(\n    finalize = function() {\n      DBI::dbDisconnect(self$con)\n    }\n  )\n)\n\ndb_a &lt;- TemporaryDatabase$new()\ndb_b &lt;- TemporaryDatabase$new()\n\ndb_a$file$path == db_b$file$path\n#&gt; [1] FALSE",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/14 R6.html#why-r6",
    "href": "Books/Advanced R(2e)/14 R6.html#why-r6",
    "title": "14 R6",
    "section": "Why R6?",
    "text": "Why R6?\nR6 OOP系统相较于 RC OOP系统的一些优势：\n\nR6 更简单。R6 基于S3，RC 基于S4。\nR6 有全面的文档。https://r6.r-lib.org/\nR6 提供了一种更简单的跨包子类化机制，这种机制无需思考就能正常工作。\nR6 对属性方法的管理更加明确。\nR6 更快。\nRC 与 base R 绑定，意味着你需要修改不同R版本的bug。",
    "crumbs": [
      "14 R6"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/16 Trade-offs.html",
    "href": "Books/Advanced R(2e)/16 Trade-offs.html",
    "title": "16 Trade-offs",
    "section": "",
    "text": "前面我们对R中的三种OOP系统进行了系统介绍，了解了它们的基本使用方法，现在我们将三种系统进行对比，进一步了解它们各自的优劣势。这对我们在处理问题选择何种系统时会有所帮助。\n总的来说，作者推荐S3系统，因为它足够简单，并且广泛应用在base R和CRAN中；虽然它不是完善的，但是有一些处理方法可以避免。并不推荐总是使用R6系统。\n\n\n\n16.2节：S3与S4系统进行对比。简而言之，S4更加正式，往往需要更多的前期规划。这使得它更适合由团队而非个人开发的大型项目。\n16.3节：S3与R6系统进行对比。这一部分相当长，因为这两个系统在根本上是不同的，你需要考虑一些权衡。",
    "crumbs": [
      "16 Trade-offs"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/16 Trade-offs.html#introduction",
    "href": "Books/Advanced R(2e)/16 Trade-offs.html#introduction",
    "title": "16 Trade-offs",
    "section": "",
    "text": "前面我们对R中的三种OOP系统进行了系统介绍，了解了它们的基本使用方法，现在我们将三种系统进行对比，进一步了解它们各自的优劣势。这对我们在处理问题选择何种系统时会有所帮助。\n总的来说，作者推荐S3系统，因为它足够简单，并且广泛应用在base R和CRAN中；虽然它不是完善的，但是有一些处理方法可以避免。并不推荐总是使用R6系统。\n\n\n\n16.2节：S3与S4系统进行对比。简而言之，S4更加正式，往往需要更多的前期规划。这使得它更适合由团队而非个人开发的大型项目。\n16.3节：S3与R6系统进行对比。这一部分相当长，因为这两个系统在根本上是不同的，你需要考虑一些权衡。",
    "crumbs": [
      "16 Trade-offs"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/16 Trade-offs.html#s4-versus-s3",
    "href": "Books/Advanced R(2e)/16 Trade-offs.html#s4-versus-s3",
    "title": "16 Trade-offs",
    "section": "S4 versus S3",
    "text": "S4 versus S3\n一旦你熟悉使用S3系统，S4系统也就水到渠成，因为底层思想是一致的，只是S4系统更加正式严格，这使得S4系统更适合大型项目。因为S4系统提供了许多工具，如构造函数，验证函数等，我们无需为此过多耗神，而是将更多精力花在“绘制蓝图”上。\nBioconductor是一个使用S4系统取得良好效果的重要项目，其中很多包使用的数据结果，如SummarizedExperiment、IRanges、DNAStringSet，都是使用S4系统开发的。\n通过仔细地使用“methods”，在面对有关联地复杂系统时，我们可以最小程度地减少代码复用（继承）。例如“Matrix”包，它旨在高效地存储和计算多种不同类型的稀疏和密集矩阵。截至1.7.2版本，它定义了108个类、23个泛型函数和1780个方法。下面是它类图的一个小子集。\n\n矩阵计算这个领域非常适合使用S4, 因为稀疏矩阵的特定组合通常有计算捷径。S4 使得提供一个适用于所有输入的通用方法变得容易，然后在输入允许更高效实现的情况下提供更专业的方法。这需要仔细规划，以避免方法调度的模糊性，但这种规划会带来更高的性能。\n使用S4的最大挑战是日益增加的复杂性和缺乏单一文档来源的结合。S4是一个复杂的系统，在实践中有效使用可能具有挑战性。如果S4文档没有分散在R文档、书籍和网站中，这就不会是一个大问题。S4需要一本书的长度，但这本书 (目前) 还不存在。(S3 的文档并不更好，但缺乏这本书的痛苦较小，因为S3要简单得多。)",
    "crumbs": [
      "16 Trade-offs"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/16 Trade-offs.html#r6-versus-s3",
    "href": "Books/Advanced R(2e)/16 Trade-offs.html#r6-versus-s3",
    "title": "16 Trade-offs",
    "section": "R6 versus S3",
    "text": "R6 versus S3\nR6是一个与S3和S4截然不同的面向对象系统，因为它建立在封装对象之上，而不是泛型函数。此外，R6对象具有引用语义，这意味着它们可以在原位修改。这两个重大差异带来了许多不明显的后果，我们将在这里探讨。\n\n泛型函数是一个常规函数，因此它存在于全局命名空间中。R6方法属于对象，因此它存在于局部命名空间中。这影响了我们思考命名的方式。\nR6的引用语义允许方法同时返回值和修改对象。这解决了一个被称为 “线程状态” 的棘手问题。\nR6使用$调用方法，$是一个“infix”运算符。如果你正确地设置了你的方法，你可以使用方法调用链作为管道的替代方案。\n\n这些是泛函性OOP和封装OOP之间的一般权衡，因此它们也可以作为R与Python中系统设计的讨论。\n\nNamespacing\nS3和R6之间一个不明显的区别是方法所在的空间：\n\n泛型函数是全局的：所有包共享相同的命名空间。\n封装方法是本地的：方法绑定到单个对象。\n\n全局命名空间的优势在于，多个包可以使用相同的“动词”来处理不同类型的对象。泛型函数提供了统一的API, 这使得对新对象执行典型操作变得更容易，因为存在强大的命名约定。这在数据分析中效果很好，因为你经常想对不同类型的对象做相同的事情。特别是，这是R的建模系统如此有用的一个原因：无论模型在哪里实现，你总是可以使用相同的工具集 (summary ()、predict () 等) 来处理它。\n全局命名空间的缺点是它迫使你对命名进行更深入的思考。你需要避免在不同的包中使用多个名称相同的泛型，因为这需要用户频繁地输入::。这可能很困难，因为函数名称通常是英语动词，而动词通常具有多重含义。以 plot()为例：\nplot(data)       # plot some data\nplot(bank_heist) # plot a crime\nplot(land)       # create a new plot of land\nplot(movie)      # extract plot of a movie\n通常，你应该避免使用与原始泛型函数名同义的方法，而是应该为这些方法定义一个新的泛型。\nR6 方法不会出现这个问题，因为它们的作用域是对象。以下代码没有问题，因为这并不意味着两个不同R6对象的plot方法具有相同的含义：\ndata$plot()\nbank_heist$plot()\nland$plot()\nmovie$plot()\n这些考虑也适用于泛型函数的参数。S3泛型必须具有相同的核心参数，这意味着它们通常具有非特定的名称，如x或.data。S3泛型通常需要...向方法传递额外的参数，但这有一个缺点，即拼写错误的参数名称不会产生错误。相比之下，R6 方法可以有更广泛的变化，并使用更具体和更具启发性的参数名称。\n本地命名空间的一个次要优势是创建R6方法非常“廉价”。大多数封装的面向对象语言都鼓励你创建许多小方法，每个方法都有一个引人注目的名称。创建一个新的 S3 方法成本更高，因为你可能还需要创建一个泛型，并考虑上述命名问题。这意味着创建许多小方法的建议不适用于 S3。将代码分解成易于理解的小块仍然是一个好主意，但它们通常应该只是普通函数，而不是方法。\n\n\nThreading state\n使用S3编程的一个挑战是，当你想要同时返回值和修改对象时，会违反我们指导原则，即函数被调用时或返回值或执行副作用，但在少数情况下同时执行又是必要的。\n例如，假设你想创建一个“栈”对象，它有两个主要方法：\n\npush()：在栈顶添加一个新对象。\npop()：返回栈定处的值，然后从栈中删除它。\n\n栈对象的构造函数和push()方法的实现很简单。栈包含一个项目列表；将一个对象推送到栈只是简单地添加到这个列表中。\n\nnew_stack &lt;- function(items = list()) {\n  structure(list(items = items), class = \"stack\")\n}\n\npush &lt;- function(x, y) {\n  x$items &lt;- c(x$items, list(y))\n  x\n}\n\n实现pop()方法更具挑战性，因为它必须同时返回一个值 (栈顶部的对象) 和一个副作用 (从顶部移除该对象)。由于无法在S3中修改输入对象，我们需要返回两样东西：值和更新后的对象。\n\npop &lt;- function(x) {\n  n &lt;- length(x$items)\n\n  item &lt;- x$items[[n]]\n  x$items &lt;- x$items[-n]\n\n  list(item = item, x = x)\n}\n\n这会导致使用方式相当尴尬：\n\ns &lt;- new_stack()\ns &lt;- push(s, 10)\ns &lt;- push(s, 20)\n\nout &lt;- pop(s)\nout$item\n#&gt; [1] 20\ns &lt;- out$x\n\n这个问题被称为线程化状态（threading state）或累加器编程（accumulator programming），因为无论调用pop()多深，你都必须将修改后的栈对象一直线程化到它所在的位置。\n其他FP语言应对这一挑战的一种方式是提供多重赋值（multiple assign）或解构绑定运算符，允许你在一个步骤中赋值多个值。“zeallot”包为R提供了%&lt;-%的多重赋值功能。这使代码更加优雅，但并没有解决关键问题：\n\nlibrary(zeallot)\n\nc(value, s) %&lt;-% pop(s)\nvalue\n#&gt; [1] 10\n\n堆栈的R6实现更简单，因为$pop()可以就地修改对象，并且只返回最上面的值：\n\nStack &lt;- R6::R6Class(\"Stack\", list(\n  items = list(),\n  push = function(x) {\n    self$items &lt;- c(self$items, x)\n    invisible(self)\n  },\n  pop = function() {\n    item &lt;- self$items[[self$length()]]\n    self$items &lt;- self$items[-self$length()]\n    item\n  },\n  length = function() {\n    length(self$items)\n  }\n))\n\n这种方式的代码更加自然：\n\ns &lt;- Stack$new()\ns$push(10)\ns$push(20)\ns$pop()\n#&gt; [1] 20\n\n作者在“ggplot2”包种的scales中遇到了一个真实的线程状态案例。scales很复杂，因为它们需要跨每个分面和每个层组合数据。作者最初使用S3类，但这需要在许多函数之间传递scales数据。切换到R6后，代码变得简单了许多。然而，它也带来了一些问题，因为作者在修改图表时忘记调用$clone()。这使得独立的图表可以共享相同的比例数据，从而产生了一个难以追踪的微妙bug。",
    "crumbs": [
      "16 Trade-offs"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/16 Trade-offs.html#method-chaining",
    "href": "Books/Advanced R(2e)/16 Trade-offs.html#method-chaining",
    "title": "16 Trade-offs",
    "section": "Method chaining",
    "text": "Method chaining\n管道符%&gt;%很有用，因为它提供了一个“infix”运算符，使得从左到右组合函数变得容易。有趣的是，管道对R6对象来说并不那么重要，因为它们已经使用了一个“infix”运算符：$。这允许用户在单个表达式中链接多个方法调用，这种技术被称为方法链（method chaining）。\n\ns &lt;- Stack$new()\ns$\n  push(10)$\n  push(20)$\n  pop()\n#&gt; [1] 20\n\n这种技术通常用于其他编程语言，如Python和JavaScript, 并且是通过一个约定实现的：任何主要因其副作用 (通常是修改对象) 而调用的R6方法都应该返回invisible(self)。\n方法链的主要优点是可以获得有用的自动补全；主要缺点是只有类的创建者才能添加新的方法 (而且没有办法使用多分派)。",
    "crumbs": [
      "16 Trade-offs"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html",
    "href": "Books/Advanced R(2e)/18 Expressions.html",
    "title": "18 Expressions",
    "section": "",
    "text": "想要理解“expression”的结构并运用它，我们需要学习一些新的概念，新的工具以及新的思考方式。\n首先就是在代码的执行和结果上有着显著差异，例如，当我们没有定义x时，计算y &lt;- x * 10会报错：\n\ny &lt;- x * 10\n#&gt; Error: object 'x' not found\n\n但是如果我们能够将“要执行的代码”和“执行的过程”分开，那么问题就迎刃而解了。例如使用rlang::expr()：\n\nz &lt;- rlang::expr(y &lt;- x * 10)\nz\n#&gt; y &lt;- x * 10\n\nexpr()返回一个“表达式”，这个表达式对象包含了“代码”，但是没有“执行”。我们可以使用base::eval()来执行这个“表达式”：\n\nx &lt;- 5\neval(z)\ny\n#&gt; [1] 50\n\n这样就将两个过程分离开来，本章的主要内容就是“表达式”这类对象的数据结构。掌握了这些内容，你就可以查看和修改“表达式”，也即修改了代码或者生成了代码。\n\n\n\n18.2节：介绍语法抽象树（Abstract Syntax Tree，AST）思想，它是所有R代码的底层数据结构。\n18.3节：深入了解AST，构成“表达式”的数据类型：constants，symbol，call，pairlist，missing argument。\n18.4节：介绍解析字符串为表达式，并探讨R中的语法。\n18.5节：介绍如何使用递归函数来处理“language”对象和“表达式”。\n18.6节：介绍三种特殊的数据类型：pairlist，missing argument，expression vector。\n\n\n\n\n\nlibrary(rlang)\nlibrary(lobstr)",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#introduction",
    "href": "Books/Advanced R(2e)/18 Expressions.html#introduction",
    "title": "18 Expressions",
    "section": "",
    "text": "想要理解“expression”的结构并运用它，我们需要学习一些新的概念，新的工具以及新的思考方式。\n首先就是在代码的执行和结果上有着显著差异，例如，当我们没有定义x时，计算y &lt;- x * 10会报错：\n\ny &lt;- x * 10\n#&gt; Error: object 'x' not found\n\n但是如果我们能够将“要执行的代码”和“执行的过程”分开，那么问题就迎刃而解了。例如使用rlang::expr()：\n\nz &lt;- rlang::expr(y &lt;- x * 10)\nz\n#&gt; y &lt;- x * 10\n\nexpr()返回一个“表达式”，这个表达式对象包含了“代码”，但是没有“执行”。我们可以使用base::eval()来执行这个“表达式”：\n\nx &lt;- 5\neval(z)\ny\n#&gt; [1] 50\n\n这样就将两个过程分离开来，本章的主要内容就是“表达式”这类对象的数据结构。掌握了这些内容，你就可以查看和修改“表达式”，也即修改了代码或者生成了代码。\n\n\n\n18.2节：介绍语法抽象树（Abstract Syntax Tree，AST）思想，它是所有R代码的底层数据结构。\n18.3节：深入了解AST，构成“表达式”的数据类型：constants，symbol，call，pairlist，missing argument。\n18.4节：介绍解析字符串为表达式，并探讨R中的语法。\n18.5节：介绍如何使用递归函数来处理“language”对象和“表达式”。\n18.6节：介绍三种特殊的数据类型：pairlist，missing argument，expression vector。\n\n\n\n\n\nlibrary(rlang)\nlibrary(lobstr)",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#abstract-syntax-tree",
    "href": "Books/Advanced R(2e)/18 Expressions.html#abstract-syntax-tree",
    "title": "18 Expressions",
    "section": "Abstract Syntax Tree",
    "text": "Abstract Syntax Tree\n“表达式”基于抽象语法树（Abstract Syntax Tree，AST）这一数据结构类型。理解该数据结构对检查和修改“表达式”至关重要。\n\nDrawing\n本书会采用两种方式来展示AST，以f(x, \"y\", 1)为例：\n\n通过手绘：\n\n通过lobstr::ast()\n\n\nlobstr::ast(f(x, \"y\", 1))\n#&gt; █─f \n#&gt; ├─x \n#&gt; ├─\"y\" \n#&gt; └─1\n\n两种方法都遵循相同的惯例：\n\n树叶类型：symbol（如f,x），constant（如1,\"y\"）。\n\nsymbol：采用紫色（圆角矩形）\nconstant：采用灰色（黑色，方角矩形）\n字符串会带有引号。\n\n树枝类型：call，采用橙色方角矩形。\n\n“call”对象的第一个元素是函数名，其余元素是参数。\n\n\n注意：本书中由lobstr::ast()函数生成的树没有颜色，你需要在终端内查看。\n上面的例子只有一层，实际中的“表达式”会有很多层。例如f(g(1, 2), h(3, 4, i()))：\n\n\nlobstr::ast(f(g(1, 2), h(3, 4, i())))\n#&gt; █─f \n#&gt; ├─█─g \n#&gt; │ ├─1 \n#&gt; │ └─2 \n#&gt; └─█─h \n#&gt;   ├─3 \n#&gt;   ├─4 \n#&gt;   └─█─i\n\n注意：如果函数的表达式中没有参数，例如i()，在最终的树中，它仅会被表示为symbol。\n树枝的顺序通常也表示了函数的执行顺序（由深到浅执行），但由于惰性评估的存在，也可能会出现跳过某个树枝的情况。\n\n\nNon-code components\nAST在捕获代码时，会忽略空格与注释等不运行的部分。\n\nast(\n  f(x, y) # important!\n)\n#&gt; █─f \n#&gt; ├─x \n#&gt; └─y\n\n只有一个地方会考虑空格：\n\nlobstr::ast(y &lt;- x)\n#&gt; █─`&lt;-` \n#&gt; ├─y \n#&gt; └─x\nlobstr::ast(y &lt; -x)\n#&gt; █─`&lt;` \n#&gt; ├─y \n#&gt; └─█─`-` \n#&gt;   └─x\n\n\n\nInfix calls\nR 中的任何函数都可以写成“prefix”型式。y &lt;- x * 10实际上是由&lt;-和*两个函数组成：\ny &lt;- x * 10\n`&lt;-`(y, `*`(x, 10))\n\n\nlobstr::ast(`&lt;-`(y, `*`(x, 10)))\n#&gt; █─`&lt;-` \n#&gt; ├─y \n#&gt; └─█─`*` \n#&gt;   ├─x \n#&gt;   └─10\n\n\nlobstr::ast(function(x = 1, y = 2) {})\n#&gt; █─`function` \n#&gt; ├─█─x = 1 \n#&gt; │ └─y = 2 \n#&gt; ├─█─`{` \n#&gt; └─NULL",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#expressions",
    "href": "Books/Advanced R(2e)/18 Expressions.html#expressions",
    "title": "18 Expressions",
    "section": "Expressions",
    "text": "Expressions\n本节介绍构成“表达式”的主要三种数据类型——constant，symbol，call。\n\nConstants\n常量是AST中最简单的数据类型。它是除NULL意外的任意长度为1的原子向量，例如TRUE,1L,2.5,\"hello\"。可以使用rlang::is_syntactic_literal()检查是否是“常量”。\n用于表示常量的“表达式”和常量本身是相同的，可以说常量是“自引用”：\n\nidentical(expr(TRUE), TRUE)\nidentical(expr(1), 1)\nidentical(expr(2L), 2L)UE\nidentical(expr(\"x\"), \"x\")\n#&gt; Error in parse(text = input): &lt;text&gt;:3:24: unexpected symbol\n#&gt; 2: identical(expr(1), 1)\n#&gt; 3: identical(expr(2L), 2L)UE\n#&gt;                           ^\n\n\n\nSymbols\n符合表示对象绑定的名称，例如：x,mean,mtcars等。在base R中，is.name()和is.symbol()都可以用来检查是否为“符号”。\n“rlang”包提供了两种创建符号的方法：expr()和sym()。sym()函数直接将一个字符串转换为符号。\n\nexpr(x)\n#&gt; x\nsym(\"x\")\n#&gt; x\n\n可以使用as.character()和rlang::as_string()将符号转换为字符串。\n\nas.character(sym(\"x\"))\n#&gt; [1] \"x\"\nrlang::as_string(sym(\"x\"))\n#&gt; [1] \"x\"\n\n注意：符号类型不能被向量化，它的长度总是1；如果想要创建多个符号，需要将他们组合为列表，并使用rlang::syms()。\n\n\nCalls\ncall object是被捕获的函数，它是一种特殊的列表（pairlist）——首个元素是函数名，后续元素是参数。call object 在AST中组成了树枝。使用is.call()检查是否是call object，typeof()和str()作用于call object时总是返回“language”。\n\nlobstr::ast(read.table(\"important.csv\", row.names = FALSE))\n#&gt; █─read.table \n#&gt; ├─\"important.csv\" \n#&gt; └─row.names = FALSE\nx &lt;- expr(read.table(\"important.csv\", row.names = FALSE))\n\ntypeof(x)\n#&gt; [1] \"language\"\nstr(x)\n#&gt;  language read.table(\"important.csv\", row.names = FALSE)\nis.call(x)\n#&gt; [1] TRUE\n\n“表达式”中的常量或符号类型的长度始终为1和call object有关，因为c()也是函数。\n\nSubsetting\ncall object类似于列表，可以使用标准的提取函数。它的首个元素是函数名，通常是符号类型：\n\nx[[1]]\n#&gt; read.table\nis.symbol(x[[1]])\n#&gt; [1] TRUE\n\n其余元素是参数：\n\nas.list(x[-1])\n#&gt; [[1]]\n#&gt; [1] \"important.csv\"\n#&gt; \n#&gt; $row.names\n#&gt; [1] FALSE\n\n可以使用[[或$提取：\n\nx[[2]]\n#&gt; [1] \"important.csv\"\nx$row.names\n#&gt; [1] FALSE\n\n添加新的元素：\n\nx$header &lt;- TRUE\nx\n#&gt; read.table(\"important.csv\", row.names = FALSE, header = TRUE)\n\n由于R灵活的参数匹配规则，有时候提取特定的参数会变得很困难，此时可以使用call_match()将调用的函数参数补齐并标准化：\n\nrlang::call_match(x, read.table)\n#&gt; read.table(file = \"important.csv\", header = TRUE, row.names = FALSE)\n\n\n\nFunction position\ncall object 的首个元素是function position，它是“表达式”被评估时调用的函数名，通常是符号类型：\n\nlobstr::ast(foo())\n#&gt; █─foo\n\n尽管R允许函数名添加引号，但在被评估后依然会转换为符号类型：\n\nlobstr::ast(\"foo\"())\n#&gt; █─foo\n\ncall object 的首个元素无法修改其name属性，其余属性可以：\n\nx &lt;- expr(foo(x = 1))\nnames(x)\n#&gt; [1] \"\"  \"x\"\nnames(x) &lt;- c(\"x\", \"\")\nx\n#&gt; foo(1)\n\n如果函数不再当前环境中，表达式会添加对函数检索的步骤，例如：函数在某个包、属于某个R6类型、由某个函数工厂创建等。\n\nlobstr::ast(pkg::foo(1))\n#&gt; █─█─`::` \n#&gt; │ ├─pkg \n#&gt; │ └─foo \n#&gt; └─1\nlobstr::ast(obj$foo(1))\n#&gt; █─█─`$` \n#&gt; │ ├─obj \n#&gt; │ └─foo \n#&gt; └─1\nlobstr::ast(foo(1)(2))\n#&gt; █─█─foo \n#&gt; │ └─1 \n#&gt; └─2\n\n\n\n\nConstructing\n使用rlang::call2()可以构建一个call object：\n\ncall2(\"mean\", x = sym(x), na.rm = TRUE)\n#&gt; Error in `sym()`:\n#&gt; ! Can't convert a call to a symbol.\ncall2(expr(base::mean), x = sym(x), na.rm = TRUE)\n#&gt; Error in `sym()`:\n#&gt; ! Can't convert a call to a symbol.\n\ninfix call 的创建方式一样：\n\ncall2(\"&lt;-\", sym(x), 10)\n#&gt; Error in `sym()`:\n#&gt; ! Can't convert a call to a symbol.\n\n我们会在下一章介绍更强大的工具来创建复杂的call object。\n\n\n\nSummary\n下表总结了str()和typeof()处理不同“表达式”类型的结果：\n\n\n\n\nstr()\ntypeof()\n\n\n\n\nScalar constant\nlogi/int/num/chr\nlogical/integer/double/character\n\n\nSymbol\nsymbol\nsymbol\n\n\nCall object\nlanguage\nlanguage\n\n\nPairlist\nDotted pair list\npairlist\n\n\nExpression vector\nexpression()\nexpression\n\n\n\n下表总结了base R 和 “rlang” 提供的用于检测“表达式”类型的函数：\n\n\n\n\nbase\nrlang\n\n\n\n\nScalar constant\n—\nis_syntactic_literal()\n\n\nSymbol\nis.symbol()\nis_symbol()\n\n\nCall object\nis.call()\nis_call()\n\n\nPairlist\nis.pairlist()\nis_pairlist()\n\n\nExpression vector\nis.expression()\n—",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#parsing-and-grammar",
    "href": "Books/Advanced R(2e)/18 Expressions.html#parsing-and-grammar",
    "title": "18 Expressions",
    "section": "Parsing and grammar",
    "text": "Parsing and grammar\n计算机将字符串转为为“表达式”的过程称为解析（parsing），这个过程遵循一系列语法（grammar）。本节我们通过lobstr::ast()函数来探讨解析过程中的语法，以及如何相互转换“表达式”与字符串。\n\nOperator precedence\ninfix 函数的使用会产生两种歧义。第一种歧义类似于：1 + 2 * 3会产生什么，是9（(1 + 2) * 3）还是7（1 + (2 * 3)）？\n\n编程语言使用操作符优先级（operator precedence）来确定表达式的计算顺序。我们可以使用lobstr::ast()来查看实际的计算顺序： **）\n\nlobstr::ast(1 + 2 * 3)\n#&gt; █─`+` \n#&gt; ├─1 \n#&gt; └─█─`*` \n#&gt;   ├─2 \n#&gt;   └─3\n\n预测数学操作符是比较容易的，因为它遵循我们学习过的数学规则。预测其他运算符的优先级则会困难一些，例如R中的!优先级通常比较低：\n\nlobstr::ast(!x %in% y)\n#&gt; █─`!` \n#&gt; └─█─`%in%` \n#&gt;   ├─x \n#&gt;   └─y\n\nlobstr::ast(!1 + !1)\n#&gt; █─`!` \n#&gt; └─█─`+` \n#&gt;   ├─1 \n#&gt;   └─█─`!` \n#&gt;     └─1\n!1 + !1\n#&gt; [1] FALSE\n\nR 有着超过30种，被划分为18个类的infix运算符，详细细节见?Syntax。通常很少有人能记住完整的顺序，如果有任何混淆，请使用括号(！\n\nlobstr::ast((1 + 2) * 3)\n#&gt; █─`*` \n#&gt; ├─█─`(` \n#&gt; │ └─█─`+` \n#&gt; │   ├─1 \n#&gt; │   └─2 \n#&gt; └─3\n\n\n\nAssociativity\n第二种歧义类似于多个相同的infix运算符的优先级。例如1 + 2 + 3与1 + (2 + 3)的关系，很显然是等价的，但在某些情况如geom之间使用的+，执行的先后并不等价。在R中，大多数运算符都遵循从左到右的执行顺序。\n\nlobstr::ast(1 + 2 + 3)\n#&gt; █─`+` \n#&gt; ├─█─`+` \n#&gt; │ ├─1 \n#&gt; │ └─2 \n#&gt; └─3\n\n幂运算与赋值运算相反：\n\nlobstr::ast(2^2^3)\n#&gt; █─`^` \n#&gt; ├─2 \n#&gt; └─█─`^` \n#&gt;   ├─2 \n#&gt;   └─3\nlobstr::ast(x &lt;- y &lt;- z)\n#&gt; █─`&lt;-` \n#&gt; ├─x \n#&gt; └─█─`&lt;-` \n#&gt;   ├─y \n#&gt;   └─z\n\n\n\nParsing and deparsing\n在终端输入代码并执行的过程伴随着终端对代码字符串的解析。同样地，我们也可以将要运行的代码写成字符串形式，然后在必要的地方解析并执行它。\n你可以使用rlang::parse_env()函数解析：\n\nx1 &lt;- \"y &lt;- x + 10\"\nx1\n#&gt; [1] \"y &lt;- x + 10\"\nis.call(x1)\n#&gt; [1] FALSE\n\nx2 &lt;- rlang::parse_expr(x1)\nx2\n#&gt; y &lt;- x + 10\nis.call(x2)\n#&gt; [1] TRUE\n\nparse_expr()总是产生一个“表达式”，如果你的代码字符串中包含;或\\n，需要使用parse_exprs()来解析，生成一个“表达式”列表：\n\nx3 &lt;- \"a &lt;- 1; a + 1\"\nrlang::parse_exprs(x3)\n#&gt; [[1]]\n#&gt; a &lt;- 1\n#&gt; \n#&gt; [[2]]\n#&gt; a + 1\n\n如果你经常处理代码字符串，你应该重新考虑处理方式。阅读第19章，并考虑是否可以使用准引号更安全地生成“表达式”。\nbase R中的解析函数是parse()，我们会在18.6.3节中详细介绍。该函数需要使用text参数来接受代码字符串。\n\nparse(text = x1)\n#&gt; expression(y &lt;- x + 10)\n\n可以使用rlang::expr_text()函数还原（deparsing）一个“表达式”，这个函数在每次打印“表达式”时调用。\n\nz &lt;- expr(y &lt;- x + 10)\nexpr_text(z)\n#&gt; [1] \"y &lt;- x + 10\"\n\n要注意：解析与还原不是对称的，因为解析的过程会自动忽略“空格”、“注释”等不执行的符号：\n\ncat(expr_text(expr({\n  # This is a comment\n  x &lt;-     `x` + 1\n})))\n#&gt; {\n#&gt;     x &lt;- x + 1\n#&gt; }\n\n当使用base R 中的deparse()函数时，要注意它返回的是每行一个字符串的向量。\n\nas.list(deparse(expr({\n  # This is a comment\n  x &lt;-     `x` + 1\n})))\n#&gt; [[1]]\n#&gt; [1] \"{\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"    x &lt;- x + 1\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"}\"",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#walking-ast-with-recursive-functions",
    "href": "Books/Advanced R(2e)/18 Expressions.html#walking-ast-with-recursive-functions",
    "title": "18 Expressions",
    "section": "Walking AST with recursive functions",
    "text": "Walking AST with recursive functions\n本节我们以一个复杂的案例来总结上述有关AST的内容。案例的灵感来自于“codetools”包中的两个函数：\n\nfindGlobals()：可以找出某个函数中使用的全局变量，帮助你检查函数是否独立。\ncheckUsage()：可以检查函数中未使用的局部变量和参数以及参数匹配时只部分使用。\n\n在这里，我们不会实现上面两个函数的全部内容，而只关注它们共同的底层逻辑——递归（变量）AST。因为构成递归函数的两部分与AST的树形数据结构高度适配：\n\nrecursive case处理树AST中的节点。递归函数在处理完某个节点后，会继续递归处理该节点的子节点，最终整理结果。就“表达式”而言，节点就是“call object”和“pairlists”。\nbase case处理AST的叶子。当递归函数处理到“叶子”时，表示递归终止。就“表达式”而言，叶子就是“constant”和“symbol”。\n\n为了使这个模式更容易理解，我们需要两个辅助函数。首先，我们定义expr_type()函数, 它将返回“constant”、“symbol”、“call”、“pairlist”以及其他任何类型的“type”:\n\nexpr_type &lt;- function(x) {\n  if (rlang::is_syntactic_literal(x)) {\n    \"constant\"\n  } else if (is.symbol(x)) {\n    \"symbol\"\n  } else if (is.call(x)) {\n    \"call\"\n  } else if (is.pairlist(x)) {\n    \"pairlist\"\n  } else {\n    typeof(x)\n  }\n}\n\nexpr_type(expr(\"a\"))\n#&gt; [1] \"constant\"\nexpr_type(expr(x))\n#&gt; [1] \"symbol\"\nexpr_type(expr(f(1, 2)))\n#&gt; [1] \"call\"\n\n将expr_type()函数与switch()函数结合：\n\nswitch_expr &lt;- function(x, ...) {\n  switch(expr_type(x),\n    ...,\n    stop(\"Don't know how to handle type \", typeof(x), call. = FALSE)\n  )\n}\n\n使用switch_expr()函数对不同类型“表达式”进行处理（感觉作者是在秀操作，直接expr_tyee()里面加处理步骤不行吗？）：\n\nrecurse_call &lt;- function(x) {\n  switch_expr(x,\n    # Base cases\n    symbol = ,\n    constant = ,\n\n    # Recursive cases\n    call = ,\n    pairlist =\n    )\n}\n\n\nFinding F and T\n我们首先定义一个能判断代码中是否使用了TRUE或FALSE的缩写T或F的函数。\nTRUE与T在AST中是不同的类型：TRUE属于常量，而T属于符号。\n\nexpr_type(expr(TRUE))\n#&gt; [1] \"constant\"\nexpr_type(expr(T))\n#&gt; [1] \"symbol\"\n\n所以我们可以根据数据类型来判断：\n\nlogical_abbr_rec &lt;- function(x) {\n  switch_expr(x,\n    constant = FALSE,\n    symbol = as_string(x) %in% c(\"F\", \"T\")\n  )\n}\n\nlogical_abbr_rec(expr(TRUE))\n#&gt; [1] FALSE\nlogical_abbr_rec(expr(T))\n#&gt; [1] TRUE\n\n上面的函数可以结合enexpr()，无需每次输入都捕获“表达式”：\n\nlogical_abbr &lt;- function(x) {\n  logical_abbr_rec(enexpr(x))\n}\n\nlogical_abbr(T)\n#&gt; [1] TRUE\nlogical_abbr(FALSE)\n#&gt; [1] FALSE\n\n接下来，我们补齐递归函数，使用purrr::some()函数执行递归：\n\nlogical_abbr_rec &lt;- function(x) {\n  switch_expr(x,\n    # Base cases\n    constant = FALSE,\n    symbol = as_string(x) %in% c(\"F\", \"T\"),\n\n    # Recursive cases\n    call = ,\n    pairlist = purrr::some(x, logical_abbr_rec)\n  )\n}\n\nlogical_abbr(mean(x, na.rm = T))\n#&gt; [1] TRUE\nlogical_abbr(function(x, na.rm = T) FALSE)\n#&gt; [1] TRUE\n\n\n\nFinding all variables created by assignment\n本节我们实现函数——找到代码中所有通过赋值函数创建的变量。\n简单回顾一下赋值函数的AST结构：第一个是赋值函数&lt;-，第二个是变量名，第三个是值。\n\nast(x &lt;- 10)\n#&gt; █─`&lt;-` \n#&gt; ├─x \n#&gt; └─10\n\n同样，我们先编写“base case”部分，提取的变量名在AST中是符合类型，所以我们只处理“symbol”：\n\nfind_assign_rec &lt;- function(x) {\n  switch_expr(x,\n    constant = ,\n    symbol = character()\n  )\n}\nfind_assign &lt;- function(x) find_assign_rec(enexpr(x))\n\nfind_assign(\"x\")\n#&gt; character(0)\nfind_assign()\n#&gt; character(0)\n\n使用purrr::flatten_chr将递归的结果展开为向量：\n\nflat_map_chr &lt;- function(.x, .f, ...) {\n  purrr::flatten_chr(purrr::map(.x, .f, ...))\n}\n\nflat_map_chr(letters[1:3], ~ rep(., sample(3, 1)))\n#&gt; [1] \"a\" \"a\" \"b\" \"b\" \"b\" \"c\" \"c\" \"c\"\n\n“recursive case”部分，我们对“pairlists”类型递归处理并整合结果，对“call”类型进行&lt;-检验，直接提取&lt;-中的变量或接着递归处理：\n\nfind_assign_rec &lt;- function(x) {\n  switch_expr(x,\n    # Base cases\n    constant = ,\n    symbol = character(),\n\n    # Recursive cases\n    pairlist = flat_map_chr(as.list(x), find_assign_rec),\n    call = {\n      if (is_call(x, \"&lt;-\")) {\n        as_string(x[[2]])\n      } else {\n        flat_map_chr(as.list(x), find_assign_rec)\n      }\n    }\n  )\n}\n\nfind_assign(a &lt;- 1)\n#&gt; [1] \"a\"\nfind_assign({\n  a &lt;- 1\n  {\n    b &lt;- 2\n  }\n})\n#&gt; [1] \"a\" \"b\"\n\n现在我们需要将函数变得更加鲁棒，考虑如果一个变量被多次赋值：\n\nfind_assign({\n  a &lt;- 1\n  a &lt;- 2\n})\n#&gt; [1] \"a\" \"a\"\n\n我们需要对结果做一些处理：\n\nfind_assign &lt;- function(x) unique(find_assign_rec(enexpr(x)))\n\nfind_assign({\n  a &lt;- 1\n  a &lt;- 2\n})\n#&gt; [1] \"a\"\n\n再考虑，如果多次调用&lt;-：\n\nfind_assign({\n  a &lt;- b &lt;- c &lt;- 1\n})\n#&gt; [1] \"a\"\n\n我们需要对“call”部分单独使用额外的函数来处理：\n\nfind_assign_call &lt;- function(x) {\n  if (is_call(x, \"&lt;-\") && is_symbol(x[[2]])) {\n    lhs &lt;- as_string(x[[2]])\n    children &lt;- as.list(x)[-1]\n  } else {\n    lhs &lt;- character()\n    children &lt;- as.list(x)\n  }\n\n  c(lhs, flat_map_chr(children, find_assign_rec))\n}\n\nfind_assign_rec &lt;- function(x) {\n  switch_expr(x,\n    # Base cases\n    constant = ,\n    symbol = character(),\n\n    # Recursive cases\n    pairlist = flat_map_chr(x, find_assign_rec),\n    call = find_assign_call(x)\n  )\n}\n\nfind_assign(a &lt;- b &lt;- c &lt;- 1)\n#&gt; [1] \"a\" \"b\" \"c\"\nfind_assign(system.time(x &lt;- print(y &lt;- 5)))\n#&gt; [1] \"x\" \"y\"\nast(a &lt;- b &lt;- c &lt;- 1)\n#&gt; █─`&lt;-` \n#&gt; ├─a \n#&gt; └─█─`&lt;-` \n#&gt;   ├─b \n#&gt;   └─█─`&lt;-` \n#&gt;     ├─c \n#&gt;     └─1",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/18 Expressions.html#specialised-data-structures",
    "href": "Books/Advanced R(2e)/18 Expressions.html#specialised-data-structures",
    "title": "18 Expressions",
    "section": "Specialised data structures",
    "text": "Specialised data structures\n本节介绍AST中剩余的两种数据结构和一种特殊的符号。它们在实践中不是很重要。\n\nPairlists\n“pairlist”是R过去的遗留物，几乎被列表（list）所取代。唯一用到的地方是function()函数，它用来储存函数正式的参数。\n\nf &lt;- expr(function(x, y = 10) x + y)\n\nargs &lt;- f[[2]]\nargs\n#&gt; $x\n#&gt; \n#&gt; \n#&gt; $y\n#&gt; [1] 10\ntypeof(args)\n#&gt; [1] \"pairlist\"\n\n在实践中，你可以直接将“pairlist”转换成列表使用：\n\npl &lt;- pairlist(x = 1, y = 2)\nlength(pl)\n#&gt; [1] 2\npl$x\n#&gt; [1] 1\n\n“pairlist”的底层使用链表（linked list）这种数据结构实现，这导致它的提取子集动作要慢一些，但不影响实际体验。\n\n\nMissing arguments\n空符号用来表示缺失的参数，而非缺失值。它唯一出现的场景是你在创建有缺失参数的函数时。\n可以使用missing_arg()或expr()创建一个缺失参数。\n\nmissing_arg()\ntypeof(missing_arg())\n#&gt; [1] \"symbol\"\n\n空符号不会打印任何内容，需要使用rlang::is_missing()来检查。\n\nis_missing(missing_arg())\n#&gt; [1] TRUE\n\n可以在原生的函数形式中发现它：\n\nf &lt;- expr(function(x, y = 10) x + y)\nargs &lt;- f[[2]]\nis_missing(args[[1]])\n#&gt; [1] TRUE\n\n注意：...始终与空符号匹配。\n\nf &lt;- expr(function(...) list(...))\nargs &lt;- f[[2]]\nis_missing(args[[1]])\n#&gt; [1] TRUE\n\n空符号有一个特殊的属性：如果你将他赋值给一个变量，在调用整个变量时，会报错。\n\nx &lt;- missing_arg()\nx\n#&gt; Error: argument \"x\" is missing, with no default\n\n但如果你将它存储到某个数据结构中，它将正常地工作。\n\nms &lt;- list(missing_arg(), missing_arg())\nms[[1]]\n\n如果需要保留变量的缺失性，rlang::maybe_missing()通常很有用。它允许你引用潜在的缺失变量，而不会触发错误。请参阅用例文档和更多详细信息。\n\n\nExpression vectors\n最后，我们简单讨论一下“expression vectors”。它是由两个base R环境中的函数生成——parse()和expression()。\n\nexp1 &lt;- parse(text = c(\"\nx &lt;- 4\nx\n\"))\nexp2 &lt;- expression(x &lt;- 4, x)\n\ntypeof(exp1)\n#&gt; [1] \"expression\"\ntypeof(exp2)\n#&gt; [1] \"expression\"\n\nexp1\n#&gt; expression(x &lt;- 4, x)\nexp2\n#&gt; expression(x &lt;- 4, x)\n\n“expression vectors”的行为更像一个列表：\n\nlength(exp1)\n#&gt; [1] 2\nexp1[[1]]\n#&gt; x &lt;- 4\n\n本质上，“expression vectors”与我们前进介绍的“表达式”列表是相同的。",
    "crumbs": [
      "18 Expressions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html",
    "href": "Books/Advanced R(2e)/2 Names and values.html",
    "title": "2 Names and values",
    "section": "",
    "text": "厘清object和其name的区别十分重要，这可以帮助你：\n\n精准地判断代码对内存的消耗。\n理解代码运行缓慢的原因并优化。\n更好理解R的函数式编程。\n\n\n\n\n\n\n\nTip\n\n\n\n创建一个对象，你不需要使用&lt;-来绑定一个名字。1:10能创建一个，x &lt;- 1:10也能创建一个。\n\n\n\n\n\n2.2节：介绍对象和其name的不同。\n2.3节：copy-on-modify模式，使用tracemem()追踪对象内存地址变化。\n2.4节：R 对象消耗的内存，使用lobstr::obj_size()查看占用大小。\n2.5节：copy-on-modify模式的两个例外，环境对象和只有一个name的对象。\n2.6节：使用gc()释放内存。\n\n\n\n\n\nlibrary(lobstr)\n\n\n\n\n本章节的很多内容来自于下面3处：\n\nR documentation: ?Memory, ?gc.\nmemory profiling in Writing R extensions.\nSEXPs in R internals",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#introduction",
    "href": "Books/Advanced R(2e)/2 Names and values.html#introduction",
    "title": "2 Names and values",
    "section": "",
    "text": "厘清object和其name的区别十分重要，这可以帮助你：\n\n精准地判断代码对内存的消耗。\n理解代码运行缓慢的原因并优化。\n更好理解R的函数式编程。\n\n\n\n\n\n\n\nTip\n\n\n\n创建一个对象，你不需要使用&lt;-来绑定一个名字。1:10能创建一个，x &lt;- 1:10也能创建一个。\n\n\n\n\n\n2.2节：介绍对象和其name的不同。\n2.3节：copy-on-modify模式，使用tracemem()追踪对象内存地址变化。\n2.4节：R 对象消耗的内存，使用lobstr::obj_size()查看占用大小。\n2.5节：copy-on-modify模式的两个例外，环境对象和只有一个name的对象。\n2.6节：使用gc()释放内存。\n\n\n\n\n\nlibrary(lobstr)\n\n\n\n\n本章节的很多内容来自于下面3处：\n\nR documentation: ?Memory, ?gc.\nmemory profiling in Writing R extensions.\nSEXPs in R internals",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#binding-basics",
    "href": "Books/Advanced R(2e)/2 Names and values.html#binding-basics",
    "title": "2 Names and values",
    "section": "Binding basics",
    "text": "Binding basics\n考虑下面的代码：\n\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\n我们创建了一个名为x，值为1,2,3的对象，然后复制了一份讲其命名为y。那么，R在内存中是否也复制了一份，也即消耗了2倍的内存呢？事实上，此时消耗的内存并不是两倍，实际情况如下图所示。\n\n解释一下：\n\nc(1,2,3)创建了对象，并占用内存，地址为0x74b。\n&lt;- 为对象绑定一个名字，即x。\ny &lt;- x虽然将x复制了一份，但是x和y的值相同，都是对象c(1,2,3)，所以内存地址是不变。\n\n也即是说：内存和&lt;-后面的对象有关系，后面的不变，内存地址不变，内存消耗几乎不变；&lt;-前面的只是对象绑定的名字，因为实际的内存地址会随着终端变动而发生变动，需要绑定一个标签，你在代码的其他地方可以调用。\n查看对象的地址可以使用lobstr::obj_addr()，当你运行时，结果肯定与我的结果不同，因为终端发生了变动。\n\nobj_addr(x)\n#&gt; [1] \"0x275ef8a13f8\"\nobj_addr(y)\n#&gt; [1] \"0x275ef8a13f8\"\n\n请注意：上述说的“x和y的值相同”，与“创建值相同的两个对象”不同。\n\na &lt;- c(1, 2, 3)\nb &lt;- c(1, 2, 3)\nobj_addr(a)\n#&gt; [1] \"0x275f52aa298\"\nobj_addr(b)\n#&gt; [1] \"0x275f52a5828\"\n\n\nNon-syntactic names\nR 中对变量名的要求：必须由字母、数字、.、_构成，但_、数字、.+数字不能位于开头；不能使用关键字，查看关键字?Reserved；使用` 可以将任意字符包裹起来当作变量名；不建议使用'或\"将变量名包裹。\n\n_abc &lt;- 1\n\nif &lt;- 10\n\n`_abc` &lt;- 1\n`_abc`\n\n`if` &lt;- 10\n`if`\n#&gt; Error in parse(text = input): &lt;text&gt;:1:2: unexpected symbol\n#&gt; 1: _abc\n#&gt;      ^\n\n\n\nExercises\n\n下面所指的mean函数内存地址都相同。\n\n\nobj_addr(mean)\n#&gt; [1] \"0x275ed7aa260\"\nobj_addr(base::mean)\n#&gt; [1] \"0x275ed7aa260\"\nobj_addr(get(\"mean\"))\n#&gt; [1] \"0x275ed7aa260\"\nobj_addr(evalq(mean))\n#&gt; [1] \"0x275ed7aa260\"\nobj_addr(match.fun(\"mean\"))\n#&gt; [1] \"0x275ed7aa260\"\n\n\nutils::read.csv()添加参数check.names = FALSE可以抑制列名的强制转换。\nmake.names()在将非法名转换为合法名时，会遵循下面的规则：\n\n必要时添加前缀X。\n非法字符转换为.。\n缺失值转换为NA。\nR中的关键字后添加.。",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#copy-on-modify",
    "href": "Books/Advanced R(2e)/2 Names and values.html#copy-on-modify",
    "title": "2 Names and values",
    "section": "Copy-on-modify",
    "text": "Copy-on-modify\n诚如上述，当对象相同时，没有发生内存的消耗；但是如果对y进行了值得修改，那么内存会发生变动，如下所示。\n\ny[[3]] &lt;- 4\nx\n#&gt; [1] 1 2 3\ny\n#&gt; [1] 1 2 4\nobj_addr(x)\n#&gt; [1] \"0x275ef8a13f8\"\nobj_addr(y)\n#&gt; [1] \"0x275f532c6f8\"\n\nx绑定得原对象值未改变，R 创建新的对象，重新与y进行绑定。\n\n如果你修改多次对象，R 会创建多个新的对象，然后重新绑定，但是旧的对象不会消失，仍然存在于内存中，如下所示。\n\ny[[3]] &lt;- 5\nobj_addr(y)\n#&gt; [1] \"0x275f534ca08\"\n\n\n\n\n\n\n\nNote\n\n\n\n请注意：在Rstudio中进行上述内存地址检查时，会有所不同。\n\n\n\ntracemem()\ntracemem()可以跟踪对象，在对象发生变动时显示变动情况，如下所示。\n\ncat(tracemem(x), \"\\n\")\n#&gt; &lt;00000275EF8A13F8&gt;\n\ny &lt;- x\ny[[3]] &lt;- 4L\n#&gt; tracemem[0x00000275ef8a13f8 -&gt; 0x00000275f5366aa8]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main\ny[[3]] &lt;- 5L\n\n使用untracemem()可以停止跟踪，如下所示。\n\nuntracemem(y)\ny[[3]] &lt;- 6L\n\n\n\nFunction calls\n函数生成对象时遵循相同的规则，如下所示。\n\nf &lt;- function(a) {\n  a\n}\n\ncat(tracemem(x), \"\\n\")\n#&gt; &lt;00000275EF8A13F8&gt;\n\nz &lt;- f(x)\n# there's no copy here!\n\nuntracemem(x)\n\n\n解释一下：\n\n黄色部分表示函数，有参数a。\n灰色部分表示执行环境，返回函数运行后的结果a。\n因为返回结果和x一致，没有改变对象，所以仍然绑定相同的对象。\n当函数返回结果与x不一致时，会创建新的对象，重新绑定。\n\n\n\nLists\n与上面的向量不同，list格式的对象不仅本身有内存地址指定，它的元素也有内存地址指定。\n下面是一个简单的list对象，虽然看似简单，但是在内存分配上却不简单。\n\nl1 &lt;- list(1, 2, 3)\n\n\n当复制list对象时，同样内存不会发生改变：\n\nl2 &lt;- l1\n\n\n当复制的对象l2发生元素变动时，虽然R会创建一个新的内存地址，但同上面的情况略有不同：对list的复制是浅复制，不会复制所有的元素。与浅复制相对的是深复制，在R 3.1.0之前，都是深复制。\n\nl2[[3]] &lt;- 4\n\n\n使用lobstr::ref()可以查看list中每个元素的内存地址。注意对一个list单独使用，和对两个list同时使用的结果中前面的数字有不同（自己尝试一下）。\n\nref(l1, l2)\n#&gt; █ [1:0x275f0477f98] &lt;list&gt; \n#&gt; ├─[2:0x275ef1a57e8] &lt;dbl&gt; \n#&gt; ├─[3:0x275ef17bb70] &lt;dbl&gt; \n#&gt; └─[4:0x275ef16deb8] &lt;dbl&gt; \n#&gt;  \n#&gt; █ [5:0x275efe4adf8] &lt;list&gt; \n#&gt; ├─[2:0x275ef1a57e8] \n#&gt; ├─[3:0x275ef17bb70] \n#&gt; └─[6:0x275f07739e8] &lt;dbl&gt;\n\n关于list中的内存指向，你可以试着嵌套一些复杂的list，然后观察内存分配情况。\n\n\nData frames\ndata.frame 其本质就是list，所以它的行为同list一样。\n\nd1 &lt;- data.frame(x = c(1, 5, 6), y = c(2, 4, 3))\nd2 &lt;- d1\nd2[, 2] &lt;- d2[, 2] * 2\nd3 &lt;- d1\nd3[1, ] &lt;- d3[1, ] * 3\n\nref(d1, d2, d3)\n#&gt; █ [1:0x275f088e528] &lt;df[,2]&gt; \n#&gt; ├─x = [2:0x275ee9b21d8] &lt;dbl&gt; \n#&gt; └─y = [3:0x275ee9b2228] &lt;dbl&gt; \n#&gt;  \n#&gt; █ [4:0x275ef553ec8] &lt;df[,2]&gt; \n#&gt; ├─x = [2:0x275ee9b21d8] \n#&gt; └─y = [5:0x275ee94d318] &lt;dbl&gt; \n#&gt;  \n#&gt; █ [6:0x275ee856f08] &lt;df[,2]&gt; \n#&gt; ├─x = [7:0x275ee859688] &lt;dbl&gt; \n#&gt; └─y = [8:0x275ee8596d8] &lt;dbl&gt;\n\n\n\nCharacter vectors\n对于字符串类型的向量，R 使用全局字符串池来储存字符串。\n\nx &lt;- c(\"a\", \"a\", \"abc\", \"d\")\ny &lt;- c(\"a\", \"d\")\nz &lt;- list(\"a\", \"a\", \"abc\", \"d\")\nref(x, character = TRUE)\n#&gt; █ [1:0x275f3e53a48] &lt;chr&gt; \n#&gt; ├─[2:0x275e8e89290] &lt;string: \"a\"&gt; \n#&gt; ├─[2:0x275e8e89290] \n#&gt; ├─[3:0x275f02719b0] &lt;string: \"abc\"&gt; \n#&gt; └─[4:0x275ea4067d8] &lt;string: \"d\"&gt;\nref(y, character = TRUE)\n#&gt; █ [1:0x275f326da48] &lt;chr&gt; \n#&gt; ├─[2:0x275e8e89290] &lt;string: \"a\"&gt; \n#&gt; └─[3:0x275ea4067d8] &lt;string: \"d\"&gt;\nref(z, character = TRUE)\n#&gt; █ [1:0x275f3f8f678] &lt;list&gt; \n#&gt; ├─█ [2:0x275f0e7bdd8] &lt;chr&gt; \n#&gt; │ └─[3:0x275e8e89290] &lt;string: \"a\"&gt; \n#&gt; ├─█ [4:0x275f0e7bc18] &lt;chr&gt; \n#&gt; │ └─[3:0x275e8e89290] \n#&gt; ├─█ [5:0x275f0e7ba58] &lt;chr&gt; \n#&gt; │ └─[6:0x275f02719b0] &lt;string: \"abc\"&gt; \n#&gt; └─█ [7:0x275f0e7b898] &lt;chr&gt; \n#&gt;   └─[8:0x275ea4067d8] &lt;string: \"d\"&gt;\n\n\n\n\nExercises\n\n1:10在内存中创建了对象，但是没有绑定，R无法对没有name的对象进行操作。\nx最先是integer类型，x[[3]] &lt;- 4 使用了double类型的数据替换，会导致R先复制一份double类型的向量，然后再替换数据。数据类型见下一章。",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#object-size",
    "href": "Books/Advanced R(2e)/2 Names and values.html#object-size",
    "title": "2 Names and values",
    "section": "Object size",
    "text": "Object size\n使用lobstr::obj_size()可以查看对象的大小。\n\nobj_size(letters)\n#&gt; 1.71 kB\nobj_size(ggplot2::diamonds)\n#&gt; 3.46 MB\n\nlist对象，因为有上述的浅复制机制，实际大小会与想象的不同。\n\nx &lt;- runif(1e6)\nobj_size(x)\n#&gt; 8.00 MB\n\ny &lt;- list(x, x, x)\nobj_size(y)\n#&gt; 8.00 MB\n\n上述对象中，y比x要略大一些，大约80b，因为包含了三份空元素的list大小是80b。\n\nobj_size(list(NULL, NULL, NULL))\n#&gt; 80 B\n\n字符串向量也有相同的现象。\n\nbanana &lt;- \"bananas bananas bananas\"\nobj_size(banana)\n#&gt; 136 B\nobj_size(rep(banana, 100))\n#&gt; 928 B\n\n\nExercises\n…",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#modify-in-place",
    "href": "Books/Advanced R(2e)/2 Names and values.html#modify-in-place",
    "title": "2 Names and values",
    "section": "Modify-in-place",
    "text": "Modify-in-place\n诚如上述，当复制的对象发生值的改动，R会复制一份新，然后再修改值。但存在两个特例：\n\n当对象只绑定了一个name时，会直接修改对象的值。\n环境变量是一个特殊的对象，它总是直接修改对象的值。\n\n\nObjects with a single binding\n上面提到的“当对象只绑定了一个name时，会直接修改对象的值”，我在实际使用中，发现内存地址会发生变化。我简单的在不同系统中进行了实验，推测可能是系统原因。\n\nv &lt;- c(1, 2, 3)\nlobstr::obj_addr(v)\n#&gt; [1] \"0x275f58030a8\"\nv[[3]] &lt;- 4\nlobstr::obj_addr(v)\n#&gt; [1] \"0x275f580fa58\"\n\n在R中存在下面两种情况，使得无法准确预测是否会发生复制:\n\nR 对于对象所绑定的name统计只能统计为：0，1，many；一旦对象绑定了多个name，那么就会始终被认为是many，无法回退。\n绝大多数函数都会复制对象，除非是用C语言实现的函数。\n\n例如下面的示例（按道理，上面的示例应该是不会发生变化的）：\n\n# 复制一份对象，a,b的地址是一样的\na &lt;- c(1,2,3)\nb &lt;- a\nlobstr::obj_addr(a)\n#&gt; [1] \"0x275e7b88478\"\nlobstr::obj_addr(b)\n#&gt; [1] \"0x275e7b88478\"\n# 当把名字a绑定另外一个对象后，在修改b的值，b的内存地址会发生变化\na &lt;- c(2,3,4)\nb[[3]] &lt;- 4\nlobstr::obj_addr(a)\n#&gt; [1] \"0x275e7d73fb8\"\nlobstr::obj_addr(b)\n#&gt; [1] \"0x275e7d6dfb8\"\n\n上面所描述的对象复制过程，也是R base中for loop缓慢的原因，即for loop本身并不慢，而是因为每次循环都会发生复制修改对象的操作，导致运行缓慢。下面示例中每次循环都会发生两次复制，而转换为list结构时，总共只发生一次复制。\n\nx &lt;- data.frame(matrix(runif(5 * 1e4), ncol = 5))\nmedians &lt;- vapply(x, median, numeric(1))\n\n\n# 每次循环都复制两次\ncat(tracemem(x), \"\\n\")\n\nfor (i in 1:5) {\n  x[[i]] &lt;- x[[i]] - medians[[i]]\n}\n#&gt; tracemem[0x564653d5bca8 -&gt; 0x564656d3b6e8]:\n#&gt; tracemem[0x564656d3b6e8 -&gt; 0x564656d3b838]: [[&lt;-.data.frame [[&lt;-\n#&gt; tracemem[0x564656d3b838 -&gt; 0x564656d3b9f8]:\n#&gt; tracemem[0x564656d3b9f8 -&gt; 0x564656d3bbb8]: [[&lt;-.data.frame [[&lt;-\n#&gt; tracemem[0x564656d3bbb8 -&gt; 0x564656d3be58]:\n#&gt; tracemem[0x564656d3be58 -&gt; 0x564656d3bf38]: [[&lt;-.data.frame [[&lt;-\n#&gt; tracemem[0x564656d3bf38 -&gt; 0x564656d3c248]:\n#&gt; tracemem[0x564656d3c248 -&gt; 0x564656d3c558]: [[&lt;-.data.frame [[&lt;-\n#&gt; tracemem[0x564656d3c558 -&gt; 0x564656d3cc58]:\n#&gt; tracemem[0x564656d3cc58 -&gt; 0x564656d37838]: [[&lt;-.data.frame [[&lt;-\nuntracemem(x)\n\n\n\n# 总共复制一次\ny &lt;- as.list(x)\ncat(tracemem(y), \"\\n\")\n#&gt; &lt;00000275E732A928&gt;\n\nfor (i in 1:5) {\n  y[[i]] &lt;- y[[i]] - medians[[i]]\n}\n#&gt; tracemem[0x00000275e732a928 -&gt; 0x00000275e7322688]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main\n\n\n\nEnvironments\n环境变量储存着对象和name之间的绑定关系，它总是直接修改对象的值，不会进行复制。因为环境本质是一个查找表，存储变量名及其值，如果它们像向量或列表那样每次修改时都进行复制的话，会导致显著的性能开销。\n\ne1 &lt;- rlang::env(a = 1, b = 2, c = 3)\ne2 &lt;- e1\n\nlobstr::obj_addr(e1)\n#&gt; [1] \"0x275ebea9a38\"\nlobstr::obj_addr(e2)\n#&gt; [1] \"0x275ebea9a38\"\n\n\n\ne1$c &lt;- 4\ne2$c\n#&gt; [1] 4\n\nlobstr::obj_addr(e1)\n#&gt; [1] \"0x275ebea9a38\"\nlobstr::obj_addr(e2)\n#&gt; [1] \"0x275ebea9a38\"",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/2 Names and values.html#unbinding-and-the-garbage-collector",
    "href": "Books/Advanced R(2e)/2 Names and values.html#unbinding-and-the-garbage-collector",
    "title": "2 Names and values",
    "section": "Unbinding and the garbage collector",
    "text": "Unbinding and the garbage collector\n关于garbage collector（gc），可以总结为以下几点：\n\nrm()只是解除name绑定，不会清除对象。\nR 环境中没有name绑定的对象，使用gc()会被清除掉。\nR 会在内存不足时自动运行gc()，使用gcinfo(TRUE)后，R每次gc()都会输出信息。\n你无需手动运行gc()，这是没有必要的操作。",
    "crumbs": [
      "2 Names and values"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/21 Translating R code.html",
    "href": "Books/Advanced R(2e)/21 Translating R code.html",
    "title": "21 Translating R code",
    "section": "",
    "text": "first-class环境，词法作用域，元编程组成了一套实现将R代码转换为其他语言的工具箱。例如，dbplyr为dplyr处理数据库提供了支持，允许用R语言表达数据操作并自动将其翻译成SQL，可以使用translate_sql()一览其关键思想：\n\nlibrary(dbplyr)\ncon &lt;- simulate_postgres()\n\ntranslate_sql(x^2, con = con)\n#&gt; &lt;SQL&gt; POWER(`x`, 2.0)\ntranslate_sql(x &lt; 5 & !is.na(x), con = con)\n#&gt; &lt;SQL&gt; `x` &lt; 5.0 AND NOT((`x` IS NULL))\ntranslate_sql(!first %in% c(\"John\", \"Roger\", \"Robert\"), con = con)\n#&gt; &lt;SQL&gt; NOT(`first` IN ('John', 'Roger', 'Robert'))\ntranslate_sql(select == 7, con = con)\n#&gt; &lt;SQL&gt; `select` = 7.0\n\n由于SQL语言有许多特性，将R语言翻译成SQL语言的机制非常复杂，因此本章我们介绍两种简单但有用的领域特定语言(DSL): 一种用于生成HTML, 另一种用于在LaTeX中生成数学方程。\n\n\n\n21.2节：介绍创建HTML。\n21.2节：介绍创建LaTeX。\n\n\n\n\n学习本章，你需要了解：环境、表达式、整洁评估、泛函编程、元编程、S3面向对象等。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice",
    "crumbs": [
      "21 Translating R code"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/21 Translating R code.html#introduction",
    "href": "Books/Advanced R(2e)/21 Translating R code.html#introduction",
    "title": "21 Translating R code",
    "section": "",
    "text": "first-class环境，词法作用域，元编程组成了一套实现将R代码转换为其他语言的工具箱。例如，dbplyr为dplyr处理数据库提供了支持，允许用R语言表达数据操作并自动将其翻译成SQL，可以使用translate_sql()一览其关键思想：\n\nlibrary(dbplyr)\ncon &lt;- simulate_postgres()\n\ntranslate_sql(x^2, con = con)\n#&gt; &lt;SQL&gt; POWER(`x`, 2.0)\ntranslate_sql(x &lt; 5 & !is.na(x), con = con)\n#&gt; &lt;SQL&gt; `x` &lt; 5.0 AND NOT((`x` IS NULL))\ntranslate_sql(!first %in% c(\"John\", \"Roger\", \"Robert\"), con = con)\n#&gt; &lt;SQL&gt; NOT(`first` IN ('John', 'Roger', 'Robert'))\ntranslate_sql(select == 7, con = con)\n#&gt; &lt;SQL&gt; `select` = 7.0\n\n由于SQL语言有许多特性，将R语言翻译成SQL语言的机制非常复杂，因此本章我们介绍两种简单但有用的领域特定语言(DSL): 一种用于生成HTML, 另一种用于在LaTeX中生成数学方程。\n\n\n\n21.2节：介绍创建HTML。\n21.2节：介绍创建LaTeX。\n\n\n\n\n学习本章，你需要了解：环境、表达式、整洁评估、泛函编程、元编程、S3面向对象等。\n\nlibrary(rlang)\nlibrary(purrr)\n#&gt; \n#&gt; Attaching package: 'purrr'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice",
    "crumbs": [
      "21 Translating R code"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/21 Translating R code.html#html",
    "href": "Books/Advanced R(2e)/21 Translating R code.html#html",
    "title": "21 Translating R code",
    "section": "HTML",
    "text": "HTML\nHTML文件是网站的底层核心，是一种特殊的标记语言（SGML，Standard Generalised Markup Language），它和XML相似但不等同。\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100' /&gt;\n&lt;/body&gt;\nHTML文件中的关键组件是标签（tag），形如&lt;tag&gt;&lt;/tag&gt;或&lt;tag/&gt;。标签可以嵌套在其他标签中，并与文本交织在一起。HTML标签有超过100个，但在本章中，我们只关注其中的少数几个：\n\n&lt;body&gt;：文档的主体，包含文档所有内容的顶级标签。\n&lt;h1&gt;：文档的标题级别。\n&lt;p&gt;：段落。\n&lt;b&gt;：粗体。\n&lt;img&gt;：图片。\n\n标签具有带名字的属性，形如&lt;tag name1='value1' name2='value2'/&gt;&lt;/tag&gt;。其中有两个重要的属性——id和class，它们会与CSS联合使用来控制页面的外观。\n&lt;img&gt;标签不包裹任何内容，它只能被写作&lt;img /&gt;而不能写成&lt;img&gt;&lt;/img&gt;，类似&lt;img&gt;的标签被称为空标签（Void tags）。因为它们不能包裹内容，所以它们的属性非常重要，&lt;img&gt;有三个常被使用的属性：scr控制图片路径，width和height控制图片大小。\n因为&lt;和&gt;是HTML中的特殊字符，想要在文本中书写它们，必须用转义符&lt;和&gt;来代替。同样，&也必须用转义符&amp;来代替。\n\nGoal\n我们的目标是使用R生成上面的模板html文档。类似：\nwith_html(\n  body(\n    h1(\"A heading\", id = \"first\"),\n    p(\"Some text &\", b(\"some bold text.\")),\n    img(src = \"myimg.png\", width = 100, height = 100)\n  )\n)\n它有三个特点：\n\n函数名与标签名相同。\n未命名参数成为标签的内容，而命名参数成为其属性。\n& 和其他特殊字符会自动转义。\n\n\n\nEscaping\n转义功能对于代码“翻译”至关重要，它有两个难点：\n\n对输入的字符进行自动转义，&,&lt;,&gt;。\n正确识别是否需要转义，防止&amp;变为&amp;amp;。\n\n解决这两个难点的最简单方法是使用S3面向对象，区分要进行转义的普通字符，和已经转义的字符（类）。\n\nhtml &lt;- function(x) structure(x, class = \"advr_html\")\n\nprint.advr_html &lt;- function(x, ...) {\n  out &lt;- paste0(\"&lt;HTML&gt; \", x)\n  cat(paste(strwrap(out), collapse = \"\\n\"), \"\\n\", sep = \"\")\n}\n\n创建转义泛函和它针对两种类的方法：\n\nescape.character()：对普通字符进行转义。\nescape.advr_html()：对已经转义的字符不做任何处理。\n\n\nescape &lt;- function(x) UseMethod(\"escape\")\n\nescape.character &lt;- function(x) {\n  x &lt;- gsub(\"&\", \"&amp;\", x)\n  x &lt;- gsub(\"&lt;\", \"&lt;\", x)\n  x &lt;- gsub(\"&gt;\", \"&gt;\", x)\n\n  html(x)\n}\n\nescape.advr_html &lt;- function(x) x\n\n检查它是否运行成功：\n\nescape(\"This is some text.\")\n#&gt; &lt;HTML&gt; This is some text.\nescape(\"x &gt; 1 & y &lt; 2\")\n#&gt; &lt;HTML&gt; x &gt; 1 &amp; y &lt; 2\n\n# Double escaping is not a problem\nescape(escape(\"This is some text. 1 &gt; 2\"))\n#&gt; &lt;HTML&gt; This is some text. 1 &gt; 2\n\n# And text we know is HTML doesn't get escaped.\nescape(html(\"&lt;hr /&gt;\"))\n#&gt; &lt;HTML&gt; &lt;hr /&gt;\n\n\n\nBasic tag functions\n接下来，我们将手动编写一个单标签函数，然后弄清楚如何对其进行泛化，这样我们就可以用代码为每个标签生成一个函数。\n我们以&lt;p&gt;标签为例。HTML的标签可以同时具有属性和子标签（&lt;b&gt;,&lt;i&gt;）。考虑到属性有name，子标签没有，我们可以将它们类比为函数参数，在函数内部处理两种类型的参数。p()函数的使用方法可能会类似于：\np(\"Some text. \", b(i(\"some bold italic text\")), class = \"mypara\")\n考虑到标签拥有的属性数目不同，子标签的数量也会不同。我们需要使用...来获取参数，然后根据是否有name属性进行分类。\n\ndots_partition &lt;- function(...) {\n  dots &lt;- list2(...)\n\n  if (is.null(names(dots))) {\n    is_named &lt;- rep(FALSE, length(dots))\n  } else {\n    is_named &lt;- names(dots) != \"\"\n  }\n\n  list(\n    named = dots[is_named],\n    unnamed = dots[!is_named]\n  )\n}\n\nstr(dots_partition(a = 1, 2, b = 3, 4))\n#&gt; List of 2\n#&gt;  $ named  :List of 2\n#&gt;   ..$ a: num 1\n#&gt;   ..$ b: num 3\n#&gt;  $ unnamed:List of 2\n#&gt;   ..$ : num 2\n#&gt;   ..$ : num 4\n\n现在我们可以创建p()函数了。示例中引入了一些新的函数，这里不再详细讨论。\n\nhtml_attributes &lt;- function(list) {\n  if (length(list) == 0) {\n    return(\"\")\n  }\n\n  attr &lt;- map2_chr(names(list), list, html_attribute)\n  paste0(\" \", unlist(attr), collapse = \"\")\n}\nhtml_attribute &lt;- function(name, value = NULL) {\n  if (length(value) == 0) {\n    return(name)\n  } # for attributes with no value\n  if (length(value) != 1) stop(\"`value` must be NULL or length 1\")\n\n  if (is.logical(value)) {\n    # Convert T and F to true and false\n    value &lt;- tolower(value)\n  } else {\n    value &lt;- escape_attr(value)\n  }\n  paste0(name, \"='\", value, \"'\")\n}\nescape_attr &lt;- function(x) {\n  x &lt;- escape.character(x)\n  x &lt;- gsub(\"\\'\", \"&#39;\", x)\n  x &lt;- gsub(\"\\\"\", \"&quot;\", x)\n  x &lt;- gsub(\"\\r\", \"&#13;\", x)\n  x &lt;- gsub(\"\\n\", \"&#10;\", x)\n  x\n}\n\np &lt;- function(...) {\n  dots &lt;- dots_partition(...)\n  attribs &lt;- html_attributes(dots$named)\n  children &lt;- map_chr(dots$unnamed, escape)\n\n  html(paste0(\n    \"&lt;p\",\n    attribs,\n    \"&gt;\",\n    paste(children, collapse = \"\"),\n    \"&lt;/p&gt;\"\n  ))\n}\n\np(\"Some text\")\n#&gt; &lt;HTML&gt; &lt;p&gt;Some text&lt;/p&gt;\np(\"Some text\", id = \"myid\")\n#&gt; &lt;HTML&gt; &lt;p id='myid'&gt;Some text&lt;/p&gt;\np(\"Some text\", class = \"important\", `data-value` = 10)\n#&gt; &lt;HTML&gt; &lt;p class='important' data-value='10'&gt;Some text&lt;/p&gt;\n\n\n\nTag functions\n创建其他的标签函数，我们只需要替换p即可。所以tag()接受一个标签参数，返回一个rlang::new_function()创建的函数。new_function()内使用exprs(... = )来捕获参数。\n\ntag &lt;- function(tag) {\n  new_function(\n    exprs(... = ),\n    expr({\n      dots &lt;- dots_partition(...)\n      attribs &lt;- html_attributes(dots$named)\n      children &lt;- map_chr(dots$unnamed, escape)\n\n      html(paste0(\n        !!paste0(\"&lt;\", tag),\n        attribs,\n        \"&gt;\",\n        paste(children, collapse = \"\"),\n        !!paste0(\"&lt;/\", tag, \"&gt;\")\n      ))\n    }),\n    caller_env()\n  )\n}\ntag(\"b\")\n#&gt; function (...) \n#&gt; {\n#&gt;     dots &lt;- dots_partition(...)\n#&gt;     attribs &lt;- html_attributes(dots$named)\n#&gt;     children &lt;- map_chr(dots$unnamed, escape)\n#&gt;     html(paste0(\"&lt;b\", attribs, \"&gt;\", paste(children, collapse = \"\"), \n#&gt;         \"&lt;/b&gt;\"))\n#&gt; }\n\n现在可以复现上面的函数样式了：\n\np &lt;- tag(\"p\")\nb &lt;- tag(\"b\")\ni &lt;- tag(\"i\")\np(\"Some text. \", b(i(\"some bold italic text\")), class = \"mypara\")\n#&gt; &lt;HTML&gt; &lt;p class='mypara'&gt;Some text. &lt;b&gt;&lt;i&gt;some bold italic\n#&gt; text&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;\n\n在创建所有HTML标签函数前，需要为空标签类型创建泛函void_tag()。它与tag()函数类似，但在出现子标签时报错。\n\nvoid_tag &lt;- function(tag) {\n  new_function(\n    exprs(... = ),\n    expr({\n      dots &lt;- dots_partition(...)\n      if (length(dots$unnamed) &gt; 0) {\n        abort(!!paste0(\"&lt;\", tag, \"&gt; must not have unnamed arguments\"))\n      }\n      attribs &lt;- html_attributes(dots$named)\n\n      html(paste0(!!paste0(\"&lt;\", tag), attribs, \" /&gt;\"))\n    }),\n    caller_env()\n  )\n}\n\nimg &lt;- void_tag(\"img\")\nimg\n#&gt; function (...) \n#&gt; {\n#&gt;     dots &lt;- dots_partition(...)\n#&gt;     if (length(dots$unnamed) &gt; 0) {\n#&gt;         abort(\"&lt;img&gt; must not have unnamed arguments\")\n#&gt;     }\n#&gt;     attribs &lt;- html_attributes(dots$named)\n#&gt;     html(paste0(\"&lt;img\", attribs, \" /&gt;\"))\n#&gt; }\nimg(src = \"myimage.png\", width = 100, height = 100)\n#&gt; &lt;HTML&gt; &lt;img src='myimage.png' width='100' height='100' /&gt;\n\n\n\nProcessing all tags\n现在我们可以批量创建所有的标签函数：\n\ntags &lt;- c(\n  \"a\", \"abbr\", \"address\", \"article\", \"aside\", \"audio\", \"b\", \"bdi\", \"bdo\",\n  \"blockquote\", \"body\", \"button\", \"canvas\", \"caption\", \"cite\", \"code\",\n  \"colgroup\", \"data\", \"datalist\", \"dd\", \"del\", \"details\", \"dfn\", \"div\", \"dl\",\n  \"dt\", \"em\", \"eventsource\", \"fieldset\", \"figcaption\", \"figure\", \"footer\",\n  \"form\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"head\", \"header\", \"hgroup\",\n  \"html\", \"i\", \"iframe\", \"ins\", \"kbd\", \"label\", \"legend\", \"li\", \"mark\", \"map\",\n  \"menu\", \"meter\", \"nav\", \"noscript\", \"object\", \"ol\", \"optgroup\", \"option\",\n  \"output\", \"p\", \"pre\", \"progress\", \"q\", \"ruby\", \"rp\", \"rt\", \"s\", \"samp\",\n  \"script\", \"section\", \"select\", \"small\", \"span\", \"strong\", \"style\", \"sub\",\n  \"summary\", \"sup\", \"table\", \"tbody\", \"td\", \"textarea\", \"tfoot\", \"th\", \"thead\",\n  \"time\", \"title\", \"tr\", \"u\", \"ul\", \"var\", \"video\"\n)\n\nvoid_tags &lt;- c(\n  \"area\", \"base\", \"br\", \"col\", \"command\", \"embed\", \"hr\", \"img\", \"input\",\n  \"keygen\", \"link\", \"meta\", \"param\", \"source\", \"track\", \"wbr\"\n)\n\n仔细观察会发现，有些标签与base R中的函数名重复（body,col,q,sub,summary,table），我们可以将所有的函数以列表的形式保存起来，方便后续调用。\n\nhtml_tags &lt;- c(\n  tags %&gt;% set_names() %&gt;% map(tag),\n  void_tags %&gt;% set_names() %&gt;% map(void_tag)\n)\n\nhtml_tags$p(\n  \"Some text. \",\n  html_tags$b(html_tags$i(\"some bold italic text\")),\n  class = \"mypara\"\n)\n#&gt; &lt;HTML&gt; &lt;p class='mypara'&gt;Some text. &lt;b&gt;&lt;i&gt;some bold italic\n#&gt; text&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;\n\n上面的标签函数调用略有冗长，每次都要前缀html_tags$。我们可以使用一个辅助函数来实现直接使用标签函数。\n\nwith_html &lt;- function(code) {\n  code &lt;- enquo(code)\n  eval_tidy(code, html_tags)\n}\n\nwith_html(\n  body(\n    h1(\"A heading\", id = \"first\"),\n    p(\"Some text &\", b(\"some bold text.\")),\n    img(src = \"myimg.png\", width = 100, height = 100)\n  )\n)\n#&gt; &lt;HTML&gt; &lt;body&gt;&lt;h1 id='first'&gt;A heading&lt;/h1&gt;&lt;p&gt;Some text &amp;&lt;b&gt;some\n#&gt; bold text.&lt;/b&gt;&lt;/p&gt;&lt;img src='myimg.png' width='100' height='100'\n#&gt; /&gt;&lt;/body&gt;",
    "crumbs": [
      "21 Translating R code"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/21 Translating R code.html#latex",
    "href": "Books/Advanced R(2e)/21 Translating R code.html#latex",
    "title": "21 Translating R code",
    "section": "LaTeX",
    "text": "LaTeX\n使用R语言生成LaTeX语句会麻烦一些，因为要同时处理函数名和参数的转换。这意味着，我们需要使用抽象语法树（AST）来修改代码。\n\nLaTeX mathematics\n在开始之前，先简单介绍一下LaTeX中公式的表达方式。完整的标准非常复杂，但幸运的是，相关文档非常详细，而且最常见的命令结构相当简单：\n\n大多数简单的数学方程写法与在R中输入它们的方式相同：x * y，z ^ 5。下标使用_（例如，x_1）。\n特殊符号使用\\转义：\\pi = ，pm = 。LaTeX中有大量这种符号，可以在网上搜索，或者使用http://detexify.kirelabs.org/classify.html。\n复杂函数，形如\\name{arg1}{arg2}。例如，分数\\frac{a}{b}，开方\\sqrt{a}。\n使用{}将元素分组：x ^ a + b与x ^ {a + b}。\n区分函数与变量。使用\\textrm{f}(a * b)来标识f是函数，a * b是变量，不然无法确定f是函数还是变量。\n\n\n\nGoal\n我们的目标是使用这些规则自动将R表达式转换为适当的LaTeX表示形式。我们将分四个阶段处理这个问题：\n\n转换已知的符号：pi -&gt; \\pi\n保留其他符号不变：x -&gt; x，y -&gt; y\n转换已知的函数为特殊符号：sqrt(frac(a,b)) -&gt; \\sqrt{\\frac{a}{b}}\n使用\\textrm{}标识其他函数：f(a) -&gt; \\textrm{f}(a)\n\n\n\nto_math()\n首先，我们封装一个函数，将R表达式转换为LaTeX数学表达式。这将类似于to_html()，通过捕获未计算的表达式并在特殊环境中对其进行计算来实现。主要有两个区别：\n\n评估环境不再是恒定的，因为它必须根据输入而变化。这对于处理未知符号和函数是必要的。\n我们从不在参数环境中计算，因为我们将每个函数都转换为LaTeX表达式。用户需要使用!!才能正常计算。\n\n\nto_math &lt;- function(x) {\n  expr &lt;- enexpr(x)\n  out &lt;- eval_bare(expr, latex_env(expr))\n\n  latex(out)\n}\n\nlatex &lt;- function(x) structure(x, class = \"advr_latex\")\nprint.advr_latex &lt;- function(x) {\n  cat(\"&lt;LATEX&gt; \", x, \"\\n\", sep = \"\")\n}\n\n我们会逐步构建latex_env()。\n\n\nKnown symbols\n第一步，创建一个能生成在LaTeX中用来表示希腊字符的特殊字符的环境。如，pi转换为\\pi。\n\ngreek &lt;- c(\n  \"alpha\", \"theta\", \"tau\", \"beta\", \"vartheta\", \"pi\", \"upsilon\", \"gamma\",\n  \"varpi\", \"phi\", \"delta\", \"kappa\", \"rho\", \"varphi\", \"epsilon\", \"lambda\",\n  \"varrho\", \"chi\", \"varepsilon\", \"mu\", \"sigma\", \"psi\", \"zeta\", \"nu\",\n  \"varsigma\", \"omega\", \"eta\", \"xi\", \"Gamma\", \"Lambda\", \"Sigma\", \"Psi\",\n  \"Delta\", \"Xi\", \"Upsilon\", \"Omega\", \"Theta\", \"Pi\", \"Phi\"\n)\ngreek_list &lt;- set_names(paste0(\"\\\\\", greek), greek)\ngreek_env &lt;- as_environment(greek_list)\n\nlatex_env &lt;- function(expr) {\n  greek_env\n}\n\nto_math(pi)\n#&gt; &lt;LATEX&gt; \\pi\nto_math(beta)\n#&gt; &lt;LATEX&gt; \\beta\n\n\n\nUnknown symbols\n第二步，保留不是希腊字符的符号为原样。但有个问题是：我们无法预先知道输入的符号是什么，没法创建类似greek_env的环境。幸运的是，我们可以使用抽象语法树提取“表达式”中的字符。\n\nexpr_type &lt;- function(x) {\n  if (rlang::is_syntactic_literal(x)) {\n    \"constant\"\n  } else if (is.symbol(x)) {\n    \"symbol\"\n  } else if (is.call(x)) {\n    \"call\"\n  } else if (is.pairlist(x)) {\n    \"pairlist\"\n  } else {\n    typeof(x)\n  }\n}\n\nswitch_expr &lt;- function(x, ...) {\n  switch(expr_type(x),\n    ...,\n    stop(\"Don't know how to handle type \", typeof(x), call. = FALSE)\n  )\n}\n\nflat_map_chr &lt;- function(.x, .f, ...) {\n  purrr::flatten_chr(purrr::map(.x, .f, ...))\n}\n\nall_names_rec &lt;- function(x) {\n  switch_expr(\n    x,\n    constant = character(),\n    symbol = as.character(x),\n    call = flat_map_chr(as.list(x[-1]), all_names)\n  )\n}\n\nall_names &lt;- function(x) {\n  unique(all_names_rec(x))\n}\n\nall_names(expr(x + y + f(a, b, c, 10)))\n#&gt; [1] \"x\" \"y\" \"a\" \"b\" \"c\"\n\n现在，我们可以从输入的“表达式”中提取所有符号并创建环境。\n\nlatex_env &lt;- function(expr) {\n  names &lt;- all_names(expr)\n  symbol_env &lt;- as_environment(set_names(names))\n\n  symbol_env\n}\n\nto_math(x)\n#&gt; &lt;LATEX&gt; x\nto_math(longvariablename)\n#&gt; &lt;LATEX&gt; longvariablename\nto_math(pi)\n#&gt; &lt;LATEX&gt; pi\n\n接下来，我们需要将两个环境结合，将symbol_env设置为greek_env的父环境。\n\nlatex_env &lt;- function(expr) {\n  # Unknown symbols\n  names &lt;- all_names(expr)\n  symbol_env &lt;- as_environment(set_names(names))\n\n  # Known symbols\n  env_clone(greek_env, parent = symbol_env)\n}\n\nto_math(x)\n#&gt; &lt;LATEX&gt; x\nto_math(longvariablename)\n#&gt; &lt;LATEX&gt; longvariablename\nto_math(pi)\n#&gt; &lt;LATEX&gt; \\pi\n\n\n\nKnown functions\n第三步，添加函数。\n首先，我们将介绍一些辅助函数，它们可以轻松地添加新的一元和二元运算符。这些函数非常简单：它们只是组合字符串。\n\nunary_op &lt;- function(left, right) {\n  new_function(\n    exprs(e1 = ),\n    expr(\n      paste0(!!left, e1, !!right)\n    ),\n    caller_env()\n  )\n}\n\nbinary_op &lt;- function(sep) {\n  new_function(\n    exprs(e1 = , e2 = ),\n    expr(\n      paste0(e1, !!sep, e2)\n    ),\n    caller_env()\n  )\n}\n\nunary_op(\"\\\\sqrt{\", \"}\")\n#&gt; function (e1) \n#&gt; paste0(\"\\\\sqrt{\", e1, \"}\")\nbinary_op(\"+\")\n#&gt; function (e1, e2) \n#&gt; paste0(e1, \"+\", e2)\n\n使用这些辅助函数，我们可以映射一些将R转换为LaTeX的示例。请注意，有了R的词法作用域规则的帮助，我们可以轻松地为标准函数如+、-和*, 甚至(和{提供新的含义。\n\n# Binary operators\nf_env &lt;- child_env(\n  .parent = empty_env(),\n  `+` = binary_op(\" + \"),\n  `-` = binary_op(\" - \"),\n  `*` = binary_op(\" * \"),\n  `/` = binary_op(\" / \"),\n  `^` = binary_op(\"^\"),\n  `[` = binary_op(\"_\"),\n\n  # Grouping\n  `{` = unary_op(\"\\\\left{ \", \" \\\\right}\"),\n  `(` = unary_op(\"\\\\left( \", \" \\\\right)\"),\n  paste = paste,\n\n  # Other math functions\n  sqrt = unary_op(\"\\\\sqrt{\", \"}\"),\n  sin = unary_op(\"\\\\sin(\", \")\"),\n  log = unary_op(\"\\\\log(\", \")\"),\n  abs = unary_op(\"\\\\left| \", \"\\\\right| \"),\n  frac = function(a, b) {\n    paste0(\"\\\\frac{\", a, \"}{\", b, \"}\")\n  },\n\n  # Labelling\n  hat = unary_op(\"\\\\hat{\", \"}\"),\n  tilde = unary_op(\"\\\\tilde{\", \"}\")\n)\n\n我们再次修改latex_env()以包含这个环境。这应该是R查找名称的最后一个环境，这样像sin(sin)这样的表达式才能工作。\n\nlatex_env &lt;- function(expr) {\n  # Known functions\n  f_env\n\n  # Default symbols\n  names &lt;- all_names(expr)\n  symbol_env &lt;- as_environment(set_names(names), parent = f_env)\n\n  # Known symbols\n  greek_env &lt;- env_clone(greek_env, parent = symbol_env)\n\n  greek_env\n}\n\nto_math(sin(x + pi))\n#&gt; &lt;LATEX&gt; \\sin(x + \\pi)\nto_math(log(x[i]^2))\n#&gt; &lt;LATEX&gt; \\log(x_i^2)\nto_math(sin(sin))\n#&gt; &lt;LATEX&gt; \\sin(sin)\n\n\n\nUnknown functions\n第四步，添加未知函数到环境中。同样，我们再次使用抽象语法树来提取：\n\nall_calls_rec &lt;- function(x) {\n  switch_expr(x, constant = , symbol = character(), call = {\n    fname &lt;- as.character(x[[1]])\n    children &lt;- flat_map_chr(as.list(x[-1]), all_calls)\n    c(fname, children)\n  })\n}\nall_calls &lt;- function(x) {\n  unique(all_calls_rec(x))\n}\n\nall_calls(expr(f(g + b, c, d(a))))\n#&gt; [1] \"f\" \"+\" \"d\"\n\n创建一个闭包函数生成未知函数：\n\nunknown_op &lt;- function(op) {\n  new_function(\n    exprs(... = ),\n    expr({\n      contents &lt;- paste(..., collapse = \", \")\n      paste0(!!paste0(\"\\\\mathrm{\", op, \"}(\"), contents, \")\")\n    })\n  )\n}\nunknown_op(\"foo\")\n#&gt; function (...) \n#&gt; {\n#&gt;     contents &lt;- paste(..., collapse = \", \")\n#&gt;     paste0(\"\\\\mathrm{foo}(\", contents, \")\")\n#&gt; }\n#&gt; &lt;environment: 0x0000020234685060&gt;\n\n更新latex_env()：\n\nlatex_env &lt;- function(expr) {\n  calls &lt;- all_calls(expr)\n  call_list &lt;- map(set_names(calls), unknown_op)\n  call_env &lt;- as_environment(call_list)\n\n  # Known functions\n  f_env &lt;- env_clone(f_env, call_env)\n\n  # Default symbols\n  names &lt;- all_names(expr)\n  symbol_env &lt;- as_environment(set_names(names), parent = f_env)\n\n  # Known symbols\n  greek_env &lt;- env_clone(greek_env, parent = symbol_env)\n  greek_env\n}\n\n测试：\n\nto_math(sin(pi) + f(a))\n#&gt; &lt;LATEX&gt; \\sin(\\pi) + \\mathrm{f}(a)\n\n你可以进一步拓展这个想法，翻译数学表达式的类型，但你应该不再需要任何额外的元编程工具了。",
    "crumbs": [
      "21 Translating R code"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/23 Measuring performance.html",
    "href": "Books/Advanced R(2e)/23 Measuring performance.html",
    "title": "23 Measuring performance",
    "section": "",
    "text": "Tip\n\n\n\nProgrammers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered.\n— Donald Knuth\n\n\n在你想让代码运行得更快之前，首先需要弄清楚是什么让它变慢。这听起来很简单，但实际上并非如此。即使是有经验的程序员，也很难找出代码中的瓶颈。所以，不要依赖直觉，你应该使用剖析器（profiling）对代码进行性能分析：使用真实的输入来测量每一行代码的运行时间。\n一旦你找出了瓶颈，就需要谨慎地尝试各种替代方案，以找到更快且功能相同的代码。在第24章中，你将学到很多加快代码运行速度的方法，但首先你需要学习如何进行微基准测试（microbenchmark），这样才能精确地衡量性能上的差异。\n\n\n23.2节：介绍如何使用剖析器来分析导致代码运行慢的原因。\n23.3节：如何使用微基准测试来探索可替代方案并找到运行最快的方案。\n\n使用“profvis”进行性能分析和微基准测试。\n\nlibrary(profvis)\nlibrary(bench)",
    "crumbs": [
      "23 Measuring performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/23 Measuring performance.html#introduction",
    "href": "Books/Advanced R(2e)/23 Measuring performance.html#introduction",
    "title": "23 Measuring performance",
    "section": "",
    "text": "Tip\n\n\n\nProgrammers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered.\n— Donald Knuth\n\n\n在你想让代码运行得更快之前，首先需要弄清楚是什么让它变慢。这听起来很简单，但实际上并非如此。即使是有经验的程序员，也很难找出代码中的瓶颈。所以，不要依赖直觉，你应该使用剖析器（profiling）对代码进行性能分析：使用真实的输入来测量每一行代码的运行时间。\n一旦你找出了瓶颈，就需要谨慎地尝试各种替代方案，以找到更快且功能相同的代码。在第24章中，你将学到很多加快代码运行速度的方法，但首先你需要学习如何进行微基准测试（microbenchmark），这样才能精确地衡量性能上的差异。\n\n\n23.2节：介绍如何使用剖析器来分析导致代码运行慢的原因。\n23.3节：如何使用微基准测试来探索可替代方案并找到运行最快的方案。\n\n使用“profvis”进行性能分析和微基准测试。\n\nlibrary(profvis)\nlibrary(bench)",
    "crumbs": [
      "23 Measuring performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/23 Measuring performance.html#profiling",
    "href": "Books/Advanced R(2e)/23 Measuring performance.html#profiling",
    "title": "23 Measuring performance",
    "section": "Profiling",
    "text": "Profiling\n剖析器是所有编程语言中用来分析代码性能的主要工具。剖析器的种类繁多，R使用一种很简单称为采样（统计）剖析器的工具。它的工作原理是每个几毫秒停止程序执行，记录当前的调用栈，进而统计每个栈消耗的时间。例如下面的f()函数：\n\nf &lt;- function() {\n  pause(0.1)\n  g()\n  h()\n}\ng &lt;- function() {\n  pause(0.1)\n  h()\n}\nh &lt;- function() {\n  pause(0.1)\n}\n\n使用profvis::pause()而非Sys.sleep()是因为后者不会出现在结果中，R会认为Sys.sleep()不消耗时间。\n如果我们剖析f()的运行过程，每隔 0.1s 停止运行，我们会看到：\n\"pause\" \"f\"\n\"pause\" \"g\" \"f\"\n\"pause\" \"h\" \"g\" \"f\"\n\"pause\" \"h\" \"f\"\n每一行代表剖析器的一个 “刻度”（本例中为 0.1s）, 函数调用从右到左记录：第一行显示f()调用pause()。它表示代码运行f()花费了 0.1s，然后运行g()花费了 0.2s，最后运行h()花费了 0.1s。\n使用utils::Rprof()可以真实地显示剖析过程，但不太可能得到上面那种清晰地结果：\n\ntmp &lt;- tempfile()\nRprof(tmp, interval = 0.1)\nf()\n#&gt; NULL\nRprof(NULL)\nwriteLines(readLines(tmp))\n#&gt; sample.interval=100000\n#&gt; \"pause\" \"f\" \"eval\" \"eval\" \"withVisible\" \"withCallingHandlers\" \"eval\" \"eval\" \"with_handlers\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"withRestarts\" \"evaluate::evaluate\" \"evaluate\" \"in_dir\" \"in_input_dir\" \"eng_r\" \"block_exec\" \"call_block\" \"process_group\" \"withCallingHandlers\" \"xfun:::handle_error\" \"process_file\" \"knitr::knit\" \"rmarkdown::render\" \"execute\" \".main\" \n#&gt; \"pause\" \"g\" \"f\" \"eval\" \"eval\" \"withVisible\" \"withCallingHandlers\" \"eval\" \"eval\" \"with_handlers\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"withRestarts\" \"evaluate::evaluate\" \"evaluate\" \"in_dir\" \"in_input_dir\" \"eng_r\" \"block_exec\" \"call_block\" \"process_group\" \"withCallingHandlers\" \"xfun:::handle_error\" \"process_file\" \"knitr::knit\" \"rmarkdown::render\" \"execute\" \".main\" \n#&gt; \"pause\" \"h\" \"g\" \"f\" \"eval\" \"eval\" \"withVisible\" \"withCallingHandlers\" \"eval\" \"eval\" \"with_handlers\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"withRestarts\" \"evaluate::evaluate\" \"evaluate\" \"in_dir\" \"in_input_dir\" \"eng_r\" \"block_exec\" \"call_block\" \"process_group\" \"withCallingHandlers\" \"xfun:::handle_error\" \"process_file\" \"knitr::knit\" \"rmarkdown::render\" \"execute\" \".main\" \n#&gt; \"pause\" \"h\" \"f\" \"eval\" \"eval\" \"withVisible\" \"withCallingHandlers\" \"eval\" \"eval\" \"with_handlers\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"doWithOneRestart\" \"withOneRestart\" \"withRestartList\" \"withRestarts\" \"evaluate::evaluate\" \"evaluate\" \"in_dir\" \"in_input_dir\" \"eng_r\" \"block_exec\" \"call_block\" \"process_group\" \"withCallingHandlers\" \"xfun:::handle_error\" \"process_file\" \"knitr::knit\" \"rmarkdown::render\" \"execute\" \".main\"\n\n这是因为所有剖析器都必须在准确性和性能之间做出基本的权衡。使用采样剖析器时，这种权衡对性能的影响微乎其微，但本质上是随机的，因为计时器的准确性和每次操作所花费的时间都存在一定的变异性。这意味着每次进行性能分析时，你得到的答案都会略有不同。幸运的是，变异性影响对多的是那些运行时间很短的函数，而这些函数也是最不受关注的。\nVisualising profiles\n剖析器的默认时间间隔很小，即使你得函数会运行几秒钟也会被采样几百次，这导致你无法在终端清晰的看到剖析结果。除了使用utils::Rprof()来捕获剖析结果，也可以使用“profvis”提供的可视化功能。“profvis”还将性能分析数据与底层源代码联系起来，让开发者能直观地看到源代码中哪些具体的行或函数是性能瓶颈（比如某行代码运行时间过长、内存分配过多），从而更轻松地在脑海中构建出代码性能问题的清晰图景。如果“profvis”无法帮到你，你也可以尝试utils::summaryRprof()或“proftools”包。\n有两种使用“profvis”的方式：\n\n使用Rstudio的“Profile”菜单。\n\n使用profvis::profvis()。推荐保存代码到单独的文件中，然后source()它，最后在剖析。这样可以保证性能分析数据与源代码联系的更准确。\nsource(\"profiling-example.R\")\nprofvis(f())\n在剖析结束后，“profvis”会生成可交互的网页来探索结果。界面如下：\n\n\n\n顶栏显示源码，并且在侧边有对应的内存和时间消耗信息。这些信息可以让你对瓶颈有一个整体的感觉，但并不总是能帮助你准确地识别原因。例如，你可以看到h()消耗了150ms，是g()的两倍；但这并不是因函数h()更慢，而是因为它被调用的频率是g()的两倍。\n底栏展示全部调用栈的火焰图（Flame Graph），显示了每个函数的完整调用序列。从这里也可以看到，h()被调用了两次。将鼠标悬停在某个调用栈上，可以获取更多信息。\n\n你也可以使用顶栏的“data tab”信息，可以交互地查看每个节点的性能信息。这基本上与火焰图的显示方式相同（旋转90度）, 但当你有非常大或深度嵌套的调用栈时，它更有用，因为你可以选择交互式地缩放仅选定的组件。\n\nMemory profiling\n火焰图中有一个与源代码不匹配的特殊条目——&lt;GC&gt;，表示垃圾收集器gc()正在运行。如果&lt;GC&gt;消耗大量时间，通常表示代码创建了大量临时对象。例如：\nx &lt;- integer()\nfor (i in 1:1e4) {\n  x &lt;- c(x, i)\n}\n如果剖析上面的代码，你会发现垃圾收集器占用了大量时间。\n\n当你在自己的代码中看到垃圾回收器占用大量时间时，可以通过查看顶栏内存消耗信息来找出问题的根源：右侧的柱子表示被分配的内存，左侧的柱子表示被释放的内存。上面代码出现这种问题的根源是“修改后复制”机制（2.3节）: 循环的每次迭代都会创建x的另一个副本。在24.6节中，将介绍解决这种类型问题的策略。\nLimitations\n采样剖析器仍然有一些缺点：\n\n剖析器对C/C++语言无效。你可以检查R代码是否调用了C/C++代码，但无法检查C/C++代码中调用了哪些函数。用于分析编译代码的工具超出了本书的范围；请从 https://github.com/r-prof/jointprof 开始。\n如果你正在使用匿名函数进行大量的函数式编程，可能很难准确地判断调用的是哪个函数。解决这个问题最简单的方法就是给你的函数命名。\n\n惰性评估意味着参数通常在另一个函数中进行评估，这使得调用栈更加复杂（见7.5.2节）。不幸的是，R的性能分析器没有存储足够的信息来解开惰性评估，因此在以下代码中，性能分析会使i()看起来像是由 j()调用的，因为参数在被j()需要之前不会被评估。\ni &lt;- function() {\n  pause(0.1)\n  10\n}\nj &lt;- function(x) {\n  x + 10\n}\nj(i())",
    "crumbs": [
      "23 Measuring performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/23 Measuring performance.html#microbenchmarking",
    "href": "Books/Advanced R(2e)/23 Measuring performance.html#microbenchmarking",
    "title": "23 Measuring performance",
    "section": "Microbenchmarking",
    "text": "Microbenchmarking\n某段代码的运行耗时可能只有几毫秒，几微秒，甚至几纳秒，微基准测试（microbenchmark）可以用来测量这些代码的性能，进而进行对比。但一定要警惕将微基准测试的结果推广到实际代码中：微基准测试中观察到的差异在实际代码中通常会被更高阶的效应所主导；就像在烘焙时，对亚原子物理学的深入理解并没有太大帮助一样。\n“bench”包提供了很好的用来进行微基准测试的工具。“bench”包采用了更精准的计时器，能够尽可能地对比那些耗时极少的操作。例如，对比两种不同开方函数的性能：\n\nx &lt;- runif(100)\n(lb &lt;- bench::mark(\n  sqrt(x),\n  x^0.5\n))\n#&gt; # A tibble: 2 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 sqrt(x)       400ns    500ns  1285099.      848B        0\n#&gt; 2 x^0.5         2.4µs    2.6µs   347513.      848B        0\n\nbench::mark()默认会将所有方法都至少运行一次（min_iterations = 1），并且最多花费0.5s（min_time = 0.5）。它会检查每次运行返回的值是否相同，如果你想测试返回不同值的速度，可以设置check = FALSE。\n\nbench::mark() results\nbench::mark()会将结果返回为一个tibble，每行是一个表达式，每列是一个测量结果。\n\n\nmin， mean，median，max，itr/sec估计出运行时间的分布。通常，关注运行时间的最小值和中值，因为运行时间往往右偏。例如上面的lb结果显示：sqrt()方法要更优：\n\nplot(lb)\n\n\n\n\n\n\n\n\nmen_alloc表示第一次运行时的内存占用。n_gc()表示垃圾回收的次数。\nn_itr和total_time表示整个测试过程的运行次数和运行总时间。\nresult，memory，time，gc列表列（list-column），储存了原始的运行结果。\n\n因为结果是tibble格式，你可以使用[提取列：\n\nlb[c(\"expression\", \"min\", \"median\", \"itr/sec\", \"n_gc\")]\n#&gt; # A tibble: 2 × 4\n#&gt;   expression      min   median `itr/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt;\n#&gt; 1 sqrt(x)       400ns    500ns  1285099.\n#&gt; 2 x^0.5         2.4µs    2.6µs   347513.\n\nInterpreting results\n一定要注意基准测试结果中的单位。因为单位才是实际中我们关注的内容，例如上例中的结果，运行100次后总共的耗时也是毫秒级别，即使二者有显著差异，但实际体验中并不会造成影响。",
    "crumbs": [
      "23 Measuring performance"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html",
    "title": "25 Rewriting R code in C++",
    "section": "",
    "text": "有时，即便你找出来代码的运行瓶颈，同时做了所有可以在R中做的优化，运行依然很慢。这仅仅是因为R本身就是慢（🤦‍）。本章介绍如何使用“Rcpp”包实现用C++重写关键函数来提升性能。\n“Rcpp”包提供了干净可及的API来实现C++与R之间的链接，同时隔离R本身的复杂的C的API。虽然它也可以用来些C或Fortran代码，但相较C++稍显痛苦。\nC++可以用来解析下面典型的瓶颈：\n\n无法轻易向量化的循环，因为后续迭代依赖于先前的迭代。\n递归函数，或者涉及调用函数数百万次的问题。在C++中调用函数的开销比在R中低得多。\n需要使用R没有提供的高级数据结构和算法的问题。通过标准模板库（STL），C++高效地实现了许多重要的数据结构，从有序映射到双端队列。\n\n本章的目的是仅讨论C++和“Rcpp”的那些绝对必要的方面，以帮助消除代码中的瓶颈。我们不会在面向对象编程或模板等高级功能上花费太多时间，因为重点是编写小型、自包含的函数，而不是大型程序。C++的工作知识是有用的，但不是必不可少的。许多优秀的教程和参考资料可以免费获得，包括 http://www.learncpp.com/ 和 https://en.cppreference.com/w/cpp 。对于更高级的主题，Scott Meyers的“Effective C++”系列是一个受欢迎的选择。\n\n\n25.2节：介绍如何将简单的R函数转换为等价的C++函数，并以此介绍C++与R的不同之处。\n25.2.5小节：介绍如何使用sourceCpp()函数加载C++代码。\n25.3节：介绍如何修改“Rcpp”中的属性，和一些其他重要的类。\n25.4节：介绍如何在C++中处理R的缺失值。\n25.5节：介绍如何使用标准模板库中的一些重要数据结构和算法。\n25.6节：介绍两个使用“Rcpp”极大提升性能的例子。\n25.7节：介绍如何将C++代码添加到R包中。\n25.8节：介绍更多有关“Rcpp”的资源。\n\n\nlibrary(Rcpp)\n\n我们需要一个C++的编译环境：\n\nWindows：安装Rtools。\nMac：安装Xcode。\nLinux：sudo apt-get install r-base-dev",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#introduction",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#introduction",
    "title": "25 Rewriting R code in C++",
    "section": "",
    "text": "有时，即便你找出来代码的运行瓶颈，同时做了所有可以在R中做的优化，运行依然很慢。这仅仅是因为R本身就是慢（🤦‍）。本章介绍如何使用“Rcpp”包实现用C++重写关键函数来提升性能。\n“Rcpp”包提供了干净可及的API来实现C++与R之间的链接，同时隔离R本身的复杂的C的API。虽然它也可以用来些C或Fortran代码，但相较C++稍显痛苦。\nC++可以用来解析下面典型的瓶颈：\n\n无法轻易向量化的循环，因为后续迭代依赖于先前的迭代。\n递归函数，或者涉及调用函数数百万次的问题。在C++中调用函数的开销比在R中低得多。\n需要使用R没有提供的高级数据结构和算法的问题。通过标准模板库（STL），C++高效地实现了许多重要的数据结构，从有序映射到双端队列。\n\n本章的目的是仅讨论C++和“Rcpp”的那些绝对必要的方面，以帮助消除代码中的瓶颈。我们不会在面向对象编程或模板等高级功能上花费太多时间，因为重点是编写小型、自包含的函数，而不是大型程序。C++的工作知识是有用的，但不是必不可少的。许多优秀的教程和参考资料可以免费获得，包括 http://www.learncpp.com/ 和 https://en.cppreference.com/w/cpp 。对于更高级的主题，Scott Meyers的“Effective C++”系列是一个受欢迎的选择。\n\n\n25.2节：介绍如何将简单的R函数转换为等价的C++函数，并以此介绍C++与R的不同之处。\n25.2.5小节：介绍如何使用sourceCpp()函数加载C++代码。\n25.3节：介绍如何修改“Rcpp”中的属性，和一些其他重要的类。\n25.4节：介绍如何在C++中处理R的缺失值。\n25.5节：介绍如何使用标准模板库中的一些重要数据结构和算法。\n25.6节：介绍两个使用“Rcpp”极大提升性能的例子。\n25.7节：介绍如何将C++代码添加到R包中。\n25.8节：介绍更多有关“Rcpp”的资源。\n\n\nlibrary(Rcpp)\n\n我们需要一个C++的编译环境：\n\nWindows：安装Rtools。\nMac：安装Xcode。\nLinux：sudo apt-get install r-base-dev",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#getting-started-with-c",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#getting-started-with-c",
    "title": "25 Rewriting R code in C++",
    "section": "Getting started with C++",
    "text": "Getting started with C++\ncppFunction()允许直接在R中编写C++代码：\n\ncppFunction(\n  \"\n  int add(int x, int y, int z) {\n    int sum = x + y + z;\n    return sum;\n  }\n  \"\n)\n# add works like a regular R function\nadd\n#&gt; function (x, y, z) \n#&gt; .Call(&lt;pointer: 0x00007ffc68b115f0&gt;, x, y, z)\nadd(1, 2, 3)\n#&gt; [1] 6\n\n运行这段代码时，“Rcpp”会编译C++代码，并将R函数add与C++函数add关联起来。关联过程的大量底层细节被“Rcpp”自行处理了。下面我们以不同的例子，由浅入深地介绍C++的语法。\nNo inputs, scalar output\n让我们以一个非常简单的函数开始——没有任何参数，总是返回整数1：\none &lt;- function() 1L\n等价的C++函数：\nint one() {\n  return 1;\n}\n使用cppFunction()实现在R中使用这个C++函数：\n\ncppFunction(\n  \"\n  int one() {\n    return 1;\n  }\n  \"\n)\n\n上面的示例展示了R与C++之间的一些主要区别：\n\n创建函数的语法与调用函数的语法类似；不像在R中那样使用赋值来创建函数。\n必须声明函数返回结果的类型。\n标量（scalar）与向量（vector）不同。R中的数字、整数、字符、逻辑向量的对应符号是：NumericVector、IntegerVector、CharacterVector、LogicalVector。标量的对应符号是：double、int、String、bool。\n必须有清晰的return声明来返回值。\n每个声明必须以分号;结束。\nScalar input, scalar output\n第二个例子是sign()函数的标量版本，接受一个标量输入，当输入为正数时返回1，为0时返回0，为负数时返回-1：\nsignR &lt;- function(x) {\n  if (x &gt; 0) {\n    1\n  } else if (x == 0) {\n    0\n  } else {\n    -1\n  }\n}\n\ncppFunction(\n  \"\n  int signC(int x) {\n    if (x &gt; 0) {\n      return 1;\n    } else if (x == 0) {\n      return 0;\n    } else {\n      return -1;\n    }\n  }\n \"\n)\nC++版的函数中：\n\n函数的输入与输出一样，也需要声明类型。\n\nif语句的语法一样，退出可以使用break，但是跳过需要使用continue而非next 。\nVector input, scalar output\nR与C++有一个显著的不同是在for循环的资源消耗上，C++更低。例如，在for循环中执行sum函数：\nR版本：\n\nsumR &lt;- function(x) {\n  total &lt;- 0\n  for (i in seq_along(x)) {\n    total &lt;- total + x[i]\n  }\n  total\n}\n\nC++版本：\n\ncppFunction(\n  \"\n  double sumC(NumericVector x) {\n    int n = x.size();\n    double total = 0;\n    for(int i = 0; i &lt; n; ++i) {\n      total += x[i];\n    }\n    return total;\n  }\n  \"\n)\n\nC++版与R版本在结构上很相似，但：\n\n获取向量长度，C++通过.size()方法完成。C++通过.来调用方法。\nfor的声明语法不一样：for(init; check; increment)。额外设置变量i，判断i与n的比较。\nC++中位置索引的起始是0。\n使用=赋值，而不是&lt;-。\nC++提供原位修改运算符：total += x[i]。除此还有-=、*=、/=。\n\n\nx &lt;- runif(1e3)\nbench::mark(\n  sum(x),\n  sumC(x),\n  sumR(x)\n)[1:6]\n#&gt; # A tibble: 3 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 sum(x)        700ns    800ns  1145751.        0B        0\n#&gt; 2 sumC(x)       1.6µs    1.7µs   531629.        0B        0\n#&gt; 3 sumR(x)      21.8µs   22.1µs    42750.      18KB        0\n\nVector input, vector output\n下面我们创建一个计算单个值与向量的欧几里得距离：\nR版本：\n\npdistR &lt;- function(x, ys) {\n  sqrt((x - ys)^2)\n}\n\n在R中，我们没有在函数中定义x是标量，而是在文档中明确说明这一点。在C++版本中，则必须明确说明类型：\n\ncppFunction(\n  \"\n  NumericVector pdistC(double x, NumericVector ys) {\n    int n = ys.size();\n    NumericVector out(n);\n\n    for(int i = 0; i &lt; n; ++i) {\n      out[i] = sqrt(pow(ys[i] - x, 2.0));\n    }\n\n    return out;\n  }\n  \"\n)\n\n这个C++函数引入了一些新概念：\n\n创建一个长度为n的数字向量需要构造器：NumericVector out(n)。拷贝一个需要：NumericVector zs = clone(ys)。\n使用pow()而非^计算幂。\n\n\ny &lt;- runif(1e6)\nbench::mark(\n  pdistR(0.5, y),\n  pdistC(0.5, y)\n)[1:6]\n#&gt; # A tibble: 2 × 6\n#&gt;   expression          min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;     &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 pdistR(0.5, y)   5.88ms   6.29ms      156.    7.63MB     81.6\n#&gt; 2 pdistC(0.5, y)   3.39ms   3.89ms      245.    7.63MB    122.\n\n上面的结果显示，C++并没有比R快多少，节约的时间与开发C++函数的时间相比不值一提。而且C++运行快速的真正原因是它只是用了零时标量ys[i] -x，而R使用了临时向量x - ys，额外分配的内存会占用大量时间。\nUsing sourceCpp\n在实际开发中，我们往往需要使用sourceCpp()来单独加载C++脚本。在使用支持C++的IDE编写脚本可以提供语法高亮，自动补齐，语法检查等功能。\n独立的C++脚本需要加载.cpp插件：\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n同时每个函数前需要声明// [[Rcpp::export]]才可以被R调用。\n你也可以在C++注释块中插入R代码，在测试时，会被执行并在R终端输出结果（因为source(echo = TRUE)）：\n/*** R\n# This is R code\n*/\n运行sourceCpp(\"path/to/file.cpp\")会创建对应的R函数，并加载到当前环境。需要注意：这些函数无法被保存在.Rdata文件中，每次启动R都需要重新创建。\n例如，运行sourceCpp()加载下面的脚本，会创建一个名为meanC的函数，并加载到当前环境：\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble meanC(NumericVector x) {\n  int n = x.size();\n  double total = 0;\n\n  for(int i = 0; i &lt; n; ++i) {\n    total += x[i];\n  }\n  return total / n;\n}\n\n/*** R\nx &lt;- runif(1e5)\nbench::mark(\n  mean(x),\n  meanC(x)\n)\n*/\n在Rmarkdown中，可以设置{r, engine = \"Rcpp\"}进行编译，所以后续的C++函数定义都单独放在一个代码块中，此时不能添加注释了哦。",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#other-classes",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#other-classes",
    "title": "25 Rewriting R code in C++",
    "section": "Other classes",
    "text": "Other classes\n前面介绍了“Rcpp”支持的基础向量类型：NumericVector、IntegerVector、CharacterVector、LogicalVector，基础标量类型：double、int、String、bool。“Rcpp”也支持其他数据类型，重要的如，list，dataframe，function，attribute。同时它也对很多类提供支持如，Environment，DottedPair，Language，Symbol等，但超出了本书的范围。（所谓的“支持”就是说，C++代码可以访问R对象，也可以生成一个R对象。）\nLists and data frames\n“Rcpp”提供了对list和data frame的支持，但它们常用来作为输出使用，而非输入。这是因为它们可以有任意的附加属性，当作为输入时，C++需要提前知道这些属性。如果 list 的结构已知且固定，C++可以提取它的元素并使用as()将其转换为其他类型。例如：lm()生成的结果包含的内容是固定的，可以从结果中抽取对应的元素计算“平均百分比误差”（mpe()）。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble mpe(List mod) {\n  if (!mod.inherits(\"lm\")) stop(\"Input must be a linear model\");\n\n  NumericVector resid = as&lt;NumericVector&gt;(mod[\"residuals\"]);\n  NumericVector fitted = as&lt;NumericVector&gt;(mod[\"fitted.values\"]);\n\n  int n = resid.size();\n  double err = 0;\n  for(int i = 0; i &lt; n; ++i) {\n    err += resid[i] / (fitted[i] + resid[i]);\n  }\n  return err / n;\n}\n\n注意：使用.inherits()检查对象类型是否真的是“lm”。\n\nmod &lt;- lm(mpg ~ wt, data = mtcars)\nmpe(mod)\n#&gt; [1] -0.01541615\n\nFunctions\nC++可以使用Function类直接访问R函数，但是因为输出的结果类型不确定，所以我们需要使用RObject类。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nRObject callWithOne(Function f) {\n  return f(1);\n}\n\n\ncallWithOne(function(x) x + 1)\n#&gt; [1] 2\ncallWithOne(paste)\n#&gt; [1] \"1\"\n\n当函数f使用位置索引获取参数时：\nf(\"y\", 1);\n使用带有名称的参数：\nf(_[\"x\"] = \"y\", _[\"value\"] = 1);\nAttributes\nC++可以使用.attr()访问R对象的属性。.names()可以直接获取name属性。下面是一个示例，注意可以使用类方法::create()由C++标量创建R向量。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericVector attribs() {\n  NumericVector out = NumericVector::create(1, 2, 3);\n\n  out.names() = CharacterVector::create(\"a\", \"b\", \"c\");\n  out.attr(\"my-attr\") = \"my-value\";\n  out.attr(\"class\") = \"my-class\";\n\n  return out;\n}\n\n对于S4对象，可以使用.slot()。",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#missing-values",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#missing-values",
    "title": "25 Rewriting R code in C++",
    "section": "Missing values",
    "text": "Missing values\n如果你想使用缺失值，你需要知道两件事：\n\nC++的标量类型如何处理缺失值。\nC++的向量类型如何获取和设置缺失值。\n\n\n\n\n\n\n\nNote\n\n\n\n注意：下面对数据类型的定义，有些是C++原生类型，有些是Rcpp自定义。Rcpp定义的类型可以完美地适配缺失值。\n\n\nScalars\n下面的示例展示了：将R的缺失值转换为C++标量，然后再转回为R向量。这是一种有效的方法，帮助我们弄清楚到底发生了什么。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList scalar_missings() {\n  int int_s = NA_INTEGER;\n  String chr_s = NA_STRING;\n  bool lgl_s = NA_LOGICAL;\n  double num_s = NA_REAL;\n\n  return List::create(int_s, chr_s, lgl_s, num_s);\n}\n\n\nstr(scalar_missings())\n#&gt; List of 4\n#&gt;  $ : int NA\n#&gt;  $ : chr NA\n#&gt;  $ : logi TRUE\n#&gt;  $ : num NA\n\n从结果来看，除了bool，其他类型的缺失值都正常。实际上，事情并不是如此简单。\nIntegers\n在C++中，整数缺失值被储存为最小整数。如果你不对它做任何处理，它会显得很正常。如果你对它做了一些处理，如NA_INTEGER + 1，因为C++并不知道最小整数在此处代表缺失值，它会进行计算返回值。\n\nevalCpp(\"NA_INTEGER + 1\")\n#&gt; [1] -2147483647\n\n如果你想应用整数缺失值，可以使用长度为1的IntegerVector或仔细控制代码对缺失值的处理。\nDoubles\n对于双精度浮点数，你或许可以不用理会缺失值，直接处理NaN（非数字）。这是因为R语言中的NA是 IEEE 754 浮点数 NaN 的一种特殊类型。因此，任何涉及NaN（在 C++ 中为 NAN）的逻辑表达式的求值结果都始终为 FALSE：\n\nevalCpp(\"NAN == 1\")\n#&gt; [1] FALSE\nevalCpp(\"NAN &lt; 1\")\n#&gt; [1] FALSE\nevalCpp(\"NAN &gt; 1\")\n#&gt; [1] FALSE\nevalCpp(\"NAN == NAN\")\n#&gt; [1] FALSE\n\n任何数字与NaNcy进行计算都会产生NaN：\n\nevalCpp(\"NAN + 1\")\n#&gt; [1] NaN\nevalCpp(\"NAN - 1\")\n#&gt; [1] NaN\nevalCpp(\"NAN / 1\")\n#&gt; [1] NaN\nevalCpp(\"NAN * 1\")\n#&gt; [1] NaN\n\n但是与布尔值计算时要小心：\n\nevalCpp(\"NAN && TRUE\")\n#&gt; [1] TRUE\nevalCpp(\"NAN || FALSE\")\n#&gt; [1] TRUE\n\nStrings\nString由“Rcpp”定义，能够正确处理缺失值。\nBoolean\nC++中的bool类型只有true和false，而R语言中的logical类型有TRUE、FALSE和NA，你可以使用int类型来实现缺失值（NA_INTEGER）。如果你要将一个长度为1的R-logical向量转换为C++的bool，需要注意缺失值，因为它会被转换为true。\nVectors\n对于向量，“Rcpp”有定义好的缺失值类型：NA_REAL、NA_INTEGER、NA_STRING、NA_LOGICAL。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList missing_sampler() {\n  return List::create(\n    NumericVector::create(NA_REAL),\n    IntegerVector::create(NA_INTEGER),\n    LogicalVector::create(NA_LOGICAL),\n    CharacterVector::create(NA_STRING)\n  );\n}\n\n\nstr(missing_sampler())\n#&gt; List of 4\n#&gt;  $ : num NA\n#&gt;  $ : int NA\n#&gt;  $ : logi NA\n#&gt;  $ : chr NA",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#standard-template-library",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#standard-template-library",
    "title": "25 Rewriting R code in C++",
    "section": "Standard Template Library",
    "text": "Standard Template Library\nC++真实的优势体现在它的标准模板库上，当你需要执行复杂算法时，STL提供了一套极其有用的数据结构和算法。本节将解释一些最重要的算法和数据结构，并为指明正确的学习方向，希望这些示例能向你展示STL的强大功能，并说服你相信学习更多知识是有用的。\n如果你需要的某个数据结构或算法不在STL中，你可以在boost中找找。一旦你在电脑上安装了boost，你可以在头文件中写下#include &lt;boost/test.hpp&gt;来使用boost中的算法。\nUsing iterators\n迭代器（iterator）在STL中应用广泛，许多函数接受或返回迭代器。它们时基础for-loop的升级，抽象出底层数据结构的细节。迭代器有三个主要操作符：\n\n\n*：返回迭代器指向的元素。\n\n++：将迭代器指向下一个元素。\n\n==：比较两个迭代器是否指向同一个元素。\n\n例如，我们使用迭代器重写“sum”函数：\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble sum3(NumericVector x) {\n  double total = 0;\n\n  NumericVector::iterator it;\n  for(it = x.begin(); it != x.end(); ++it) {\n    total += *it;\n  }\n  return total;\n}\n相较于for-loop，迭代器的主要改变有：\n\n循环开始于x.begin()，结束于x.end()。迭代器使用x.end储存了循环的末尾值，减少了每次迭代时进行查询末尾值的工作。\n使用解引符*获取迭代器指向的元素。\n每种数据类型有它自己的迭代器：NumericVector::iterator、IntegerVector::iterator、LogicalVector::iterator、CharacterVector::iterator。\n\n上面的代码可以使用C++11的range-based for-loop重写，Rcpp使用[[Rcpp::plugins(cpp11)]]激活：\n// [[Rcpp::plugins(cpp11)]]\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble sum4(NumericVector xs) {\n  double total = 0;\n\n  for(const auto &x : xs) {\n    total += x;\n  }\n  return total;\n}\n迭代器也允许我们使用C++中相当于apply的函数。例如，我们可以使用accumulate()函数再次重写sum()，该函数接受一个起始迭代器和一个结束迭代器，并将vector中的所有值相加。accumulate()的第三个参数给出了初始值：它特别重要，因为它决定了accumulate()使用的数据类型（使用0.0表示accumulate()使用的是double，使用0表示int）。要使用accumulate()，我们需要在头文件中引入&lt;Numeric&gt;。\n#include &lt;numeric&gt;\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble sum5(NumericVector x) {\n  return std::accumulate(x.begin(), x.end(), 0.0);\n}\nAlgorithms\n头文件&lt;algorithm&gt;提供了大量于使用迭代器的算法，你可以参考en.cppreference.com。\nfindInterval()函数接受一个向量和一个区间向量，返回一个向量，该向量包含每个元素在区间向量中的索引：\n\nx &lt;- 2:18\nv &lt;- c(5, 10, 15) # create two bins [5,10) and [10,15)\ncbind(x, findInterval(x, v))\n#&gt;        x  \n#&gt;  [1,]  2 0\n#&gt;  [2,]  3 0\n#&gt;  [3,]  4 0\n#&gt;  [4,]  5 1\n#&gt;  [5,]  6 1\n#&gt;  [6,]  7 1\n#&gt;  [7,]  8 1\n#&gt;  [8,]  9 1\n#&gt;  [9,] 10 2\n#&gt; [10,] 11 2\n#&gt; [11,] 12 2\n#&gt; [12,] 13 2\n#&gt; [13,] 14 2\n#&gt; [14,] 15 3\n#&gt; [15,] 16 3\n#&gt; [16,] 17 3\n#&gt; [17,] 18 3\n\n下面是Rcpp版的基础实现：\n#include &lt;algorithm&gt;\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nIntegerVector findInterval2(NumericVector x, NumericVector breaks) {\n  IntegerVector out(x.size());\n\n  NumericVector::iterator it, pos;\n  IntegerVector::iterator out_it;\n\n  for(it = x.begin(), out_it = out.begin(); it != x.end();\n      ++it, ++out_it) {\n    pos = std::upper_bound(breaks.begin(), breaks.end(), *it);\n    *out_it = std::distance(breaks.begin(), pos);\n  }\n\n  return out;\n}\n关键点：\n\n同时输入两个迭代器，it和out_it。\n使用*out_it来修改out的值。\nupper_bound()返回迭代器pos，然后使用std::distance()找到它在breaks中的索引。\ntips：R中的findInterval()已经是使用C实现的。如果想上面的函数和R中的一样快，我们需要计算一次对.begin()和.end()的调用，并保存结果。这样做会分散我们的注意力，所以忽略了这个优化。添加这一步骤可以生成比R的findInterval()更快的代码，但是代码量更少，大约是R底层的1/10。\n\n一般来说，使用ST 中的算法比使用手动循环更好。在《Effective STL》一书中，Scott Meyers 给出了三个原因：效率、正确性和可维护性。STL中的算法由C++专家编写，非常高效，而且它们已经存在了很长时间，并且经过了充分的测试。使用标准算法还能使代码的意图更加明确，有助于使其更加可读和可维护。\nData structures\nSTL提供了大量数据结构：array, bitset, list, forward_list, map, multimap, multiset, priority_queue, queue, deque, set, stack, unordered_map, unordered_set, unordered_multimap, unordered_multiset, vector. 其中最重要的是：vector, unordered_set, unordered_map。本节我们将重点介绍这三种数据结构，其他数据结构的使用方法相似，只是性能略有不同。你可以参考en.cppreference.com。\nRcpp知道如何将许多STL数据结构转换为R等价物，因此你可以从函数中返回它们，而无需显式转换为R数据结构。\nVectors\nSTL vector 与R中的vector 类似，但它能高效扩容，适合在你不知道输出向量的大小时使用。需要注意的是，vector是模板化的，你需要包含不同元素的向量进行声明：vector&lt;int&gt;、vector&lt;bool&gt;、vector&lt;double&gt;、vector&lt;String&gt;。你可以使用[]访问元素，使用.push_back()追加元素，使用.reserve()为向量分配储存空间。\nrle()函数用来执行游程编码算法，统计连续相同元素的个数，返回values和其对应的length：\n\nx &lt;- c(1,1,1,2,2,1,1,3,3,3,3,3,4,4)\nrle(x)\n#&gt; Run Length Encoding\n#&gt;   lengths: int [1:5] 3 2 2 5 2\n#&gt;   values : num [1:5] 1 2 1 3 4\n\n下面是Rcpp版：\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList rleC(NumericVector x) {\n  std::vector&lt;int&gt; lengths;\n  std::vector&lt;double&gt; values;\n\n  // Initialise first value\n  int i = 0;\n  double prev = x[0];\n  values.push_back(prev);\n  lengths.push_back(1);\n\n  NumericVector::iterator it;\n  for(it = x.begin() + 1; it != x.end(); ++it) {\n    if (prev == *it) {\n      lengths[i]++;\n    } else {\n      values.push_back(*it);\n      lengths.push_back(1);\n\n      i++;\n      prev = *it;\n    }\n  }\n\n  return List::create(\n    _[\"lengths\"] = lengths,\n    _[\"values\"] = values\n  );\n}\nvector中描述了vector的更多方法。\nSets\nset 包含的元素不能重复，可以有效解决查找重复或唯一元素的问题。C++提供了有序集合（std::set）和无序集合（std::unordered_set）两种数据结构。无序集合通常运行很快，有时可以使用无序集合+排序的两布走来替代有序集合。同vector一样，set也是模板化的，你需要包含不同元素的set进行声明：set&lt;int&gt;、set&lt;bool&gt;、set&lt;double&gt;、set&lt;String&gt;。更多信息你可以参考set和unordered_set。\n下面是R中duplicated()函数的Rcpp实现，注意seen.insert(x[i]).second.insert()返回一个pair值，它的.first值是一个指向元素的迭代器，.second值是一个判断是否新插入集合的布尔值：\n// [[Rcpp::plugins(cpp11)]]\n#include &lt;Rcpp.h&gt;\n#include &lt;unordered_set&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nLogicalVector duplicatedC(IntegerVector x) {\n  std::unordered_set&lt;int&gt; seen;\n  int n = x.size();\n  LogicalVector out(n);\n\n  for (int i = 0; i &lt; n; ++i) {\n    out[i] = !seen.insert(x[i]).second;\n  }\n\n  return out;\n}\nMap\nmap与set类似，但是map不仅可以用来判断元素是否存在，还关联其他数据，它由键值对组成。十分适合R中类似table()或match()等函数的实现。map也是模板化的，但是由于它同时有键和值，所以需要声明两个类型，例如：map&lt;int, int&gt;、map&lt;bool, int&gt;、map&lt;double, int&gt;、map&lt;String, int&gt;。map同样分有序（std::map）和无序（std::unordered_map）两种。更多信息你可以参考map和unordered_map。 ）。\n下面是R中table()函数的Rcpp实现：\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nstd::map&lt;double, int&gt; tableC(NumericVector x) {\n  std::map&lt;double, int&gt; counts;\n\n  int n = x.size();\n  for (int i = 0; i &lt; n; i++) {\n    counts[x[i]]++;\n  }\n\n  return counts;\n}",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#case-studies",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#case-studies",
    "title": "25 Rewriting R code in C++",
    "section": "Case studies",
    "text": "Case studies\n下面是一些使用C++替换R中运行缓慢函数的例子。\nGibbs sampler\n示例来源自Dirk Eddelbuettel的blog。R和C++的吉布斯采样函数实现类似，但是运行速度相差极大。Dirk在博客中还展示了另一种使速度更快的方法：使用GSL（R可以通过RcppGSL包轻访问）中更快的随机数生成器函数, 可以使速度再提高两到三倍。\nR版本：\n\ngibbs_r &lt;- function(N, thin) {\n  mat &lt;- matrix(nrow = N, ncol = 2)\n  x &lt;- y &lt;- 0\n\n  for (i in 1:N) {\n    for (j in 1:thin) {\n      x &lt;- rgamma(1, 3, y * y + 4)\n      y &lt;- rnorm(1, 1 / (x + 1), 1 / sqrt(2 * (x + 1)))\n    }\n    mat[i, ] &lt;- c(x, y)\n  }\n  mat\n}\n\nC++版本：\n\n声明所有变量的数据类型。\n使用(而不是[来获取矩阵索引。\n将rgamma()和rnorm()函数的结果由向量转换为标量。\n\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericMatrix gibbs_cpp(int N, int thin) {\n  NumericMatrix mat(N, 2);\n  double x = 0, y = 0;\n\n  for(int i = 0; i &lt; N; i++) {\n    for(int j = 0; j &lt; thin; j++) {\n      x = rgamma(1, 3, 1 / (y * y + 4))[0];\n      y = rnorm(1, 1 / (x + 1), 1 / sqrt(2 * (x + 1)))[0];\n    }\n    mat(i, 0) = x;\n    mat(i, 1) = y;\n  }\n\n  return(mat);\n}\n\n基准测试两个方法：\n\nbench::mark(\n  gibbs_r(100, 10),\n  gibbs_cpp(100, 10),\n  check = FALSE\n)\n#&gt; # A tibble: 2 × 6\n#&gt;   expression              min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;         &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 gibbs_r(100, 10)     1.99ms   2.14ms      441.   102.8KB     17.5\n#&gt; 2 gibbs_cpp(100, 10)  216.8µs 239.65µs     4000.    1.61KB     16.9\n\nR vectorisation versus C++ vectorisation\n示例来源子“Rcpp is smoking fast for agent-based models in data frames”。它的目的是通过三个输入来预测一个模型的输出：\n\nvacc1a &lt;- function(age, female, ily) {\n  p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily\n  p &lt;- p * if (female) 1.25 else 0.75\n  p &lt;- max(0, p)\n  p &lt;- min(1, p)\n  p\n}\n\n当三个输入是一个向量时，我们需要使用for-loop来处理：\n\nvacc1 &lt;- function(age, female, ily) {\n  n &lt;- length(age)\n  out &lt;- numeric(n)\n  for (i in seq_len(n)) {\n    out[i] &lt;- vacc1a(age[i], female[i], ily[i])\n  }\n  out\n}\n\n结合前面章节，我们可以判定vacc1()运行速度会是慢的。有两种方法可以解决：\n\n\n使用R中的向量化函数ifelse()、pmin()和pmax()来代替for-loop。\n\nvacc2 &lt;- function(age, female, ily) {\n  p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily\n  p &lt;- p * ifelse(female, 1.25, 0.75)\n  p &lt;- pmax(0, p)\n  p &lt;- pmin(1, p)\n  p\n}\n\n\n\n使用C++重构函数vacc1a()和vacc1()。\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\ndouble vacc3a(double age, bool female, bool ily){\n  double p = 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily;\n  p = p * (female ? 1.25 : 0.75);\n  p = std::max(p, 0.0);\n  p = std::min(p, 1.0);\n  return p;\n}\n\n// [[Rcpp::export]]\nNumericVector vacc3(NumericVector age, LogicalVector female,\n                    LogicalVector ily) {\n  int n = age.size();\n  NumericVector out(n);\n\n  for(int i = 0; i &lt; n; ++i) {\n    out[i] = vacc3a(age[i], female[i], ily[i]);\n  }\n\n  return out;\n}\n\n\n\n生成示例数据并进行基准测试：\n\nn &lt;- 1000\nage &lt;- rnorm(n, mean = 50, sd = 10)\nfemale &lt;- sample(c(T, F), n, rep = TRUE)\nily &lt;- sample(c(T, F), n, prob = c(0.8, 0.2), rep = TRUE)\n\nstopifnot(\n  all.equal(vacc1(age, female, ily), vacc2(age, female, ily)),\n  all.equal(vacc1(age, female, ily), vacc3(age, female, ily))\n)\n\nbench::mark(\n  vacc1 = vacc1(age, female, ily),\n  vacc2 = vacc2(age, female, ily),\n  vacc3 = vacc3(age, female, ily)\n)\n#&gt; # A tibble: 3 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 vacc1        1.17ms   1.25ms      768.    7.86KB    27.1 \n#&gt; 2 vacc2        64.5µs   73.1µs    12610.  146.67KB    45.2 \n#&gt; 3 vacc3        36.2µs   38.2µs    25475.   11.98KB     2.55",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#using-rcpp-in-a-package",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#using-rcpp-in-a-package",
    "title": "25 Rewriting R code in C++",
    "section": "Using Rcpp in a package",
    "text": "Using Rcpp in a package\nsourceCpp()函数可以将C++脚本嵌入R包中。将C++脚本绑定到R包中有下面三点好处：\n\n使用者可以脱离C++开发工具，直接使用C++编写的函数。\nR包构建系统自动处理多个源文件及其依赖关系。\n软件包为测试、文档和一致性提供了额外的基础工具。\n\n在R包中使用Rcpp函数，你需要：\n\n在scr/目录下放置C++源文件。\n\n在DESCRIPTION文件中添加：\nLinkingTo: Rcpp\nImports: Rcpp\n\n\n在NAMESPACE文件中添加：\nuseDynLib(mypackage)\nimportFrom(Rcpp, sourceCpp)\n我们需要从Rcpp导入一些东西（任何东西）, 以便内部Rcpp代码能够正确加载。这是R中的一个bug（不确定现在是否修复）。\n\n\n可以使用usethis::use_rcpp()来快捷地添加这些内容。\n在构建R包时，你需要先运行Rcpp::compileAttributes()。该函数扫描C++脚本中有Rcpp::export属性的函数，并生成使函数在R中可用所需的代码。每当添加、删除或更改函数签名时，都会重新运行compileAttributes()。这是由devtools包和Rstudio自动完成的。\n更多细节可以参考vignette(\"Rcpp-package\")。",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#learning-more",
    "href": "Books/Advanced R(2e)/25 Rewriting R code in C++.html#learning-more",
    "title": "25 Rewriting R code in C++",
    "section": "Learning more",
    "text": "Learning more\n本章只涉及了Rcpp的一小部分，提供了使用C++重写性能不佳的R代码的基本工具。如前所述，Rcpp还有许多其他功能，使得将R与现有C++代码接口变得容易，包括：\n\n属性的其他功能包括指定默认参数、链接外部C++依赖项以及从包导出C++接口。这些功能及更多内容将在vinette(\"Rcpp-attributes\")中介绍。\n自动创建C++数据结构和R数据结构之间的包装，包括将C++类映射到引用类。对这个主题的一个很好的介绍是vienette(\"Rcpp-modules\")。\nRcpp快速参考指南vignette(\"Rcpp-quickref\")包含了Rcpp类和常见编程习语的有用总结。\n\n可以关注一下Rcpp homepage或登录Rcpp mailing list以获取更多信息。\n下面是一些对学习C++很有帮助的资源：\n\nEffective C++、Effective STL。\nC++ Annotations\nAlgorithm Libraries\n\n编写高性能代码也可能需要你重新思考你的基本方法，对基本数据结构和算法的扎实理解非常有帮助。你可以参考：\n\nAlgorithm Design Manual\nMIT的Introduction to Algorithms\nAlgorithms 4th Edition，textbook。",
    "crumbs": [
      "25 Rewriting R code in C++"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html",
    "href": "Books/Advanced R(2e)/4 Subsetting.html",
    "title": "4 Subsetting",
    "section": "",
    "text": "R 中的提取子集操作上手很快，使用起来也很方便。但是想要掌握，需要理解并整合下面几点内容：\n\n有6种方法提取atomic向量的子集。\n有3种提取函数：[、[[和$。\n提取不同类型的对象，提取函数有不同的表现。\n提取函数可以搭配&lt;-来赋值。\n\n\n\n\n\n\n\nNote\n\n\n\n[、[[和$ 本质上是S3面向对象类型的函数\n\n\n\n\n\n4.2节：介绍[函数，及其在不同类型对象上的用法。\n4.3节：介绍[[和$函数，及其在不同类型对象上的用法。\n4.4节：介绍提取函数与&lt;-的搭配使用。\n4.5节：介绍8个实践案例。",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html#introduction",
    "href": "Books/Advanced R(2e)/4 Subsetting.html#introduction",
    "title": "4 Subsetting",
    "section": "",
    "text": "R 中的提取子集操作上手很快，使用起来也很方便。但是想要掌握，需要理解并整合下面几点内容：\n\n有6种方法提取atomic向量的子集。\n有3种提取函数：[、[[和$。\n提取不同类型的对象，提取函数有不同的表现。\n提取函数可以搭配&lt;-来赋值。\n\n\n\n\n\n\n\nNote\n\n\n\n[、[[和$ 本质上是S3面向对象类型的函数\n\n\n\n\n\n4.2节：介绍[函数，及其在不同类型对象上的用法。\n4.3节：介绍[[和$函数，及其在不同类型对象上的用法。\n4.4节：介绍提取函数与&lt;-的搭配使用。\n4.5节：介绍8个实践案例。",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html#selecting-multiple-elements",
    "href": "Books/Advanced R(2e)/4 Subsetting.html#selecting-multiple-elements",
    "title": "4 Subsetting",
    "section": "Selecting multiple elements",
    "text": "Selecting multiple elements\n\nAtomic vectors\n以提取atomic向量为例，介绍用作提取子集时的6种坐标：\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n\n正整数：表示元素在向量中的位置。\n\n\nx[c(3, 1)]\n#&gt; [1] 3.3 2.1\nx[order(x)]\n#&gt; [1] 2.1 3.3 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n#&gt; [1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n#&gt; [1] 4.2 4.2\n\n\n负整数：负号表示删除，正负不能同时存在。\n\n\nx[-c(3, 1)]\n#&gt; [1] 4.2 5.4\n\n\nx[c(-1, 2)]\n#&gt; Error in x[c(-1, 2)]: only 0's may be mixed with negative subscripts\n\n\n逻辑值：TRUE表示保留，FALSE表示删除，NA返回NA。在x[y]的模式中，如果二者长度不等，会发生循环，遵循R base中的循环原则：循环短的一方。\n\n\nx[c(TRUE, TRUE, FALSE, FALSE)]\n#&gt; [1] 2.1 4.2\nx[x &gt; 3]\n#&gt; [1] 4.2 3.3 5.4\nx[c(TRUE, NA, FALSE, TRUE)]\n#&gt; [1] 2.1  NA 5.4\n\n\nNothing：返回完整的对象，在后面对data.frame提取时有用。\n\n\nx[]\n#&gt; [1] 2.1 4.2 3.3 5.4\n\n\nZero：返回长度为0的向量。\n\n\nx[0]\n#&gt; numeric(0)\n\n\n字符串：有name属性的向量元素。\n\n\n(y &lt;- setNames(x, letters[1:4]))\n#&gt;   a   b   c   d \n#&gt; 2.1 4.2 3.3 5.4\ny[c(\"d\", \"c\", \"a\")]\n#&gt;   d   c   a \n#&gt; 5.4 3.3 2.1\n\n# Like integer indices, you can repeat indices\ny[c(\"a\", \"a\", \"a\")]\n#&gt;   a   a   a \n#&gt; 2.1 2.1 2.1\n\n# When subsetting with [, names are always matched exactly\nz &lt;- c(abc = 1, def = 2)\nz[c(\"a\", \"d\")]\n#&gt; &lt;NA&gt; &lt;NA&gt; \n#&gt;   NA   NA\n\n\n\n\n\n\n\nNote\n\n\n\n不要使用因子化的字符串向量提取子集，字符串向量因子化后，会视为整数。\n\ny[factor(\"b\")]\n#&gt;   a \n#&gt; 2.1\n\n\n\n\n\nLists\n[函数作用于list时，返回得仍然是一个list；[[和$函数作用于list时，返回得是list中的元素。\n\n\nMatrices and arrays\n对于多维的atomic Vector，只需要在每个维度上应用上述6种坐标，就可以得到子集。\n\na &lt;- matrix(1:9, nrow = 3)\ncolnames(a) &lt;- c(\"A\", \"B\", \"C\")\na[1:2, ]\n#&gt;      A B C\n#&gt; [1,] 1 4 7\n#&gt; [2,] 2 5 8\na[c(TRUE, FALSE, TRUE), c(\"B\", \"A\")]\n#&gt;      B A\n#&gt; [1,] 4 1\n#&gt; [2,] 6 3\na[0, -2]\n#&gt;      A C\n\n因为Matrices和Arrays是带有特殊属性的向量，所以仍然可以只使用一维的向量来提取，但要注意：它们都是列向量。\n\nvals &lt;- outer(1:5, 1:5, FUN = \"paste\", sep = \",\")\nvals\n#&gt;      [,1]  [,2]  [,3]  [,4]  [,5] \n#&gt; [1,] \"1,1\" \"1,2\" \"1,3\" \"1,4\" \"1,5\"\n#&gt; [2,] \"2,1\" \"2,2\" \"2,3\" \"2,4\" \"2,5\"\n#&gt; [3,] \"3,1\" \"3,2\" \"3,3\" \"3,4\" \"3,5\"\n#&gt; [4,] \"4,1\" \"4,2\" \"4,3\" \"4,4\" \"4,5\"\n#&gt; [5,] \"5,1\" \"5,2\" \"5,3\" \"5,4\" \"5,5\"\n\nvals[c(4, 15)]\n#&gt; [1] \"4,1\" \"5,3\"\n\n可以使用一个两列matrix提取2维Matrices，三列matrix提取3维Arrays；一行表示一个坐标，返回一个向量。\n\nselect &lt;- matrix(ncol = 2, byrow = TRUE, c(\n  1, 1,\n  3, 1,\n  2, 4\n))\nselect\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    1\n#&gt; [2,]    3    1\n#&gt; [3,]    2    4\nvals[select]\n#&gt; [1] \"1,1\" \"3,1\" \"2,4\"\n\n\n\nData frames and tibbles\nData.frame具有list和matrix的特性：\n\n当只提供一个index时，会将其视作list，返回列。\n当提供两个index时，将其视作matrix，返回矩阵。\n\n\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\ndf[df$x == 2, ]\n#&gt;   x y z\n#&gt; 2 2 2 b\ndf[c(1, 3), ]\n#&gt;   x y z\n#&gt; 1 1 3 a\n#&gt; 3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n#&gt;   x z\n#&gt; 1 1 a\n#&gt; 2 2 b\n#&gt; 3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n#&gt;   x z\n#&gt; 1 1 a\n#&gt; 2 2 b\n#&gt; 3 3 c\n\n\n# There's an important difference if you select a single\n# column: matrix subsetting simplifies by default, list\n# subsetting does not.\nstr(df[\"x\"])\n#&gt; 'data.frame':    3 obs. of  1 variable:\n#&gt;  $ x: int  1 2 3\nstr(df[, \"x\"])\n#&gt;  int [1:3] 1 2 3\n\n对tibble使用[，始终返回tibble。\n\ntib &lt;- tibble::tibble(x = 1:3, y = 3:1, z = letters[1:3])\n\nstr(tib[\"x\"])\n#&gt; tibble [3 × 1] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ x: int [1:3] 1 2 3\nstr(tib[, \"x\"])\n#&gt; tibble [3 × 1] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ x: int [1:3] 1 2 3\n\n\n\nPreserving dimensionality\n[函数有额外的参数drop用于控制是否在只有一列时降维，默认为TRUE。\n正如上面例子中的结果一样，data.frame在列方向上的index长度为1时，会发生降维：df[\"x\"]没有发生降维，df[, \"x\"]则发生了降维。\n\nstr(df[, \"x\", drop = FALSE])\n#&gt; 'data.frame':    3 obs. of  1 variable:\n#&gt;  $ x: int  1 2 3\n\nmatrix则表现为任意维度的index长度为1时，都会发生降维：\n\na &lt;- matrix(1:4, nrow = 2)\nstr(a[1, ])\n#&gt;  int [1:2] 1 3\n\nstr(a[1, , drop = FALSE])\n#&gt;  int [1, 1:2] 1 3\n\n在factor中使用[时，也有参数drop；但该参数的意义为：是否丢弃没有值的级别，默认为FALSE。\n\nz &lt;- factor(c(\"a\", \"b\"))\nz[1]\n#&gt; [1] a\n#&gt; Levels: a b\nz[1, drop = TRUE]\n#&gt; [1] a\n#&gt; Levels: a\n\n\n\nExercises\n…",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html#selecting-a-single-element",
    "href": "Books/Advanced R(2e)/4 Subsetting.html#selecting-a-single-element",
    "title": "4 Subsetting",
    "section": "Selecting a single element",
    "text": "Selecting a single element\n\n[[\n[[函数广泛应用在list或dataframe对象上，它与[函数的不同在于，返回得是降维的子对象。将一个list比作一列火车，[[返回的是火车中某车厢上的所有元素，[返回的是带有选取车厢的整列火车。\n\nx &lt;- list(1:3, \"a\", 4:6)\n\n\n\n\n使用[[函数时要注意：\n\n只能提供长度为1的整数或字符串作为index。\n当提供的长度大于1后，会递归地提取子对象。\n\n\nx1 &lt;- list(\n  1:3,\n  list( \"a\", \"b\"),\n  4:6\n)\n\nx1[[c(2,1)]]\n#&gt; [1] \"a\"\n# 等价于\nx1[[2]][[1]]\n#&gt; [1] \"a\"\n\n\n\n$\nx$y大致等于x[[\"y\", exact = FALSE]]。\n在使用$时，常见的错误是：使用当前环境变量中的某些变量来替代数据框或list中的name，此时推荐使用[[。\n\nvar &lt;- \"cyl\"\n# Doesn't work - mtcars$var translated to mtcars[[\"var\"]]\nmtcars$var\n#&gt; NULL\n\n# Instead use [[\nmtcars[[var]]\n#&gt;  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\n$与[[最大的不同是，$会自动地执行从左到右地部分匹配；可以添加options(warnPartialMatchDollar = TRUE)来添加提醒。\n\nx &lt;- list(abc = 1)\nx$a\n#&gt; [1] 1\nx[[\"a\"]]\n#&gt; NULL\n\n\noptions(warnPartialMatchDollar = TRUE)\nx$a\n#&gt; Warning in x$a: partial match of 'a' to 'abc'\n#&gt; [1] 1\n\n\n\nMissing and out-of-bounds indices\n在使用[[函数时，如果index无效，不同类型对象的结果不同。如下表中总结，三种类型的数据：atomic向量，list和NULL，四种无效的index：长度为0，超出范围（整数、字符串），缺失。\n\n\n\n\n\n\n\n\n\n\nrow[[col]]\nZero-length\nOut of bounds(Integer)\nOut of bounds(character)\nMissing\n\n\n\n\nAtomic\nError\nError\nError\nError\n\n\nList\nError\nError\nNULL\nNULL\n\n\nNULL\nNULL\nNULL\nNULL\nNULL\n\n\n\n\nx &lt;- setNames(1:3, letters[1:3])\ny &lt;- list(A = 1:3, B = 4:6, C = 7:9)\nz &lt;- NULL\n\nx[[NULL]]\nx[[4]]\nx[[\"d\"]]\nx[[NA]]\n\ny[[NULL]]\ny[[4]]\ny[[\"D\"]]\ny[[NA]]\n\nz[[NULL]]\nz[[4]]\nz[[\"D\"]]\nz[[NA]]\n\n从表中可以看出，[[的结果存在非一致性。purrr包提供了另外两种取子集的函数purrr::pluck(),purrr::chuck()。purrr::pluck()可以设置元素缺失时的默认返回值（默认为NULL），purrr::chuck()总是返回错误。pluck()也允许混合整数和字符串的index。pluck()的优点，使得其在处理结构化数据json或web api结果时非常有用。\n\nx &lt;- list(\n  a = list(1, 2, 3),\n  b = list(3, 4, 5)\n)\n\npurrr::pluck(x, \"a\", 1)\n#&gt; [1] 1\n\npurrr::pluck(x, \"c\", 1)\n#&gt; NULL\n\npurrr::pluck(x, \"c\", 1, .default = NA)\n#&gt; [1] NA\n\n\n\n@ and slot()\n@和slot()设计到S4面向对象系统，我们将在后面的章节中学习。\n\n\nExercises\n…",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html#subsetting-and-assignment",
    "href": "Books/Advanced R(2e)/4 Subsetting.html#subsetting-and-assignment",
    "title": "4 Subsetting",
    "section": "Subsetting and assignment",
    "text": "Subsetting and assignment\n查看函数说明文档，如果包含FUN(x) &lt;-形式的函数，就支持赋值。\n\nx &lt;- 1:5\nx[c(1, 2)] &lt;- c(101, 102)\nx\n#&gt; [1] 101 102   3   4   5\n\n在使用赋值前，一定要检查好提取的子集长度等于赋的值长度、子集index唯一。因为base R的循环原则，会使得结果完全不符合预期。\n对于list，可以使用x[[i]] &lt;- NULL删除某个元素，如果是增加一个内容为NULL的元素可以使用x[i] &lt;- list(NULL)。\n\nx &lt;- list(a = 1, b = 2)\nx[[\"b\"]] &lt;- NULL\nstr(x)\n#&gt; List of 1\n#&gt;  $ a: num 1\n\ny &lt;- list(a = 1, b = 2)\ny[\"b\"] &lt;- list(NULL)\nstr(y)\n#&gt; List of 2\n#&gt;  $ a: num 1\n#&gt;  $ b: NULL\n\n前面讲到，提取空元素对atomic向量没有太多用处，但对数据框有重要作用：它可以保持数据框的数据结构：\n\nmtcars[] &lt;- lapply(mtcars, as.integer)\nis.data.frame(mtcars)\n#&gt; [1] TRUE\n\nmtcars &lt;- lapply(mtcars, as.integer)\nis.data.frame(mtcars)\n#&gt; [1] FALSE",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/4 Subsetting.html#applications",
    "href": "Books/Advanced R(2e)/4 Subsetting.html#applications",
    "title": "4 Subsetting",
    "section": "Applications",
    "text": "Applications\n利用取子集的功能，你可以对数据框进行查找、拼接、排序、抽样、解压、删除等操作，下面是一些应用广泛的例子：\n\nLookup tables (character subsetting)\n创建查询表格，进行数据转换。\n\nx &lt;- c(\"m\", \"f\", \"u\", \"f\", \"f\", \"m\", \"m\")\nlookup &lt;- c(m = \"Male\", f = \"Female\", u = NA)\nlookup[x]\n#&gt;        m        f        u        f        f        m        m \n#&gt;   \"Male\" \"Female\"       NA \"Female\" \"Female\"   \"Male\"   \"Male\"\n\n# 去除name\nunname(lookup[x])\n#&gt; [1] \"Male\"   \"Female\" NA       \"Female\" \"Female\" \"Male\"   \"Male\"\n\n\n\nMatching and merging by hand (integer subsetting)\n使用match()函数与整数索引进行数据匹配和合并。\n\ngrades &lt;- c(1, 2, 2, 3, 1)\n\ninfo &lt;- data.frame(\n  grade = 3:1,\n  desc = c(\"Excellent\", \"Good\", \"Poor\"),\n  fail = c(F, F, T)\n)\n\nid &lt;- match(grades, info$grade)\nid\n#&gt; [1] 3 2 2 1 3\ninfo[id, ]\n#&gt;     grade      desc  fail\n#&gt; 3       1      Poor  TRUE\n#&gt; 2       2      Good FALSE\n#&gt; 2.1     2      Good FALSE\n#&gt; 1       3 Excellent FALSE\n#&gt; 3.1     1      Poor  TRUE\n\n\n\nRandom samples and bootstraps (integer subsetting)\n同上，使用整数索引，搭配sample()函数，模拟抽样与bootstrap。\n\ndf &lt;- data.frame(x = c(1, 2, 3, 1, 2), y = 5:1, z = letters[1:5])\n\n# Randomly reorder\ndf[sample(nrow(df)), ]\n#&gt;   x y z\n#&gt; 5 2 1 e\n#&gt; 3 3 3 c\n#&gt; 4 1 2 d\n#&gt; 1 1 5 a\n#&gt; 2 2 4 b\n\n# Select 3 random rows\ndf[sample(nrow(df), 3), ]\n#&gt;   x y z\n#&gt; 4 1 2 d\n#&gt; 2 2 4 b\n#&gt; 1 1 5 a\n\n# Select 6 bootstrap replicates\ndf[sample(nrow(df), 6, replace = TRUE), ]\n#&gt;     x y z\n#&gt; 5   2 1 e\n#&gt; 5.1 2 1 e\n#&gt; 5.2 2 1 e\n#&gt; 2   2 4 b\n#&gt; 3   3 3 c\n#&gt; 3.1 3 3 c\n\n\n\nOrdering (integer subsetting)\n同上，使用整数索引，搭配order()函数，对数据框排序。\n\n# Randomly reorder df\ndf2 &lt;- df[sample(nrow(df)), 3:1]\ndf2\n#&gt;   z y x\n#&gt; 5 e 1 2\n#&gt; 1 a 5 1\n#&gt; 4 d 2 1\n#&gt; 2 b 4 2\n#&gt; 3 c 3 3\n\ndf2[order(df2$x), ]\n#&gt;   z y x\n#&gt; 1 a 5 1\n#&gt; 4 d 2 1\n#&gt; 5 e 1 2\n#&gt; 2 b 4 2\n#&gt; 3 c 3 3\ndf2[, order(names(df2))]\n#&gt;   x y z\n#&gt; 5 2 1 e\n#&gt; 1 1 5 a\n#&gt; 4 1 2 d\n#&gt; 2 2 4 b\n#&gt; 3 3 3 c\n\n\n\nExpanding aggregated counts (integer subsetting)\n使用函数rep()，将行相同且具有重复数的数据框解压。\n\ndf &lt;- data.frame(x = c(2, 4, 1), y = c(9, 11, 6), n = c(3, 5, 1))\nrep(1:nrow(df), df$n)\n#&gt; [1] 1 1 1 2 2 2 2 2 3\n\ndf[rep(1:nrow(df), df$n), ]\n#&gt;     x  y n\n#&gt; 1   2  9 3\n#&gt; 1.1 2  9 3\n#&gt; 1.2 2  9 3\n#&gt; 2   4 11 5\n#&gt; 2.1 4 11 5\n#&gt; 2.2 4 11 5\n#&gt; 2.3 4 11 5\n#&gt; 2.4 4 11 5\n#&gt; 3   1  6 1\n\n\n\nRemoving columns from data frames (character)\n删除数据框的某列。\n\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf$z &lt;- NULL\n\n\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf[c(\"x\", \"y\")]\n#&gt;   x y\n#&gt; 1 1 3\n#&gt; 2 2 2\n#&gt; 3 3 1\n\ndf[setdiff(names(df), \"z\")]\n#&gt;   x y\n#&gt; 1 1 3\n#&gt; 2 2 2\n#&gt; 3 3 1\n\n\n\nSelecting rows based on a condition (logical subsetting)\n使用逻辑向量筛选数据框的行。\n\ndata(mtcars)\nmtcars[mtcars$gear == 5, ]\n#&gt;                 mpg cyl  disp  hp drat    wt qsec vs am gear carb\n#&gt; Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\n#&gt; Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\n#&gt; Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\n#&gt; Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\n#&gt; Maserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\n\nmtcars[mtcars$gear == 5 & mtcars$cyl == 4, ]\n#&gt;                mpg cyl  disp  hp drat    wt qsec vs am gear carb\n#&gt; Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\n#&gt; Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\n\n\n\nBoolean algebra versus sets (logical and integer )\nwhich()函数可以将布尔索引转换为整数索引。但x[which(y)]与x[y]仍然有一些差别：\n\n当布尔索引中存在缺失值NA时，对应得返回值也是NA。而which()会自动丢掉NA。\nx[-which(y)]与x[!y]不是等价的：如有y全部是FALSE，which(y)返回integer(0)，而-integer(0)仍然是integer(0)，最终前者返回一个空的向量，后者返回全部值。\n\n\nx &lt;- c(1, 2, 3, 4, NA, 5)\n\nx[which(x &gt; 2)]\n#&gt; [1] 3 4 5\nx[x &gt; 2]\n#&gt; [1]  3  4 NA  5\n\nx[-which(x &gt; 10)]\n#&gt; numeric(0)\nx[!x &gt; 10]\n#&gt; [1]  1  2  3  4 NA  5\n\n布尔向量的运算可以使用intersect()，`union()，setdiff()等函数进行等价替换。\n\n(x1 &lt;- 1:10 %% 2 == 0)\n#&gt;  [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n(x2 &lt;- which(x1))\n#&gt; [1]  2  4  6  8 10\n(y1 &lt;- 1:10 %% 5 == 0)\n#&gt;  [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n(y2 &lt;- which(y1))\n#&gt; [1]  5 10\n\n# X & Y &lt;-&gt; intersect(x, y)\nx1 & y1\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nintersect(x2, y2)\n#&gt; [1] 10\n\n# X | Y &lt;-&gt; union(x, y)\nx1 | y1\n#&gt;  [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\nunion(x2, y2)\n#&gt; [1]  2  4  6  8 10  5\n\n# X & !Y &lt;-&gt; setdiff(x, y)\nx1 & !y1\n#&gt;  [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\nsetdiff(x2, y2)\n#&gt; [1] 2 4 6 8\n\n# xor(X, Y) &lt;-&gt; setdiff(union(x, y), intersect(x, y))\nxor(x1, y1)\n#&gt;  [1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\nsetdiff(union(x2, y2), intersect(x2, y2))\n#&gt; [1] 2 4 6 8 5",
    "crumbs": [
      "4 Subsetting"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html",
    "href": "Books/Advanced R(2e)/6 Functions.html",
    "title": "6 Functions",
    "section": "",
    "text": "在学习本章之前，你一定已经写过很多用来减少重复工作的函数，本章会将你在工作中的一些知识进行整合提升，帮助你从理论的视角看待函数。在本章，你会看到一些有趣的小技巧和复杂技术，请一定要留心，这些内容是后面章节中的基石。\n\n\n\n下面的代码运行时会报错吗？\n\n\nf2 &lt;- function(a, b) {\n  a * 10\n}\nf2(10, stop(\"This is an error!\"))\n\n\n无论运行成功或失败，函数如何都执行一些操作？\n\n\n\n\n\n6.2节：构成函数的三要素和原始函数（primitive function）。\n6.3节：函数联合使用的三种方法及其优劣。\n6.4节：词法作用域规则（rules of lexical scoping）——如何根据名字找到对应的值。\n6.5节：参数评估原则：只在第一次使用时评估，避免循环。\n6.6节：特殊参数...。\n6.7节：函数退出机制。\n6.8节：四种函数格式。",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#introduction",
    "href": "Books/Advanced R(2e)/6 Functions.html#introduction",
    "title": "6 Functions",
    "section": "",
    "text": "在学习本章之前，你一定已经写过很多用来减少重复工作的函数，本章会将你在工作中的一些知识进行整合提升，帮助你从理论的视角看待函数。在本章，你会看到一些有趣的小技巧和复杂技术，请一定要留心，这些内容是后面章节中的基石。\n\n\n\n下面的代码运行时会报错吗？\n\n\nf2 &lt;- function(a, b) {\n  a * 10\n}\nf2(10, stop(\"This is an error!\"))\n\n\n无论运行成功或失败，函数如何都执行一些操作？\n\n\n\n\n\n6.2节：构成函数的三要素和原始函数（primitive function）。\n6.3节：函数联合使用的三种方法及其优劣。\n6.4节：词法作用域规则（rules of lexical scoping）——如何根据名字找到对应的值。\n6.5节：参数评估原则：只在第一次使用时评估，避免循环。\n6.6节：特殊参数...。\n6.7节：函数退出机制。\n6.8节：四种函数格式。",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#function-fundamentals",
    "href": "Books/Advanced R(2e)/6 Functions.html#function-fundamentals",
    "title": "6 Functions",
    "section": "Function fundamentals",
    "text": "Function fundamentals\n\nFunction components\n函数由三部分组成：\n\nformals()：参数和函数名，控制如何调用函数。\nbody()：函数具体实现。\nenvironment()：环境，决定函数如何找到参数对应的值。\n\n在这三部分中，formals和body十分显性，构建出函数时就可以直观地看到；environment相对隐性，需要使用函数environment()。\n\nf02 &lt;- function(x, y) {\n  # A comment\n  x + y\n}\n\nformals(f02)\n#&gt; $x\n#&gt; \n#&gt; \n#&gt; $y\n\nbody(f02)\n#&gt; {\n#&gt;     x + y\n#&gt; }\n\nenvironment(f02)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n与R中的其他对象一样，函数也会拥有一些属性。一个常见的属性是srcref（source reference），记录了函数的源代码，如下所示。\n\nattr(f02, \"srcref\")\n#&gt; NULL\n\n\n\nPrimitive functions\n原始函数（primitive function）是一个例外，不包含上面的三要素。\n\nsum\n#&gt; function (..., na.rm = FALSE)  .Primitive(\"sum\")\n`[`\n#&gt; .Primitive(\"[\")\n\n它们的base type不同。（base type 见第12章）\n\ntypeof(sum)\n#&gt; [1] \"builtin\"\ntypeof(`[`)\n#&gt; [1] \"special\"\n\n因为它们的底层是C语言，所以formals()、body()和environment()返回值都是NULL。\n\nformals(sum)\n#&gt; NULL\nbody(sum)\n#&gt; NULL\nenvironment(sum)\n#&gt; NULL\n\n\n\nFirst-class functions\n所谓的first-class，就是指函数本身就是对象，可以用来赋值给变量，可以传递给函数等等。创建函数只需要function()来定义，使用&lt;-进行绑定。\n如果一个函数没有绑定名字，那么它就是匿名函数。匿名函数通常用在lapply()等函数中，用来实现高效的数据处理。\n\nlapply(mtcars, function(x) length(unique(x)))\nFilter(function(x) !is.numeric(x), mtcars)\nintegrate(function(x) sin(x)^2, 0, pi)\n\n有关R函数的闭包特性，见第7章。\n\n\nInvoking a function\n使用函数的常见方式是myfun(param1, param2, ...)，如果你有了一组参数数据，可以使用do.call()来调用函数。\n\nargs &lt;- list(1:10, na.rm = TRUE)\ndo.call(mean, args)\n#&gt; [1] 5.5\n\n\n\nExercises\n\n使用is.function()来判断一个对象是否是函数。对于原始函数，使用is.primitive()。",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#function-composition",
    "href": "Books/Advanced R(2e)/6 Functions.html#function-composition",
    "title": "6 Functions",
    "section": "Function composition",
    "text": "Function composition\nbase R 提供了两种函数组合使用的方式。例如，当你想使用sqrt()和mean()计算某个群体的方差：\n\nx &lt;- runif(100)\nsquare &lt;- function(x) x^2\ndeviation &lt;- function(x) x - mean(x)\n\n第一种方法：\n\nsqrt(mean(square(deviation(x))))\n#&gt; [1] 0.2744786\n\n第二种方法：\n\nout &lt;- deviation(x)\nout &lt;- square(out)\nout &lt;- mean(out)\nout &lt;- sqrt(out)\nout\n#&gt; [1] 0.2744786\n\nmagrittr包提供了第三种方式——管道符%&gt;%（R 4.0 之后可以之间使用 |&gt; 代替）。\n\nlibrary(magrittr)\n\nx %&gt;%\n  deviation() %&gt;%\n  square() %&gt;%\n  mean() %&gt;%\n  sqrt()\n#&gt; [1] 0.2744786",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#lexical-scoping",
    "href": "Books/Advanced R(2e)/6 Functions.html#lexical-scoping",
    "title": "6 Functions",
    "section": "Lexical scoping",
    "text": "Lexical scoping\n在第二章中，我们讲到为对象命名；在这里，我们介绍它的反面，根据名字找到对象——词法作用域(lexical scoping)。\nR 的词法作用域遵循下面四个规则：\n\nName masking——函数内部的变量优先于函数外部的变量。\nFunctions versus variables——当某个函数与变量同名时，R自动找到对应的对象。\nA fresh start——函数每次执行都是独立的。\nDynamic lookup——函数只有在运行时才会查找对应的对象。\n\n\nName masking\n函数查找变量时，首先查找函数内的变量，然后再查找函数外，即——由内到外逐级查找，直至找到为止。\n\nx &lt;- 10\ny &lt;- 20\nz &lt;- 30\ng05 &lt;- function() {\n  x &lt;- 1\n  y &lt;- 2\n  c(x, y, z)\n}\ng05()\n#&gt; [1]  1  2 30\n\n\n\nFunctions versus variables\n诚如上述，函数也是普通的对象，在进行函数的查找时，遵循相同的规则。\n\ng07 &lt;- function(x) x + 1\ng08 &lt;- function() {\n  g07 &lt;- function(x) x + 100\n  g07(10)\n}\ng08()\n#&gt; [1] 110\n\n如果，函数名和变量名重复（函数和变量在不同的环境中）时，R会自动找到对应类型的对象，例如下面的g09。实际编写代码时，我们要尽可能避免这种情况的发生，因为十分歧义和迷惑。\n\ng09 &lt;- function(x) x + 100\ng10 &lt;- function() {\n  g09 &lt;- 10\n  g09(g09)\n}\ng10()\n#&gt; [1] 110\n\n\n\nA fresh start\n下面的例子，每次运行g11()结果都是相同的，因为每次运行函数时，都会创建一个变量域（环境），它们之间相互独立。\n\ng11 &lt;- function() {\n  if (!exists(\"a\")) {\n    a &lt;- 1\n  } else {\n    a &lt;- a + 1\n  }\n  a\n}\n\ng11()\n#&gt; [1] 1\ng11()\n#&gt; [1] 1\n\n当你使用a &lt;- g11()时，就打破了独立。\n\na &lt;- g11()\ng11()\n#&gt; [1] 2\ng11()\n#&gt; [1] 2\n\n\n\nDynamic lookup\n函数只有在运行时才会根据名字查找对象，也即前后环境不一致时，函数前后运行值也就不一样。\n\ng12 &lt;- function() x + 1\nx &lt;- 15\ng12()\n#&gt; [1] 16\n\nx &lt;- 20\ng12()\n#&gt; [1] 21\n\n使用codetools::findGlobals()可以里列出函数内的外部依赖项。\n\ncodetools::findGlobals(g12)\n#&gt; [1] \"+\" \"x\"\n\nenvironment(g12) &lt;- emptyenv()\ng12()\n#&gt; Error in x + 1: could not find function \"+\"\n\n\n\nExercises\n…",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#lazy-evaluation",
    "href": "Books/Advanced R(2e)/6 Functions.html#lazy-evaluation",
    "title": "6 Functions",
    "section": "Lazy evaluation",
    "text": "Lazy evaluation\nR 函数中的参数具有惰性评估（Lazy evaluation）特点：只有参数被使用时，才会对其进行评估（运行）。\n\nh01 &lt;- function(x) {\n  10\n}\nh01(stop(\"This is an error!\"))\n#&gt; [1] 10\n\n这一特性允许解析耗时的参数只有在函数运行且被调用时进行解析。\n\nPromises\n惰性评估由一种被称为promise或thunk的数据结构赋能（这种数据结构，本人也不是很了解，可以GPT一下）。\npromise有三种类型：\n\n表达式，如1 + 1。\n创建的环境，如：函数自己的变量域。\n\n\ny &lt;- 10\nh02 &lt;- function(x) {\n  y &lt;- 100\n  x + 1\n}\n\nh02(y)\n#&gt; [1] 11\nh02(y &lt;- 1000)\n#&gt; [1] 1001\ny\n#&gt; [1] 1000\n\n\n无需重复计算的值，如下面的message结果只打印一次。\n\n\ndouble &lt;- function(x) {\n  message(\"Calculating...\")\n  x * 2\n}\n\nh03 &lt;- function(x) {\n  c(x, x)\n}\n\nh03(double(20))\n#&gt; [1] 40 40\n\nx &lt;- double(20)\nh03(x)\n#&gt; [1] 40 40\n\npromise类型的惰性评估具有一种“薛定谔的猫”的特点，任何尝试用R去评估它的操作都会破坏其promise特性。\n\n\nDefault arguments\n惰性评估，允许函数在设置默认参数时，引用函数内部变量域，例如下面的例子。在base R中的许多函数都有类似的应用，但是不推荐这样使用，这样会增加函数理解的困难。\n\nh04 &lt;- function(x = 1, y = x * 2, z = a + b) {\n  a &lt;- 10\n  b &lt;- 100\n\n  c(x, y, z)\n}\n\nh04()\n#&gt; [1]   1   2 110\n\n另外一个需要注意的是：将要惰性评估的表达式，“作为默认参数”和“直接传递给函数”，是两种不同的情况。下面的示例中，ls()作为默认参数时，评估的是函数内部的变量域，而不是外部环境。\n\nh05 &lt;- function(x = ls()) {\n  a &lt;- 1\n  x\n}\n\n# ls() evaluated inside h05:\nh05()\n#&gt; [1] \"a\" \"x\"\n\nh05(ls())\n#&gt;  [1] \"a\"               \"args\"            \"deviation\"       \"double\"         \n#&gt;  [5] \"f02\"             \"g05\"             \"g07\"             \"g08\"            \n#&gt;  [9] \"g09\"             \"g10\"             \"g11\"             \"g12\"            \n#&gt; [13] \"h01\"             \"h02\"             \"h03\"             \"h04\"            \n#&gt; [17] \"h05\"             \"out\"             \"pandoc_dir\"      \"quarto_bin_path\"\n#&gt; [21] \"square\"          \"status\"          \"x\"               \"y\"              \n#&gt; [25] \"z\"\n\n\n\nMissing arguments\n函数missing()可以用来判断函数参数值来源，若来自于默认值，那么返回TRUE，否则返回FALSE。\n\nh06 &lt;- function(x = 10) {\n  list(missing(x), x)\n}\nstr(h06())\n#&gt; List of 2\n#&gt;  $ : logi TRUE\n#&gt;  $ : num 10\nstr(h06(10))\n#&gt; List of 2\n#&gt;  $ : logi FALSE\n#&gt;  $ : num 10\n\n\n\nExercises\n\n下面代码发生的过程：\n\npromisex = {y &lt;- 1; 2}在函数f1自己创建的环境中被评估，赋值1给y，返回数值2。\npromise最终的运行结果——2，赋值给函数参数x。\n因为Name masking，不使用函数默认值，使用先前赋值为1的y。\n因为函数内部评估不影响外部变量，所以最外面的y仍为10。\n\n\n\ny &lt;- 10\nf1 &lt;- function(x = {\n                 y &lt;- 1\n                 2\n               }, y = 0) {\n  c(x, y)\n}\nf1()\n#&gt; [1] 2 1\ny\n#&gt; [1] 10",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#dot-dot-dot",
    "href": "Books/Advanced R(2e)/6 Functions.html#dot-dot-dot",
    "title": "6 Functions",
    "section": "...(dot-dot-dot)",
    "text": "...(dot-dot-dot)\n...是R函数的一个特殊参数，它使得R函数可以有任意数目的参数。\n\n应用...\n...主要应用在下面两种情况：\n\n要传递额外参数给另外一个函数。\n\n\ni01 &lt;- function(y, z) {\n  list(y = y, z = z)\n}\n\ni02 &lt;- function(x, ...) {\n  i01(...)\n}\n\nstr(i02(x = 1, y = 2, z = 3))\n#&gt; List of 2\n#&gt;  $ y: num 2\n#&gt;  $ z: num 3\n\n# 常见的apply家族函数\nx &lt;- list(c(1, 3, NA), c(4, NA, 6))\nstr(lapply(x, mean, na.rm = TRUE))\n#&gt; List of 2\n#&gt;  $ : num 2\n#&gt;  $ : num 5\n\n\nS3面向对象中的方法函数，如下面的print()函数，对于不同类使用不同参数。S3面向对象详见第13章。\n\n\nprint(factor(letters), max.levels = 4)\n#&gt;  [1] a b c d e f g h i j k l m n o p q r s t u v w x y z\n#&gt; 26 Levels: a b c ... z\n\nprint(y ~ x, showEnv = TRUE)\n#&gt; y ~ x\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\n\n\n解析...\n\n可以使用..N的形式，来访问...中的第N个参数。\n\n\ni03 &lt;- function(...) {\n  list(first = ..1, third = ..3)\n}\nstr(i03(1, 2, 3))\n#&gt; List of 2\n#&gt;  $ first: num 1\n#&gt;  $ third: num 3\n\n\n可以使用list(...)将其转换为list，储存起来。\n\n\ni04 &lt;- function(...) {\n  list(...)\n}\nstr(i04(a = 1, b = 2))\n#&gt; List of 2\n#&gt;  $ a: num 1\n#&gt;  $ b: num 2\n\n\nrlang包提供了额外的解析方法。\n\n\nrlang::list2()\nrlang::enquos()",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#exiting-a-function",
    "href": "Books/Advanced R(2e)/6 Functions.html#exiting-a-function",
    "title": "6 Functions",
    "section": "Exiting a function",
    "text": "Exiting a function\n大多数函数的退出机制有两种：\n\n显性或隐性的返回一个值，表示运行成功。\n抛出错误信息，表示运行失败。\n\n\nImplicit versus explicit returns\n\n使用return()指定返回值。\n如果不使用return()指定返回值，默认使用最后运行代码的值作为返回值。\n\n\nj01 &lt;- function(x) {\n  if (x &lt; 10) {\n    0\n  } else {\n    10\n  }\n}\nj01(5)\n#&gt; [1] 0\nj01(15)\n#&gt; [1] 10\n\nj02 &lt;- function(x) {\n  if (x &lt; 10) {\n    return(0)\n  } else {\n    return(10)\n  }\n}\nj02(5)\n#&gt; [1] 0\nj02(15)\n#&gt; [1] 10\n\n\n\nInvisible values\n如果没有将函数的返回值赋值给某个变量，函数会将结果打印出来。\n\nj03 &lt;- function() 1\nx &lt;- j03()\nj03()\n#&gt; [1] 1\n\n使用invisible()可以阻止函数自动打印。\n\nj04 &lt;- function() invisible(1)\nj04()\nprint(j04())\n#&gt; [1] 1\n(j04())\n#&gt; [1] 1\n\n使用withVisible()可以获取函数的返回值和是否可见。\n\nstr(withVisible(j04()))\n#&gt; List of 2\n#&gt;  $ value  : num 1\n#&gt;  $ visible: logi FALSE\n\n最常见的隐藏返回值的函数就是&lt;-。\n\na &lt;- 2\n(a &lt;- 2)\n#&gt; [1] 2\n\n\n\nErrors\n当函数运行失败时，应当使用stop()函数抛出错误信息，并终止函数运行。抛出错误信息是为了让使用者知道函数运行失败的原因，以及如何处理。\n\nj05 &lt;- function() {\n  stop(\"I'm an error\")\n  return(10)\n}\nj05()\n#&gt; Error in j05(): I'm an error\n\n\n\nExit handlers\n在函数处理过程中，经常会有更新当前工作路径、绘图参数等全局变量，在函数运行结束后又要复原这些全局变量的操作。此时可以使用on.exit()函数来添加函数在退出时的操作。下面的示例显示了：无论函数运行成功还是失败，on.exit()函数都会执行。\n\nj06 &lt;- function(x) {\n  cat(\"Hello\\n\")\n  on.exit(cat(\"Goodbye!\\n\"), add = TRUE)\n\n  if (x) {\n    return(10)\n  } else {\n    stop(\"Error\")\n  }\n}\n\nj06(TRUE)\n#&gt; Hello\n#&gt; Goodbye!\n#&gt; [1] 10\n\nj06(FALSE)\n#&gt; Hello\n#&gt; Error in j06(FALSE): Error\n\non.exit()函数的另外两个参数：\n\nadd：当有多个退出操作时，如果add为FALSE则新的操作会覆盖原来的操作，推荐总是设置为TRUE。\nafter：当有多个退出操作时，如果after为FALSE，新的操作会最先执行。\n\n\nj08 &lt;- function() {\n  on.exit(message(\"a\"), add = TRUE)\n  on.exit(message(\"b\"), add = FALSE)\n}\nj08()\n\nj09 &lt;- function() {\n  on.exit(message(\"a\"), add = TRUE, after = TRUE)\n  on.exit(message(\"b\"), add = TRUE, after = TRUE)\n  on.exit(message(\"c\"), add = TRUE, after = FALSE)\n}\nj09()\n\n\n\nExercises\n了解一下sink(),capture.output()函数。",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/6 Functions.html#function-forms",
    "href": "Books/Advanced R(2e)/6 Functions.html#function-forms",
    "title": "6 Functions",
    "section": "Function forms",
    "text": "Function forms\n\n\n\n\n\n\nTip\n\n\n\nR 里面的两句slogan：\n\nEverything that exists is an object.\nEverything that happens is a function call. — John Chambers\n\n\n\nR 中的函数有四种变体：\n\nprefix：函数名在参数前，例如mean(x)。\ninfix：函数名在参数之间，例如x + y的+；可以使用%前后包裹函数名，进行自定义。\nreplacement：带有&lt;-赋值操作的函数，例如names(df) &lt;- c(\"a\", \"b\")。\nspecial：例如[[、if、for等。\n\n\nRewriting to prefix form\n任何形式的函数都可以改写成prefix形式。\n\nx + y\n`+`(x, y)\n\nnames(df) &lt;- c(\"x\", \"y\", \"z\")\n`names&lt;-`(df, c(\"x\", \"y\", \"z\"))\n\nfor (i in 1:10) print(i)\n`for`(i, 1:10, print(i))\n\nR 的这种特性，可以让你随意地更改R中的基本函数。下面是一个更新了(函数的例子，大约10次中有1次，返回值会加1。\n\n`(` &lt;- function(e1) {\n  if (is.numeric(e1) && runif(1) &lt; 0.1) {\n    e1 + 1\n  } else {\n    e1\n  }\n}\nreplicate(50, (1 + 2))\n#&gt;  [1] 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [37] 3 3 4 3 4 3 3 3 3 4 3 3 3 3\nrm(\"(\")\n\n\n\nPrefix form\nprefix 格式的函数可以通过下面三种方式检索参数（也是检索优先级，由高至低）：\n\n使用参数名。\n使用参数名部分匹配。\n使用参数位置。\n\n\nk01 &lt;- function(abcdef, bcde1, bcde2) {\n  list(a = abcdef, b1 = bcde1, b2 = bcde2)\n}\n\n\nstr(k01(1, 2, 3))\nstr(k01(2, 3, abcdef = 1)) 3\n\n# Can abbreviate long argument names:\nstr(k01(2, 3, a = 1))\n\n# But this doesn't work because abbreviation is ambiguous\nstr(k01(1, 3, b = 1))\n#&gt; Error in parse(text = input): &lt;text&gt;:2:28: unexpected numeric constant\n#&gt; 1: str(k01(1, 2, 3))\n#&gt; 2: str(k01(2, 3, abcdef = 1)) 3\n#&gt;                               ^\n\n通常使用位置的参数是函数参数中最常用的几个，不推荐使用部分匹配设置参数。很遗憾，在R中无法禁用模糊匹配，但可以设置options(warnPartialMatchArgs = TRUE)，生成警告信息。\n\noptions(warnPartialMatchArgs = TRUE)\nx &lt;- k01(a = 1, 2, 3)\n\n\n\nInfix form\ninfix 格式的函数要求只能有两个参数。在base R中有许多这种格式的函数：:, ::, :::, $, @, ^, *, /, +, -, &gt;, &gt;=, &lt;, &lt;=, ==, !=, !, &, &&, |, ||, ~, &lt;-, and &lt;&lt;-。也可以使用%前后包裹函数名，进行自定义。例如%*%,%in%等。\n\n`%+%` &lt;- function(a, b) paste0(a, b)\n\"new \" %+% \"string\"\n#&gt; [1] \"new string\"\n\n%之间的函数名可以是除%外的任意字符，需要转义的字符只需在定义时进行转义，使用时无须转义。\n\n`% %` &lt;- function(a, b) paste(a, b)\n`%/\\\\%` &lt;- function(a, b) paste(a, b)\n\n\"a\" % % \"b\"\n#&gt; [1] \"a b\"\n\"a\" %/\\% \"b\"\n#&gt; [1] \"a b\"\n\ninfix 格式的函数总是将其左右两端的参数作为输入。\n\n`%-%` &lt;- function(a, b) paste0(\"(\", a, \" %-% \", b, \")\")\n\"a\" %-% \"b\" %-% \"c\"\n#&gt; [1] \"((a %-% b) %-% c)\"\n\n\n\nReplacement form\nreplacement 格式的函数要求：\n\n至少两个参数，分别是待赋值的对象和值。\n必须返回更新值后的对象。\n\n\n`second&lt;-` &lt;- function(x, value) {\n  x[2] &lt;- value\n  x\n}\n\nx &lt;- 1:10\nsecond(x) &lt;- 5L\nx\n#&gt;  [1]  1  5  3  4  5  6  7  8  9 10\n\n如果你要添加额外参数，需要将其放置在x和value之间。\n\n`modify&lt;-` &lt;- function(x, position, value) {\n  x[position] &lt;- value\n  x\n}\nmodify(x, 1) &lt;- 10\nx\n#&gt;  [1] 10  5  3  4  5  6  7  8  9 10\n\n使用tracemem()追踪内存地址的变化。\n\nx &lt;- 1:10\ntracemem(x)\n#&gt; [1] \"&lt;000001511E91C240&gt;\"\n\nsecond(x) &lt;- 6L\n#&gt; tracemem[0x000001511e91c240 -&gt; 0x000001512074b538]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main \n#&gt; tracemem[0x000001512074b538 -&gt; 0x00000151207565d8]: second&lt;- eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main\n\n\n\nSepcial forms\n下面是一些特殊格式的函数和它转换为prefix后的格式\n\n\n\n\n\n\n\nspecial form\nprefix form\n\n\n\n\n(x)\n`(`(x)\n\n\n{x}\n`{`(x)\n\n\nx[i]\n`[`(x, i)\n\n\nx[[i]]\n`[[`(x, i)\n\n\nif (cond) true\n`if`(cond, true)\n\n\nif (cond) true else false\n`if`(cond, true, false)\n\n\nfor(var in seq) action\n`for`(var, seq, action)\n\n\nwhile (cond) action\n`while`(cond, action)\n\n\nrepeat expr\n`repeat`(expr)\n\n\nnext\n`next`()\n\n\nbreak\n`break`()\n\n\nfunction(arg1, arg2) {body}\n`function`(alist(arg1, arg2), body, env )\n\n\n\n\n\nExercises\n…",
    "crumbs": [
      "6 Functions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html",
    "href": "Books/Advanced R(2e)/8 Conditions.html",
    "title": "8 Conditions",
    "section": "",
    "text": "情况系统（Condition System）包含两部分，一方面是函数内部根据不同情况生成不同等级的信息，另一方面是函数使用者根据返回的信息进行不同的处理。\nR 提供了一个基于 Common Lisp 思想的非常强大的情况系统。本章介绍 R 情况系统的主要思想，以及一些实用工具，这些工具将使你的代码更加健壮。\n\n\n\n8.2 节介绍了情况系统的基本工具，并讨论了何时适合使用每种工具。\n8.3 节介绍最简单的情况处理工具：像try()和suppressMessages()这样的函数，它们会吞噬情况信息并阻止其达到顶层。\n8.4 节介绍了情况对象，以及两个基本的情况处理工具：用于错误情况的tryCatch()和用于其他一切的withCallingHandlers()。\n8.5 节展示了如何扩展内置情况对象，以存储情况处理程序可用于做出更明智决策的有用数据。\n8.6 节以一系列基于前面章节中提到的低级工具的实际应用程序作为本章的结尾。\n\n\n\n\n本章使用rlang包中的状态信号与处理函数。\n\nlibrary(rlang)",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#introduction",
    "href": "Books/Advanced R(2e)/8 Conditions.html#introduction",
    "title": "8 Conditions",
    "section": "",
    "text": "情况系统（Condition System）包含两部分，一方面是函数内部根据不同情况生成不同等级的信息，另一方面是函数使用者根据返回的信息进行不同的处理。\nR 提供了一个基于 Common Lisp 思想的非常强大的情况系统。本章介绍 R 情况系统的主要思想，以及一些实用工具，这些工具将使你的代码更加健壮。\n\n\n\n8.2 节介绍了情况系统的基本工具，并讨论了何时适合使用每种工具。\n8.3 节介绍最简单的情况处理工具：像try()和suppressMessages()这样的函数，它们会吞噬情况信息并阻止其达到顶层。\n8.4 节介绍了情况对象，以及两个基本的情况处理工具：用于错误情况的tryCatch()和用于其他一切的withCallingHandlers()。\n8.5 节展示了如何扩展内置情况对象，以存储情况处理程序可用于做出更明智决策的有用数据。\n8.6 节以一系列基于前面章节中提到的低级工具的实际应用程序作为本章的结尾。\n\n\n\n\n本章使用rlang包中的状态信号与处理函数。\n\nlibrary(rlang)",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#signalling-conditions",
    "href": "Books/Advanced R(2e)/8 Conditions.html#signalling-conditions",
    "title": "8 Conditions",
    "section": "Signalling conditions",
    "text": "Signalling conditions\nR 提供了三种情况信号：errors，warnings，messages。\n\nerror：最严重，表示函数无法继续执行，必须终止。\nwarning：次之，表示函数内部某些是错的，但是不影响函数运行。\nmessage：仅用于显示函数内某些动作的状态。\n\n情况系统的信息通常是瞩目的，例如加粗，红色等。\n\nstop(\"This is what an error looks like\")\n#&gt; Error: This is what an error looks like\n\nwarning(\"This is what a warning looks like\")\n#&gt; Warning: This is what a warning looks like\n\nmessage(\"This is what a message looks like\")\n#&gt; This is what a message looks like\n\n\nErrors\nbase R 通过stop()函数抛出错误信息。\n\nf &lt;- function() g()\ng &lt;- function() h()\nh &lt;- function() stop(\"This is an error!\")\n\nf()\n#&gt; Error in h(): This is an error!\n\nstop()函数有参数.call，控制是否进行调用栈朔源（traceback()也可以）。\n\nh &lt;- function() stop(\"This is an error!\", call. = FALSE)\nf()\n#&gt; Error: This is an error!\n\nrlang 包中的abort()与stop()等价，但其功能更加全面，后面我们会继续介绍它。\n\nh &lt;- function() abort(\"This is an error!\")\nf()\n#&gt; Error in `h()`:\n#&gt; ! This is an error!\n\n错误信息最好可以指出哪里处了问题，引导用户改进。但是编写好的错误信息很困难，因为错误通常发生在用户对函数有一个有缺陷的心理模型时。作为开发人员，很难想象用户会如何错误地思考你的函数，因此很难编写一条能够引导用户走向正确方向的信息。\n\n\nWarnings\n警告信息比错误信息弱，它表示程序某些地方出错，但不影响程序正常运行。函数内可以有多条警告信息。\n\nfw &lt;- function() {\n  cat(\"1\\n\")\n  warning(\"W1\")\n  cat(\"2\\n\")\n  warning(\"W2\")\n  cat(\"3\\n\")\n  warning(\"W3\")\n}\n\n与错误信息不同，警告信息默认在程序运行中缓存，结束后显示。\n\nfw()\n#&gt; 1\n#&gt; Warning in fw(): W1\n#&gt; 2\n#&gt; Warning in fw(): W2\n#&gt; 3\n#&gt; Warning in fw(): W3\n\noptions()可以设置警告信息的行为。\n\noptions(warn = 0)：默认设置。\noptions(warn = 1)：警告信息会立即显示。\noptions(warn = 0)：警告信息视作错误信息。\n\nwarning()函数同样有call.参数，建议设置为FALSE。rlang 中也有类似函数rlang::warn()。\nbase R 中的有些警告信息，作者认为改写为报错信息会给更好。例如：\n\nformals(1)\n#&gt; Warning in formals(fun): argument is not a function\n#&gt; NULL\n\nfile.remove(\"this-file-doesn't-exist\")\n#&gt; Warning in file.remove(\"this-file-doesn't-exist\"): cannot remove file\n#&gt; 'this-file-doesn't-exist', reason 'No such file or directory'\n#&gt; [1] FALSE\n\nlag(1:3, k = 1.5)\n#&gt; Warning in lag.default(1:3, k = 1.5): 'k' is not an integer\n#&gt; [1] 1 2 3\n#&gt; attr(,\"tsp\")\n#&gt; [1] -1  1  1\n\nas.numeric(c(\"18\", \"30\", \"50+\", \"345,678\"))\n#&gt; Warning: NAs introduced by coercion\n#&gt; [1] 18 30 NA NA\n\n有两种情况，使用警告信息会更好：\n\n当你升级了某个函数，但是不推荐使用它时，可以打印一个版本警告信息。\n当你确定可以通过警告信息提醒使用者正确使用函数时。\n\n\n\nMessages\n提示信息由message()函数生成，函数没有call.参数，生成的提示信息会实时打印在控制台。恰到好处的提示信息可以告诉使用者，你的程序运行到了哪里，此刻的运行状态是什么。\n\nfm &lt;- function() {\n  cat(\"1\\n\")\n  message(\"M1\")\n  cat(\"2\\n\")\n  message(\"M2\")\n  cat(\"3\\n\")\n  message(\"M3\")\n}\n\nfm()\n#&gt; 1\n#&gt; M1\n#&gt; 2\n#&gt; M2\n#&gt; 3\n#&gt; M3\n\n下面是一些使用提示信息的情况：\n\n当函数的默认参数值需要一些计算时，你需要告诉使用者计算的情况。例如ggplot中的binwidth参数，如果用户没有指定参数，ggplot会根据数据集自动计算一个合适的参数值。\n当函数调用了其他必要且耗时的函数时，你需要告诉使用者，你的程序正在做什么。\n当函数运行耗时特别长时，你需要提供一个进度条。\n为R包添加加载后的提示信息（使用packageStartupMessage()）。\n\n每个函数都应当有一个quiet = TRUE参数，用来禁用提示信息。\ncat()函数与messages()函数类似，但是cat()函数面向使用者，而messages()函数面向开发者。\n\ncat(\"Hi!\\n\")\n#&gt; Hi!\n\nmessage(\"Hi!\")\n#&gt; Hi!",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#ignoring-conditions",
    "href": "Books/Advanced R(2e)/8 Conditions.html#ignoring-conditions",
    "title": "8 Conditions",
    "section": "Ignoring conditions",
    "text": "Ignoring conditions\nbase R 中忽略三种信息的方法：\n\ntry()：忽略所有错误信息。\nsuppressWarnings()：忽略所有警告信息。\nsuppressMessages()：忽略所有提示信息。\n\n这三种方法的共同缺点是，无法忽略单一某条信息，而保证其他信息通过，它们的作用是全局的。\n\ntry()\n通常函数报错会停止运行，try() 函数可以忽略错误信息，让函数继续执行。\n\nf1 &lt;- function(x) {\n  log(x)\n  10\n}\nf1(\"x\")\n#&gt; Error in log(x): non-numeric argument to mathematical function\n\nf2 &lt;- function(x) {\n  try(log(x))\n  10\n}\nf2(\"a\")\n#&gt; Error in log(x) : non-numeric argument to mathematical function\n#&gt; [1] 10\n\n为了实现根据运行情况（成功或失败）返回不同的值时，不建议将try()的结果直接赋值给变量，而是事先定义变量，然后在try()内部进行赋值。除了try()函数，也可以使用更高级的tryCatch()函数。\n\n# 不推荐\ndefault &lt;- try(read.csv(\"possibly-bad-input.csv\"), silent = TRUE)\n#&gt; Warning in file(file, \"rt\"): cannot open file 'possibly-bad-input.csv': No\n#&gt; such file or directory\n# 推荐\ndefault &lt;- NULL\ntry(default &lt;- read.csv(\"possibly-bad-input.csv\"), silent = TRUE)\n#&gt; Warning in file(file, \"rt\"): cannot open file 'possibly-bad-input.csv': No\n#&gt; such file or directory\n\n\n\nsuppress*\n\nsuppressWarnings({\n  warning(\"Uhoh!\")\n  warning(\"Another warning\")\n  1\n})\n#&gt; [1] 1\n\nsuppressMessages({\n  message(\"Hello there\")\n  2\n})\n#&gt; [1] 2\n\nsuppressWarnings({\n  message(\"You can still see me\")\n  3\n})\n#&gt; You can still see me\n#&gt; [1] 3",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#handling-conditions",
    "href": "Books/Advanced R(2e)/8 Conditions.html#handling-conditions",
    "title": "8 Conditions",
    "section": "Handling conditions",
    "text": "Handling conditions\n每种情况都有默认行为：错误信息终止程序运行，警告信息在程序运行结束后打印，提示信息即时打印。情况处理系统允许我们暂时压制或补充这些默认行为。\nbase R 提供了两个函数 tryCatch() 和 withCallingHandlers() 来处理情况。前者在情况触发时进入到退出函数（exiting handlers），适合处理错误情况；后者在情况触发时会接着运行（calling handlers），适合处理警告和提示情况。\ntryCatch(\n  error = function(cnd) {\n    # code to run when error is thrown\n  },\n  code_to_run_while_handlers_are_active\n)\n\nwithCallingHandlers(\n  warning = function(cnd) {\n    # code to run when warning is signalled\n  },\n  message = function(cnd) {\n    # code to run when message is signalled\n  },\n  code_to_run_while_handlers_are_active\n)\n\nCondition objects\n每种情况触发时，都会创建一个不被我们看到的condition对象，使用rlang::catch_cnd()函数可以查看此对象。\n\n# cnd &lt;- stop(\"An error\") # 也可以，但是会显示报错信息\ncnd &lt;- catch_cnd(stop(\"An error\"))\nstr(cnd)\n#&gt; List of 2\n#&gt;  $ message: chr \"An error\"\n#&gt;  $ call   : language force(expr)\n#&gt;  - attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\nconditionMessage(cnd)\n#&gt; [1] \"An error\"\nconditionCall(cnd)\n#&gt; force(expr)\n\ncondition对象包含两个元素：\n\nmessage：长度为1的字符串，用来展示信息。可以使用conditionMessage()函数查看。\ncall：触发情况的函数调用，如果参数call. = FALSE则为NULL。可以使用conditionCall()函数查看。\n\n自定义的condition对象也可以包含其他元素。\n该对象同时具有class属性，表示对象属于S3类。\n\n\nExiting handlers\ntryCatch()函数通常用在错误情况处理中，能够覆盖默认的错误行为。例如下面的函数在错误时返回NA。\n\nf3 &lt;- function(x) {\n  tryCatch(\n    error = function(cnd) NA,\n    log(x)\n  )\n}\n\nf3(\"x\")\n#&gt; [1] NA\n\n如果情况没有被触发或不符合定义的condition对象，则会正常运行。\n\ntryCatch(\n  error = function(cnd) 10,\n  1 + 1\n)\n#&gt; [1] 2\n\ntryCatch(\n  error = function(cnd) 10,\n  {\n    message(\"Hi!\")\n    1 + 1\n  }\n)\n#&gt; Hi!\n#&gt; [1] 2\n\ntryCatch()定义的handler称作 exiting handler，因为在情况触发后，程序不会再运行触发情况的代码。\n\ntryCatch(\n  message = function(cnd) \"There\",\n  {\n    message(\"Here\")\n    stop(\"This code is never run!\")\n  }\n)\n#&gt; [1] \"There\"\n\n注意：定义的handler是一个函数，它的运行环境与外面代码的运行环境不用。\nhandler函数只接受一个参数——condition对象，可以提取对象中的信息，这对后续介绍的自定义condition对象十分有用。\n\ntryCatch(\n  error = function(cnd) {\n    paste0(\"--\", conditionMessage(cnd), \"--\")\n  },\n  stop(\"This is an error\")\n)\n#&gt; [1] \"--This is an error--\"\n\n\nfinally\ntryCatch()函数还有一个finally参数，接受一个代码块。其功能类似于on.exit()，无论情况是否触发，都会运行这段代码，通常用来清理缓存，删除临时文件或关闭链接等。\npath &lt;- tempfile()\ntryCatch(\n  {\n    writeLines(\"Hi!\", path)\n    # ...\n  },\n  finally = {\n    # always run\n    unlink(path)\n  }\n)\n\n\n\nCalling handlers\nwithCallingHandlers()函数通常用来处理警告或提示情况。与tryCatch()不同，代码触发情况后，会执行handler函数，待handler函数运行结束后接着运行。这好像中间插入了一段运行代码。\n下面是tryCatch()和withCallingHandlers()的比较：\n\ntryCatch(\n  message = function(cnd) cat(\"Caught a message!\\n\"),\n  {\n    message(\"Someone there?\")\n    message(\"Why, yes!\")\n  }\n)\n#&gt; Caught a message!\n\nwithCallingHandlers(\n  message = function(cnd) cat(\"Caught a message!\\n\"),\n  {\n    message(\"Someone there?\")\n    message(\"Why, yes!\")\n  }\n)\n#&gt; Caught a message!\n#&gt; Someone there?\n#&gt; Caught a message!\n#&gt; Why, yes!\n\nhandler函数按顺序执行，不必担心内部情况被捕捉，造成死循环。\n\nwithCallingHandlers(\n  message = function(cnd) message(\"Second message\"),\n  message(\"First message\")\n)\n#&gt; Second message\n#&gt; First message\n\n但是要注意：如果有多个handler函数，某些handler函数的情况可能会被其他handler函数捕获，要考虑handler函数的顺序。\n\nwithCallingHandlers( # (1)\n  message = function(cnd) message(\"b\"),\n  withCallingHandlers( # (2)\n    message = function(cnd) message(\"a\"),\n    message(\"c\")\n  )\n)\n#&gt; b\n#&gt; a\n#&gt; b\n#&gt; c\n\n\nmuffle\nwithCallingHandlers()中的handler函数也会返回值，但是与tryCatch()不同，它的返回值没有被使用，calling handler 函数只发挥了它的副作用。其中一个重要副作用就是屏蔽信息。\n当情况处理函数发生嵌套时，会自动触发父级handler函数。\n\n# Bubbles all the way up to default handler which generates the message\nwithCallingHandlers(\n  message = function(cnd) cat(\"Level 2\\n\"),\n  withCallingHandlers(\n    message = function(cnd) cat(\"Level 1\\n\"),\n    message(\"Hello\")\n  )\n)\n#&gt; Level 1\n#&gt; Level 2\n#&gt; Hello\n\n\n# Bubbles up to tryCatch\ntryCatch(\n  message = function(cnd) cat(\"Level 2\\n\"),\n  withCallingHandlers(\n    message = function(cnd) cat(\"Level 1\\n\"),\n    message(\"Hello\")\n  )\n)\n#&gt; Level 1\n#&gt; Level 2\n\n可以使用rlang::cnd_muffle()来屏蔽信息。\n\n# Muffles the default handler which prints the messages\nwithCallingHandlers(\n  message = function(cnd) {\n    cat(\"Level 2\\n\")\n    cnd_muffle(cnd)\n  },\n  withCallingHandlers(\n    message = function(cnd) cat(\"Level 1\\n\"),\n    message(\"Hello\")\n  )\n)\n#&gt; Level 1\n#&gt; Level 2\n\n# Muffles level 2 handler and the default handler\nwithCallingHandlers(\n  message = function(cnd) cat(\"Level 2\\n\"),\n  withCallingHandlers(\n    message = function(cnd) {\n      cat(\"Level 1\\n\")\n      cnd_muffle(cnd)\n    },\n    message(\"Hello\")\n  )\n)\n#&gt; Level 1",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#call-stacks",
    "href": "Books/Advanced R(2e)/8 Conditions.html#call-stacks",
    "title": "8 Conditions",
    "section": "Call stacks",
    "text": "Call stacks\nexiting handler 函数与 calling handler 函数的调用栈不同。\n\nf &lt;- function() g()\ng &lt;- function() h()\nh &lt;- function() message(\"!\")\n\ncalling handler 是在函数f()的调用栈中被调用\n\nwithCallingHandlers(\n  f(),\n  message = function(cnd) {\n    lobstr::cst()\n    cnd_muffle(cnd)\n  }\n)\n#&gt;      ▆\n#&gt;   1. ├─base::withCallingHandlers(...)\n#&gt;   2. ├─global f()\n#&gt;   3. │ └─global g()\n#&gt;   4. │   └─global h()\n#&gt;   5. │     └─base::message(\"!\")\n#&gt;   6. │       ├─base::withRestarts(...)\n#&gt;   7. │       │ └─base (local) withOneRestart(expr, restarts[[1L]])\n#&gt;   8. │       │   └─base (local) doWithOneRestart(return(expr), restart)\n#&gt;   9. │       └─base::signalCondition(cond)\n#&gt;  10. └─global `&lt;fn&gt;`(`&lt;smplMssg&gt;`)\n#&gt;  11.   └─lobstr::cst()\n\nexiting handler 是在函数tryCatch()的调用栈中被调用\n\ntryCatch(\n  f(),\n  message = function(cnd) lobstr::cst()\n)\n#&gt;     ▆\n#&gt;  1. └─base::tryCatch(f(), message = function(cnd) lobstr::cst())\n#&gt;  2.   └─base (local) tryCatchList(expr, classes, parentenv, handlers)\n#&gt;  3.     └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])\n#&gt;  4.       └─value[[3L]](cond)\n#&gt;  5.         └─lobstr::cst()\n\n\ncatch_cnd(stop(\"An error\"))\n#&gt; &lt;simpleError in force(expr): An error&gt;\ncatch_cnd(abort(\"An error\"))\n#&gt; &lt;error/rlang_error&gt;\n#&gt; Error:\n#&gt; ! An error\n#&gt; ---\n#&gt; Backtrace:\n#&gt; ▆",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#custom-conditions",
    "href": "Books/Advanced R(2e)/8 Conditions.html#custom-conditions",
    "title": "8 Conditions",
    "section": "Custom conditions",
    "text": "Custom conditions\nbase R 内置的condition对象包含的信息有限——message和call。rlang包提供了额外的函数——abort(),warn(),inform(),singal()，帮助自定义condition对象。它们的使用方法和base R 中的一样，例如：rlang::abort()通过参数class添加额外的类和附加信息。\n\nabort(\n  class = \"error_not_found\",\n  message = \"Path `blah.csv` not found\",\n  path = \"blah.csv\"\n)\n#&gt; Error:\n#&gt; ! Path `blah.csv` not found\n\n\nMotivation\n下面以base::log()函数为例，阐述自定义condition对象的优势。\n当参数不符合标准时，log()会返回一个错误。\n\nlog(letters)\n#&gt; Error in log(letters): non-numeric argument to mathematical function\nlog(1:10, base = letters)\n#&gt; Error in log(1:10, base = letters): non-numeric argument to mathematical function\n\n上面的报错信息不是很友好，因为它没有具体指出那个参数错误，原因是什么。我们可以进行下面的修改：\n\nmy_log &lt;- function(x, base = exp(1)) {\n  if (!is.numeric(x)) {\n    abort(paste0(\n      \"`x` must be a numeric vector; not \", typeof(x), \".\"\n    ))\n  }\n  if (!is.numeric(base)) {\n    abort(paste0(\n      \"`base` must be a numeric vector; not \", typeof(base), \".\"\n    ))\n  }\n\n  base::log(x, base = base)\n}\n\n\nmy_log(letters)\n#&gt; Error in `my_log()`:\n#&gt; ! `x` must be a numeric vector; not character.\nmy_log(1:10, base = letters)\n#&gt; Error in `my_log()`:\n#&gt; ! `base` must be a numeric vector; not character.\n\n现在的报错信息就显得用户友好了，但是对于开发者来说不够友好，所有关键信息都被储存在了报错信息中，我们无法函数式地编写这类报错。\n\n\nSignalling\n为了实现上述功能，我们先自定义一个abort()函数：函数通过glue::glue()将附加信息拼接到错误信息中，然后传递到abort()函数中。注意我们定义了一个新condition类型——error_bad_argument。\n\nabort_bad_argument &lt;- function(arg, must, not = NULL) {\n  msg &lt;- glue::glue(\"`{arg}` must {must}\")\n  if (!is.null(not)) {\n    not &lt;- typeof(not)\n    msg &lt;- glue::glue(\"{msg}; not {not}.\")\n  }\n\n  abort(\"error_bad_argument\",\n    message = msg,\n    arg = arg,\n    must = must,\n    not = not\n  )\n}\n\n不基于rlang包，也可以实现上面的功能：\n\nstop_custom &lt;- function(.subclass, message, call = NULL, ...) {\n  err &lt;- structure(\n    list(\n      message = message,\n      call = call,\n      ...\n    ),\n    class = c(.subclass, \"error\", \"condition\")\n  )\n  stop(err)\n}\n\nerr &lt;- catch_cnd(\n  stop_custom(\"error_new\", \"This is a custom error\", x = 10)\n)\nclass(err)\n#&gt; [1] \"error_new\" \"error\"     \"condition\"\nerr$x\n#&gt; [1] 10\n\n现在我们可以重新改写my_log()：\n\nmy_log &lt;- function(x, base = exp(1)) {\n  if (!is.numeric(x)) {\n    abort_bad_argument(\"x\", must = \"be numeric\", not = x)\n  }\n  if (!is.numeric(base)) {\n    abort_bad_argument(\"base\", must = \"be numeric\", not = base)\n  }\n\n  base::log(x, base = base)\n}\n\n\nmy_log(letters)\n#&gt; Error in `abort_bad_argument()`:\n#&gt; ! `x` must be numeric; not character.\nmy_log(1:10, base = letters)\n#&gt; Error in `abort_bad_argument()`:\n#&gt; ! `base` must be numeric; not character.\n\n\n\nHandling\n自定义的condition类十分利于编程。\n我们可以使用testthat中的函数来检测这个类包含的内容是否符合预期：\n\nlibrary(testthat)\n#&gt; \n#&gt; Attaching package: 'testthat'\n#&gt; The following objects are masked from 'package:rlang':\n#&gt; \n#&gt;     is_false, is_null, is_true\n\nerr &lt;- catch_cnd(my_log(\"a\"))\nexpect_s3_class(err, \"error_bad_argument\")\nexpect_equal(err$arg, \"x\")\nexpect_equal(err$not, \"character\")\n\n自定义的类也可以用在handler函数中：\n\ntryCatch(\n  error_bad_argument = function(cnd) \"bad_argument\",\n  error = function(cnd) \"other error\",\n  my_log(\"a\")\n)\n#&gt; [1] \"bad_argument\"\n\n需要注意的是，因为自定义的类属于子类，所以无法完美的进行类判断。handler函数的顺序会直接影响类的判断结果。\n\ntryCatch(\n  error = function(cnd) \"other error\",\n  error_bad_argument = function(cnd) \"bad_argument\",\n  my_log(\"a\")\n)\n#&gt; [1] \"other error\"",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/8 Conditions.html#applications",
    "href": "Books/Advanced R(2e)/8 Conditions.html#applications",
    "title": "8 Conditions",
    "section": "Applications",
    "text": "Applications\n本节介绍一些使用tryCatch()和withCallingHandlers()的常见模式。\n\nFailure value\ntryCatch()的errorhandler函数返回默认值。\n\nfail_with &lt;- function(expr, value = NULL) {\n  tryCatch(\n    error = function(cnd) value,\n    expr\n  )\n}\n\nfail_with(log(10), NA_real_)\n#&gt; [1] 2.302585\nfail_with(log(\"x\"), NA_real_)\n#&gt; [1] NA\n\n创建base::try()的类似函数try2()。\n\ntry2 &lt;- function(expr, silent = FALSE) {\n  tryCatch(\n    error = function(cnd) {\n      msg &lt;- conditionMessage(cnd)\n      if (!silent) {\n        message(\"Error: \", msg)\n      }\n      structure(msg, class = \"try-error\")\n    },\n    expr\n  )\n}\n\ntry2(1)\n#&gt; [1] 1\ntry2(stop(\"Hi\"))\n#&gt; Error: Hi\n#&gt; [1] \"Hi\"\n#&gt; attr(,\"class\")\n#&gt; [1] \"try-error\"\ntry2(stop(\"Hi\"), silent = TRUE)\n#&gt; [1] \"Hi\"\n#&gt; attr(,\"class\")\n#&gt; [1] \"try-error\"\n\n\n\nSuccess and failure values\n将上面的模式再进一步改写为：成功返回一个值，失败返回另一个值。\nfoo &lt;- function(expr) {\n  tryCatch(\n    error = function(cnd) error_val,\n    {\n      expr\n      success_val\n    }\n  )\n}\n也可以改写为检测是否成功：\ndoes_error &lt;- function(expr) {\n  tryCatch(\n    error = function(cnd) TRUE,\n    {\n      expr\n      FALSE\n    }\n  )\n}\n还可以捕获condtion对象，类似于rlang::catch_cnd()：\ncatch_cnd &lt;- function(expr) {\n  tryCatch(\n    condition = function(cnd) cnd,\n    {\n      expr\n      NULL\n    }\n  )\n}\n利用这一模式，我们可以创建一个try()变体，同时返回错误对象和结果：\n\nsafety &lt;- function(expr) {\n  tryCatch(\n    error = function(cnd) {\n      list(result = NULL, error = cnd)\n    },\n    list(result = expr, error = NULL)\n  )\n}\n\nstr(safety(1 + 10))\n#&gt; List of 2\n#&gt;  $ result: num 11\n#&gt;  $ error : NULL\nstr(safety(stop(\"Error!\")))\n#&gt; List of 2\n#&gt;  $ result: NULL\n#&gt;  $ error :List of 2\n#&gt;   ..$ message: chr \"Error!\"\n#&gt;   ..$ call   : language doTryCatch(return(expr), name, parentenv, handler)\n#&gt;   ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\n上面的safety()函数类似于purrr::safely()，我们将在11章中讨论它。\n\n\nResignal\n前面讲到可以通过options(warn = 2)将警告转为错误。但这种做法是全局修改，我们可以构造下面的函数，单独将警告转为错误。\n\nwarning2error &lt;- function(expr) {\n  withCallingHandlers(\n    warning = function(cnd) abort(conditionMessage(cnd)),\n    expr\n  )\n}\n\n\nwarning2error({\n  x &lt;- 2^4\n  warn(\"Hello\")\n})\n#&gt; Error:\n#&gt; ! Hello\n\n这个函数也可以用来查找那些经常出现但有不知道来源的信息，更多信息见22章。\n\n\nRecord\n修改handler函数，实现记录每条信息。\n\ncatch_cnds &lt;- function(expr) {\n  conds &lt;- list()\n  add_cond &lt;- function(cnd) {\n    conds &lt;&lt;- append(conds, list(cnd))\n    cnd_muffle(cnd)\n  }\n\n  withCallingHandlers(\n    message = add_cond,\n    warning = add_cond,\n    expr\n  )\n\n  conds\n}\n\ncatch_cnds({\n  inform(\"a\")\n  warn(\"b\")\n  inform(\"c\")\n})\n#&gt; [[1]]\n#&gt; &lt;message/rlang_message&gt;\n#&gt; Message:\n#&gt; a\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;warning/rlang_warning&gt;\n#&gt; Warning:\n#&gt; b\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;message/rlang_message&gt;\n#&gt; Message:\n#&gt; c\n\n如果想捕获错误信息，需要将withCallingHandlers()置于tryCatch()中。\n\ncatch_cnds &lt;- function(expr) {\n  conds &lt;- list()\n  add_cond &lt;- function(cnd) {\n    conds &lt;&lt;- append(conds, list(cnd))\n    cnd_muffle(cnd)\n  }\n\n  tryCatch(\n    error = function(cnd) {\n      conds &lt;&lt;- append(conds, list(cnd))\n    },\n    withCallingHandlers(\n      message = add_cond,\n      warning = add_cond,\n      expr\n    )\n  )\n\n  conds\n}\n\ncatch_cnds({\n  inform(\"a\")\n  warn(\"b\")\n  abort(\"C\")\n})\n#&gt; [[1]]\n#&gt; &lt;message/rlang_message&gt;\n#&gt; Message:\n#&gt; a\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;warning/rlang_warning&gt;\n#&gt; Warning:\n#&gt; b\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;error/rlang_error&gt;\n#&gt; Error:\n#&gt; ! C\n#&gt; ---\n#&gt; Backtrace:\n#&gt; ▆\n\n这种模式同时也是evaluate包的主要思想，该包用于knitr中。\n\n\nNo default behaviour\n使用rlang::signal()函数可以创建不基于message,warning,error的condition类。\n\nlog &lt;- function(message, level = c(\"info\", \"error\", \"fatal\")) {\n  level &lt;- match.arg(level)\n  signal(message, \"log\", level = level)\n}\n\n因为没有默认的handler函数，log()函数不会打印任何信息。\n\nlog(\"This code was run\")\n\n搭配withCallingHandlers()函数，可以定义一个log的handler函数。\n\nrecord_log &lt;- function(expr, path = stdout()) {\n  withCallingHandlers(\n    log = function(cnd) {\n      cat(\n        \"[\", cnd$level, \"] \", cnd$message, \"\\n\",\n        sep = \"\",\n        file = path, append = TRUE\n      )\n    },\n    expr\n  )\n}\n\nrecord_log(log(\"Hello\"))\n#&gt; [info] Hello\n\n也可以创建一个不显示某个日志级别信息的handler函数。\n\nignore_log_levels &lt;- function(expr, levels) {\n  withCallingHandlers(\n    log = function(cnd) {\n      if (cnd$level %in% levels) {\n        cnd_muffle(cnd)\n      }\n    },\n    expr\n  )\n}\n\nrecord_log(ignore_log_levels(log(\"Hello\"), \"info\"))\n\n如果你手动创建了一个condition对象，并且通过signalCondition()触发，rlang::cnd_muffle()将不会工作。需要搭配withRestarts()。\nwithRestarts(signalCondition(cond), muffle = function() NULL)",
    "crumbs": [
      "8 Conditions"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Foundations Introduction.html",
    "href": "Books/Advanced R(2e)/Foundations Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "第二章：object和它的name之间的区别。认识到它们的区别，你会在复制对象时更加的小心，因为这涉及内存的消耗，你会明白哪些操作是廉价的，哪些是昂贵的。\n第三章：vector的类型和属性(attributes)。\n第四章：子集操作。\n第五章：控制流，if,for,switch(),while等语句。\n第六章：函数function构建细节，运行机制，退出机制；lazy 语式。\n第七章：R环境的数据结构。\n第八章：控制如何输出错误，警告，信息等。\n\n\n\n\n Back to top",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Metaprogramming Introduction.html",
    "href": "Books/Advanced R(2e)/Metaprogramming Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "R 语言中最令人感兴趣的特性之一是元编程（metaprogramming）。这种思想认为：代码即数据，可以被其他代码审查和修改，这深深地影响了R。从低水平上看，它实现了我们只需library(purrr)而无需添加\"\"就可以加载R包；使plot(x, sin(x))能自动用x和sin(x)作为图的坐标轴名称。从高水平看，它允许我们使用y ~ x1 + x2来描述一个模型；将subset(df, x == y)转换为df[df$x == df$y, , drop = FALSE]；使用dplyr::filter(db, is.na(x))（当db是数据库链接时）生成SQL语句WHERE x IS NULL。\n与元编程关系密切的是非标准性评估（Non-standard Evaluation，NSE）。这个术语通常用来描述R函数的行为，但会导致两个方面的歧义。首先，NSE实际上是函数参数的一个属性，因此谈论NSE函数有点草率。其次，通过什么是非标准来定义某个东西会让人混淆，因此本书会使用更精确的词汇表。\n本书会着重于由“rlang”包提供的整洁评估（tidy evaluation）。这将使你能够专注于重要的想法，而不会被R历史中产生的怪癖要求所分散注意力。在用rlang介绍完每个重要的想法后，本书会回过头来讨论这些想法在base R 中是如何实现的。这种方法可能看起来有些落后，但它就像学习如何使用自动变速器而不是换挡杆开车一样：它让你在学习细节之前先关注大局。本书侧重于整洁评估的理论方面，因此你可以从头到尾完全理解它是如何运作的。\n我们会在下面五个章节中介绍元编程与整洁评估：\n\n17章：介绍整个元编程框架，简要了解了所有主要组成部分，以及它们如何组合在一起使用。\n18章：介绍如何将R代码用“tree”结构进行描述。你将学习如何可视化这些树，R的语法规则如何将线性字符序列转换为这些树，以及如何使用递归函数来处理代码树。\n19章：介绍来自rlang的工具，你可以用它们来捕获未经评估的函数参数。你还将学习准引用，它提供了一套取消引用输入的技术，使得从代码片段中轻松生成新树成为可能。\n20章：继续评估捕获的代码。在这里，你将学习一个重要的数据结构，即quosure, 它通过捕获要评估的代码和评估它的环境来确保正确的评估。本章将向你展示如何将所有片段组合在一起，以理解NSE在base R中的工作原理，以及如何编写类似subset()的函数。\n21章：介绍如何结合第一类环境（first-class environments）、词法作用域和元编程，将R代码翻译成其他语言，即HTML和LaTeX。\n\n\n\n\n Back to top",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/Advanced R(2e)/Techniques Introduction.html",
    "href": "Books/Advanced R(2e)/Techniques Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "最后四章涵盖了两种一般的编程技术：发现和修复 bug, 以及发现和修复性能问题。测量和提高性能的工具尤其重要，因为 R 并不是一种快速的语言。这并非偶然：R 是有意设计的，目的是让交互式数据分析对人类来说更容易，而不是让计算机尽可能快。虽然 R 比其他编程语言慢，但在大多数情况下，它已经足够快了。这些章节将帮助你处理 R 不再足够快的情况，无论是通过提高 R 代码的性能，还是切换到专门为性能设计的语言 C++。\n\n第 22 章讨论了调试，因为找到错误的根本原因可能会非常令人沮丧。幸运的是，R 拥有一些出色的调试工具，当这些工具与一个可靠的策略相结合时，你应该能够快速且相对轻松地找到大多数问题的根本原因。\n第 23 章重点关注性能测量。\n第 24 章将展示如何提高性能。\n第 25 章介绍如何结合C++。\n\n\n\n\n Back to top",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html",
    "href": "Books/R4DS2/26 Iteration.html",
    "title": "26 Iteration",
    "section": "",
    "text": "在tidyverse中有一些常见的循环操作，如：\n\n绘制分面——facet_wrap(),facet_grid()\n分组总结–group_by(),summarise()\nlist-column解压——unnest_wider(),unnest_longer()\n\n下面我们学习另外一些循环技巧。\n\nlibrary(tidyverse)",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#选择列",
    "href": "Books/R4DS2/26 Iteration.html#选择列",
    "title": "26 Iteration",
    "section": "选择列",
    "text": "选择列\n.cols控制选择哪几列，是一个tidy-select类，类似于dplyr中的select()，可以使用适配select()的函数，如starts_with()、contains()等。\n更多有过关于tidy-select的内容，可以参考?dplyr_tidy_select。\n\ndf &lt;- tibble(\n  grp = sample(2, 10, replace = TRUE),\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf |&gt;\n  group_by(grp) |&gt;\n  summarize(across(everything(), median))\n#&gt; # A tibble: 2 × 5\n#&gt;     grp       a       b     c     d\n#&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 -0.0935 -0.0163 0.363 0.364\n#&gt; 2     2  0.312  -0.0576 0.208 0.565",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#函数",
    "href": "Books/R4DS2/26 Iteration.html#函数",
    "title": "26 Iteration",
    "section": "函数",
    "text": "函数\n.fns参数提供across()进行call的函数，可以是 - 函数名，例如mean，注意此处函数后不能有() - purrr-style lambda，例如~mean(.x, na.rm = TRUE) - 上述两种函数组成的named list，例如list(mean = mean,n_miss = ~ sum(is.na(.x))\n\n\n\n函数名\n\n# 函数名后有`()`会报错。\ndf |&gt;\n  group_by(grp) |&gt;\n  summarize(across(everything(), median()))\n#&gt; Error in `summarize()`:\n#&gt; ℹ In argument: `across(everything(), median())`.\n#&gt; Caused by error in `median.default()`:\n#&gt; ! argument \"x\" is missing, with no default\n\n\n\n\n\npurrr-style lambda\n\nrnorm_na &lt;- function(n, n_na, mean = 0, sd = 1) {\n  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))\n}\n\ndf_miss &lt;- tibble(\n  a = rnorm_na(5, 1),\n  b = rnorm_na(5, 1),\n  c = rnorm_na(5, 2),\n  d = rnorm(5)\n)\n\ndf_miss |&gt;\n  summarize(\n    across(a:d, ~ median(., na.rm = TRUE)),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b      c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.139 -1.11 -0.387  1.15     5\n\ndf_miss |&gt;\n  summarize(\n    across(a:d, function(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b      c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.139 -1.11 -0.387  1.15     5\n\ndf_miss |&gt;\n  summarize(\n    across(a:d, \\(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b      c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.139 -1.11 -0.387  1.15     5\n\n\n\n\n\n函数list\n\ndf_miss |&gt;\n  summarize(\n    across(a:d, list(\n      median = \\(x) median(x, na.rm = TRUE),\n      n_miss = \\(x) sum(is.na(x))\n    )),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 9\n#&gt;   a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1    0.139        1    -1.11        1   -0.387        2     1.15        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#列名",
    "href": "Books/R4DS2/26 Iteration.html#列名",
    "title": "26 Iteration",
    "section": "列名",
    "text": "列名\n如果你仔细观察，上面例子中的列名类似于{.col}_{.fn}（这也是为什么函数list是要有name属性的原因）。\n.names参数控制列名定义规则，使用{.col}表示原始列名，{.fn}表示函数名；单个函数默认{.col}，多个函数默认{.col}_{.fn}。\nacross()在与mutate()联用时会将原来的列覆盖，因为.names()默认是{.col}，所以看不出来，当参数值改变后就会直观的体现出来：\n\n\n\n与mutate联用\n\ndf_miss |&gt;\n  mutate(\n    across(a:d, \\(x) coalesce(x, 0), .names = \"{.col}_na_zero\")\n  )\n#&gt; # A tibble: 5 × 8\n#&gt;        a      b      c     d a_na_zero b_na_zero c_na_zero d_na_zero\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1  0.434 -1.25  NA     1.60      0.434    -1.25      0         1.60 \n#&gt; 2 NA     -1.43  -0.297 0.776     0        -1.43     -0.297     0.776\n#&gt; 3 -0.156 -0.980 NA     1.15     -0.156    -0.980     0         1.15 \n#&gt; 4 -2.61  -0.683 -0.785 2.13     -2.61     -0.683    -0.785     2.13 \n#&gt; 5  1.11  NA     -0.387 0.704     1.11      0        -0.387     0.704",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#filter中的循环",
    "href": "Books/R4DS2/26 Iteration.html#filter中的循环",
    "title": "26 Iteration",
    "section": "filter()中的循环",
    "text": "filter()中的循环\ndplyr 提供了两个across()的变体:\n\nif_any()：至少有一个条件返回TRUE\nif_all()：所有条件返回TRUE\n\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n#&gt; # A tibble: 4 × 4\n#&gt;        a      b      c     d\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.434 -1.25  NA     1.60 \n#&gt; 2 NA     -1.43  -0.297 0.776\n#&gt; 3 -0.156 -0.980 NA     1.15 \n#&gt; 4  1.11  NA     -0.387 0.704\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#与piovt_longer对比",
    "href": "Books/R4DS2/26 Iteration.html#与piovt_longer对比",
    "title": "26 Iteration",
    "section": "与piovt_longer()对比",
    "text": "与piovt_longer()对比\n使用piovt_longer()可以实现across()，反之则不成立。例如下面的两个例子\n\n\n\nacross\n\ndf |&gt;\n  summarize(across(a:d, list(median = median, mean = mean)))\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\n\n\n\n\npivot_longer 复现\n\nlong &lt;- df |&gt;\n  pivot_longer(a:d) |&gt;\n  group_by(name) |&gt;\n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong |&gt;\n  pivot_wider(\n    names_from = name,\n    values_from = c(median, mean),\n    names_vary = \"slowest\",\n    names_glue = \"{name}_{.value}\"\n  )\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\n\n\n\n\n只能使用pivot_longer\n\ndf_paired &lt;- tibble(\n  a_val = rnorm(10),\n  a_wts = runif(10),\n  b_val = rnorm(10),\n  b_wts = runif(10),\n  c_val = rnorm(10),\n  c_wts = runif(10),\n  d_val = rnorm(10),\n  d_wts = runif(10)\n)\n\ndf_long &lt;- df_paired |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\"group\", \".value\"),\n    names_sep = \"_\"\n  )\ndf_long\n#&gt; # A tibble: 40 × 3\n#&gt;   group    val   wts\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 a      0.715 0.518\n#&gt; 2 b     -0.709 0.691\n#&gt; 3 c      0.718 0.216\n#&gt; 4 d     -0.217 0.733\n#&gt; 5 a     -1.09  0.979\n#&gt; 6 b     -0.209 0.675\n#&gt; # ℹ 34 more rows\n\ndf_long |&gt;\n  group_by(group) |&gt;\n  summarize(mean = weighted.mean(val, wts))\n#&gt; # A tibble: 4 × 2\n#&gt;   group    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a      0.126 \n#&gt; 2 b     -0.0704\n#&gt; 3 c     -0.360 \n#&gt; 4 d     -0.248",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#返回结果不同",
    "href": "Books/R4DS2/26 Iteration.html#返回结果不同",
    "title": "26 Iteration",
    "section": "返回结果不同",
    "text": "返回结果不同\nmap()函数会返回一个list，其变体则不同。\n\n返回输入值\nwalk()函数只能隐性返回输入，对.f生成的结果没有处理，这在循环绘图中十分便利。\nres &lt;- 1:4 |&gt;\n  walk(\\(x) {\n    hist(rnorm(100, x))\n  })\n\nres\n#&gt; [1] 1 2 3 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n返回结果结构与输入相同\nmodify()函数会输出一个与输入结构完全相同的结果，常用来修改元素类型。\n\n# Convert to characters\niris |&gt; str()\n#&gt; 'data.frame':    150 obs. of  5 variables:\n#&gt;  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#&gt;  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#&gt;  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#&gt;  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#&gt;  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\niris |&gt;\n  modify(as.character) |&gt;\n  str()\n#&gt; 'data.frame':    150 obs. of  5 variables:\n#&gt;  $ Sepal.Length: chr  \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n#&gt;  $ Sepal.Width : chr  \"3.5\" \"3\" \"3.2\" \"3.1\" ...\n#&gt;  $ Petal.Length: chr  \"1.4\" \"1.4\" \"1.3\" \"1.5\" ...\n#&gt;  $ Petal.Width : chr  \"0.2\" \"0.2\" \"0.2\" \"0.2\" ...\n#&gt;  $ Species     : chr  \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n其还有变体modify_if()，modify_at()，modify2()，imodify()，使用方法相似，具体差异同下面讲到的一样。\n\n\n返回原子向量\n这一类有map_lgl(),map_int(),map_dbl(),map_chr(),map_vec()；要求返回的原子向量类型为函数名后缀规定的类型，否则报错。例如：map_lgl()函数要求返回结果为由TRUE和FALSE构成的向量，否则报错。map_vec()函数提供了参数.ptype，可以指定返回结果的类型。\n\nnumbers &lt;- list(1, 2, 3, 4)\n\nmap_lgl(numbers, ~ .x %% 2 == 0)\n#&gt; [1] FALSE  TRUE FALSE  TRUE\n\nmap_lgl(numbers, ~ .x * 2)\n#&gt; Error in `map_lgl()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error:\n#&gt; ! Can't coerce from a number to a logical.\n\n\n\n返回数据框\n这一类有map_dfc(),map_dfr()，这两类函数在官方文档中已经不再推荐使用，而是使用map() + list_rbind()/list_cbind()的组合方式。\n\n# map ---------------------------------------------\n# Was:\nmtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  map(\\(df) lm(mpg ~ wt, data = df)) |&gt;\n  map_dfr(\\(mod) as.data.frame(t(as.matrix(coef(mod)))))\n#&gt;   (Intercept)        wt\n#&gt; 1    39.57120 -5.647025\n#&gt; 2    28.40884 -2.780106\n#&gt; 3    23.86803 -2.192438\n# Now:\nmtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  map(\\(df) lm(mpg ~ wt, data = df)) |&gt;\n  map(\\(mod) as.data.frame(t(as.matrix(coef(mod))))) |&gt;\n  list_rbind()\n#&gt;   (Intercept)        wt\n#&gt; 1    39.57120 -5.647025\n#&gt; 2    28.40884 -2.780106\n#&gt; 3    23.86803 -2.192438",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#输入元素不同",
    "href": "Books/R4DS2/26 Iteration.html#输入元素不同",
    "title": "26 Iteration",
    "section": "输入元素不同",
    "text": "输入元素不同\n这一类有：\n\n对不同位置元素操作：map_at(),map_if(),map_depth()。\n要求元素类型为长度是1的list：lmap()和其变体lmap_at(),lmap_if()。\n\n\nmap_at()\n通过参数.at，提供元素在输入中的位置或name属性，针对特定元素进行操作。\n\nl3 &lt;- list(\n  a = 1:3,\n  b = LETTERS[1:3],\n  c = 7:9\n)\n\nl3 |&gt;\n  map_at(.at = c(\"a\", \"c\"), ~ .x + 1)\n#&gt; $a\n#&gt; [1] 2 3 4\n#&gt; \n#&gt; $b\n#&gt; [1] \"A\" \"B\" \"C\"\n#&gt; \n#&gt; $c\n#&gt; [1]  8  9 10\n\n\n\nmap_if()\n通过参数.p，提供元素要满足的条件，针对特定元素进行操作，可以是判断函数，也可以是逻辑向量。\n\nl3 |&gt;\n  map_if(.p = is.numeric, ~ .x + 1)\n#&gt; $a\n#&gt; [1] 2 3 4\n#&gt; \n#&gt; $b\n#&gt; [1] \"A\" \"B\" \"C\"\n#&gt; \n#&gt; $c\n#&gt; [1]  8  9 10\n\n\n\nmap_depth()\n通过参数.depth，提供元素的深度，针对该深度处的元素进行操作。\n\nx &lt;- list(a = list(foo = 1:2, bar = 3:4), b = list(baz = 5:6))\nx |&gt; str()\n#&gt; List of 2\n#&gt;  $ a:List of 2\n#&gt;   ..$ foo: int [1:2] 1 2\n#&gt;   ..$ bar: int [1:2] 3 4\n#&gt;  $ b:List of 1\n#&gt;   ..$ baz: int [1:2] 5 6\n\nx |&gt;\n  map_depth(2, \\(y) paste(y, collapse = \"/\")) |&gt;\n  str()\n#&gt; List of 2\n#&gt;  $ a:List of 2\n#&gt;   ..$ foo: chr \"1/2\"\n#&gt;   ..$ bar: chr \"3/4\"\n#&gt;  $ b:List of 1\n#&gt;   ..$ baz: chr \"5/6\"\n\n# Equivalent to:\nx |&gt;\n  map(\\(y) map(y, \\(z) paste(z, collapse = \"/\"))) |&gt;\n  str()\n#&gt; List of 2\n#&gt;  $ a:List of 2\n#&gt;   ..$ foo: chr \"1/2\"\n#&gt;   ..$ bar: chr \"3/4\"\n#&gt;  $ b:List of 1\n#&gt;   ..$ baz: chr \"5/6\"\n\n\n\nlmap() 类\nlmap()与map()的不同之处在于，后者提取使用[[i]]来提取list中的元素，返回一个向量，前者使用[i]提取list中的元素，返回一个list。\n由于lmap()操作对象是list，所以它可以使用list中的name属性；在返回结果时，它会使用c()将list合并。下面是一个示例：\n\nadd_minus &lt;- function(x) {\n  res1 &lt;- lapply(x, function(y) y - 1)\n  names(res1) &lt;- paste0(names(x), \"_minus\")\n  res2 &lt;- lapply(x, function(y) y + 1)\n  names(res2) &lt;- paste0(names(x), \"_plus\")\n  c(res1, res2)\n}\n\n# The output size varies each time we map f()\ntest &lt;- list(a = 1:4, c = 8:9)\ntest |&gt;\n  lmap(add_minus) |&gt;\n  str()\n#&gt; List of 4\n#&gt;  $ a_minus: num [1:4] 0 1 2 3\n#&gt;  $ a_plus : num [1:4] 2 3 4 5\n#&gt;  $ c_minus: num [1:2] 7 8\n#&gt;  $ c_plus : num [1:2] 9 10\n\n所以，lmap()对参数.x，.f有不同的要求：\n\n.x：list 或 data.frame\n.f：该函数输入为长度是1的list，输出是任意长度的list。",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/26 Iteration.html#函数参数数目",
    "href": "Books/R4DS2/26 Iteration.html#函数参数数目",
    "title": "26 Iteration",
    "section": "函数参数数目",
    "text": "函数参数数目\n这一类函数有map2()、pmap()和map2()的变体imap()。这些函数与map()的使用方法类似，只是多提供了一组或多组.f函数的参数。同时这些函数都有*_lgl(),*_int(), *_dbl(), *_chr(), *_vec(), *_dfc(), *_dfr(),*walk()等变体，使用方法同上。\n\nimap()\nimap()是map2(x, names(x), ...)或map2(x, seq_along(x), ...)的简写，其余使用方法相同。\n\nset.seed(123)\nimap_chr(sample(10), paste)\n#&gt;  [1] \"3 1\"  \"10 2\" \"2 3\"  \"8 4\"  \"6 5\"  \"9 6\"  \"1 7\"  \"7 8\"  \"5 9\"  \"4 10\"",
    "crumbs": [
      "26 Iteration"
    ]
  },
  {
    "objectID": "Books/R4DS2/index.html",
    "href": "Books/R4DS2/index.html",
    "title": "index",
    "section": "",
    "text": "本篇为书籍R for Data Science的学习笔记。\n原文见：R for Data Science (2e)\n\n\n\n Back to top",
    "crumbs": [
      "index"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html",
    "href": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html",
    "title": "2 A Tidyverse Primer",
    "section": "",
    "text": "什么是tidyverse，tidymodels框架又在其中扮演什么角色呢？tidyverse是一系列R数据分析包的集合，这些包是基于共同的理念和规范开发的。用Wickham等人的话讲：\n“At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next.”\n在本章中，我们简要讨论tidyverse设计理念的重要原则，以及这些原则如何应用于易于正确使用且支持良好统计实践的建模软件中。下一章将介绍base R中的建模惯例。通过这些讨论，你可以理解tidyverse、tidymodels与base R之间的关系：tidymodels和tidyverse都建立在base R之上，而tidymodels则将tidyverse的原则应用于模型构建。",
    "crumbs": [
      "2 A Tidyverse Primer"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#tidyverse-principles",
    "href": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#tidyverse-principles",
    "title": "2 A Tidyverse Primer",
    "section": "Tidyverse Principles",
    "text": "Tidyverse Principles\n有关以tidyverse风格编写R代码的全套策略和技巧，可以在网站https://design.tidyverse.org上找到。在这里，我们可以简要介绍tidyverse的几个通用设计原则、其背后的动机，以及我们如何将建模视为这些原则的一种应用。\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.1.0     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nDesign for humans\ntidyverse 致力于设计易于广大人群理解和使用的R包及函数。无论是过去还是现在，很大一部分R用户并非是创建软件或工具的人，而是进行分析或建模的人。因此，R用户通常没有（也不需要）计算机科学背景，而且许多人对编写自己的R包并不感兴趣。\n出于这个原因，R代码必须易于操作以实现你的目标，这一点至关重要。文档、培训、可及性以及其他因素在实现这一目标中发挥着重要作用。然而，如果语法本身难以被人们轻松理解，那么文档就不是一个好的解决方案。软件本身必须具有直观性。\n为了将tidyverse方法与更传统的R语义进行对比，我们来考虑对数据框进行排序。数据框的每一列可以表示不同类型的数据，每一行可以包含多个值。仅使用核心语言时，我们可以结合R的抽取规则和order()函数，通过重新排列行的顺序，依据一个或多个列对数据框进行排序；在这种情况下，你可能会因为一个函数的名称而想用它，但实际上用sort()是无法成功的。要依据mtcars数据中的两列对其进行排序，调用方式可能如下：\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n虽然计算效率很高，但很难说这是一个直观的用户界面。相比之下，在dplyr中，tidyverse函数arrange()直接将一组变量名作为输入参数：\nlibrary(dplyr)\narrange(.data = mtcars, gear, mpg)\n这里使用的变量名是“未加引号的”；许多传统的R函数需要用字符串来指定变量，但tidyverse函数接受未加引号的名称或选择器函数。这些选择器允许应用一个或多个易懂的规则到列名上。例如，ends_with(\"t\")会选择mtcars数据框中的drat列和wt列。\n此外，命名至关重要。如果你是R语言新手，正在编写涉及线性代数的数据分析或建模代码，那么在寻找计算矩阵逆的函数时可能会遇到困难。使用apropos(\"inv\")不会得到任何候选结果。事实证明，base R中用于此任务的函数是solve()，它用于求解线性方程组。对于矩阵X，你可以使用solve(X)来求X的逆（此时方程右侧没有向量）。这一点仅在帮助文件中某个参数的描述中有所说明。本质上，你需要知道解决方案的名称才能找到该解决方案。\ntidyverse的做法是使用描述性强且明确的函数名，而非简短且隐晦的函数名。通用方法侧重于动词（例如，fit、arrange等）。动词-名词组合尤其有效，不妨考虑将invert_matrix()作为一个假设的函数名。在建模的语境下，避免使用高度专业的术语也很重要，比如希腊字母或生僻词汇。函数名应尽可能做到自说明。\n当一个包中存在类似的函数时，函数名的设计会针对制表符补全进行优化。例如，glue包中有一系列函数都以一个共同的前缀（glue_）开头，这能让用户快速找到他们想要的函数。\nReuse existing data structures\n只要有可能，函数应避免返回全新的数据结构。如果结果适合现有的数据结构，就应使用该数据结构。这会减轻使用软件时的认知负担，无需额外的语法或方法。\n数据框是tidyverse和tidymodels包中首选的数据结构，因为其结构非常适合大量的数据科学任务。具体来说，tidyverse和tidymodels更倾向于使用tibble，这是对R语言数据框的现代重塑，我们将在下一节关于tidyverse语法示例的内容中对其进行介绍。\n例如，rsample包可用于创建数据集的重采样，如交叉验证或自助法（在第10章中描述）。重采样函数返回一个tibble，其中包含一个名为splits的列，该列中的对象定义了重采样后的数据集。一个数据集的三个自助样本可能如下所示：\n\nboot_samp &lt;- rsample::bootstraps(mtcars, times = 3)\nboot_samp\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 3 × 2\n#&gt;   splits          id        \n#&gt;   &lt;list&gt;          &lt;chr&gt;     \n#&gt; 1 &lt;split [32/12]&gt; Bootstrap1\n#&gt; 2 &lt;split [32/12]&gt; Bootstrap2\n#&gt; 3 &lt;split [32/12]&gt; Bootstrap3\nclass(boot_samp)\n#&gt; [1] \"bootstraps\" \"rset\"       \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n通过这种方法，基于向量的函数可以用于这些列，例如vapply()或purrr::map()。这个boot_samp对象有多个类，但继承了数据框（\"data.frame\"）和 tibble（\"tbl_df\"）的方法。此外，可以向结果中添加新列，而不会影响数据的类。与完全新的、其数据结构不明确的对象类型相比，这对用户来说更容易使用，也更具通用性。\n依赖常见数据结构的一个缺点是可能会损失计算性能。在某些情况下，数据可以用专门的格式进行编码，这些格式能更高效地表示数据。例如：\n\n在计算化学中，结构数据文件格式（SDF）是一种工具，它能获取化学结构并将其编码为便于计算处理的格式。\n具有大量相同值（例如二进制数据中的零）的数据可以存储在稀疏矩阵格式中。这种格式不仅可以减小数据的大小，还能启用更高效的计算技术。\n\n当问题范围明确，且潜在的数据处理方法既定义清晰又适合这种格式时，这些格式就具有优势。然而，一旦这些约束条件被违反，专用数据格式的用处就会减小。例如，如果我们对数据进行转换，将其转换为分数，那么输出就不再是稀疏的；稀疏矩阵表示有助于建模中的某个特定算法步骤，但在该特定步骤之前或之后，情况往往并非如此。专用数据结构不像通用数据结构那样，足以灵活应对整个建模工作流程。\nrsample生成的tibble中有一个重要特征，即splits列是一个列表。在这种情况下，该列表的每个元素都属于同一类型的对象——rsplit对象，其中包含关于mtcars的哪些行属于自助抽样样本的信息。列表列在数据分析中非常有用，并且正如本书通篇将要介绍的那样，它们对tidymodels而言也很重要。\nDesign for the pipe\nmagrittr包中的管道符%&gt;%(R 4.0版本后引入|&gt;)是一种将一系列R函数链接在一起的工具。为了说明这一点，考虑以下命令，对数据框进行排序，然后保留前10行：\nsmall_mtcars &lt;- arrange(mtcars, gear)\nsmall_mtcars &lt;- slice(small_mtcars, 1:10)\n\n# or more compactly:\nsmall_mtcars &lt;- slice(arrange(mtcars, gear), 1:10)\n管道符可以将左侧函数的运行结果替换为右侧函数的第一个参数，因此我们可以通过以下方式实现与之前相同的结果：\nsmall_mtcars &lt;-\n  mtcars %&gt;%\n  arrange(gear) %&gt;%\n  slice(1:10)\n这一系列函数的管道符版本更具可读性；随着更多操作被添加，这种可读性会进一步提高。这一系列函数可以被管道符串联起来，是因为所有函数都返回相同的数据结构（一个数据框），而这个数据结构随后会成为下一个函数的第一个参数。这是有意设计的，在可能的情况下，应创建能够整合到管道符中的函数。\n管道符在建模工作流中非常有用，不过建模时管道符间传递的可以不是数据框，而是诸如模型组件之类的对象。\nDesign for functional programming\nR语言拥有出色的工具来创建、修改函数以及对函数进行操作，这使其成为一门非常适合函数式编程的语言。这种方法在很多情况下可以替代迭代循环，例如当函数返回一个值且没有其他副作用时。\n让我们来看一个例子。假设你对燃油效率与汽车重量之比的对数感兴趣。对于那些刚接触R语言或者从其他编程语言转过来的人来说，循环可能看起来是个不错的选择：\n\nn &lt;- nrow(mtcars)\nratios &lt;- rep(NA_real_, n)\nfor (car in 1:n) {\n  ratios[car] &lt;- log(mtcars$mpg[car] / mtcars$wt[car])\n}\nhead(ratios)\n#&gt; [1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\n那些在R语言方面有更多经验的人可能知道，有一种更简单、更快的向量化版本可以通过以下方式计算：\n\nratios &lt;- log(mtcars$mpg / mtcars$wt)\n\n然而，在许多现实世界的案例中，我们所关注的逐元素运算对于矢量化解决方案来说过于复杂。在这种情况下，一个好的方法是编写一个函数来进行计算。当我们进行函数式编程设计时，重要的是输出应仅取决于输入，且函数没有副作用。以下函数中违背这些原则的地方已用注释标出：\ncompute_log_ratio &lt;- function(mpg, wt) {\n  log_base &lt;- getOption(\"log_base\", default = exp(1)) # gets external data\n  results &lt;- log(mpg/wt, base = log_base)\n  print(mean(results))                                # prints to the console\n  done &lt;&lt;- TRUE                                       # sets external data\n  results\n}\n一个更好的版本是：\n\ncompute_log_ratio &lt;- function(mpg, wt, log_base = exp(1)) {\n  log(mpg / wt, base = log_base)\n}\n\npurrr包包含用于函数式编程的工具。让我们重点关注map()函数族，它们对向量进行操作，并且总是返回相同类型的输出。最基本的函数map()总是返回一个列表，其基本语法为map(vector, function)。例如，要对我们的数据求平方根，我们可以：\n\nmap(head(mtcars$mpg, 3), sqrt)\n#&gt; [[1]]\n#&gt; [1] 4.582576\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4.582576\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4.774935\n\n存在map()的专门变体，当我们知道或预期函数将生成基本向量类型之一时，这些变体就会返回值。例如，由于平方根会返回双精度数：\n\nmap_dbl(head(mtcars$mpg, 3), sqrt)\n#&gt; [1] 4.582576 4.582576 4.774935\n\n还有一些映射函数可以跨多个向量进行操作：\n\nlog_ratios &lt;- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)\nhead(log_ratios)\n#&gt; [1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\nmap()函数还允许使用波浪号定义临时的匿名函数。对于map2()，参数值为.x和.y：\n\nmap2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x / .y)) %&gt;%\n  head()\n#&gt; [1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\n这些示例虽然简单，但在后面的章节中，它们将被应用于更复杂的问题。在整洁建模的函数式编程中，定义的函数应确保能被像map()这样的函数使用，便于迭代计算。",
    "crumbs": [
      "2 A Tidyverse Primer"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#examples-of-tidyverse-syntax",
    "href": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#examples-of-tidyverse-syntax",
    "title": "2 A Tidyverse Primer",
    "section": "Examples of Tidyverse Syntax",
    "text": "Examples of Tidyverse Syntax\n让我们通过更深入地探究什么是tibble以及tibble的工作原理，来开始对tidyverse语法的讨论。在R语言中，tibble与基本数据框（data.frame）的规则略有不同。例如，tibble自然支持那些不符合语法规则的变量名作为列名：\n\n# Wants valid names:\ndata.frame(`variable 1` = 1:2, two = 3:4)\n#&gt;   variable.1 two\n#&gt; 1          1   3\n#&gt; 2          2   4\n# But can be coerced to use them with an extra option:\ndf &lt;- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)\ndf\n#&gt;   variable 1 two\n#&gt; 1          1   3\n#&gt; 2          2   4\n\n# But tibbles just work:\ntbbl &lt;- tibble(`variable 1` = 1:2, two = 3:4)\ntbbl\n#&gt; # A tibble: 2 × 2\n#&gt;   `variable 1`   two\n#&gt;          &lt;int&gt; &lt;int&gt;\n#&gt; 1            1     3\n#&gt; 2            2     4\n\n标准数据框支持参数的部分匹配，因此仅使用列名的一部分的代码仍然可以运行。而tibble会阻止这种情况发生，因为这可能会导致意外错误。\n\ndf$tw\n#&gt; [1] 3 4\n\ntbbl$tw\n#&gt; Warning: Unknown or uninitialised column: `tw`.\n#&gt; NULL\n\nTibbles还能避免一种最常见的R语言错误：维度丢失。如果标准数据框将列子集化到只剩一列，该对象会被转换为向量。而Tibbles绝不会这样做：\n\ndf[, \"two\"]\n#&gt; [1] 3 4\n\ntbbl[, \"two\"]\n#&gt; # A tibble: 2 × 1\n#&gt;     two\n#&gt;   &lt;int&gt;\n#&gt; 1     3\n#&gt; 2     4\n\n使用tibble而非数据框还有其他优势，比如更好的打印效果等等。\n为了演示一些语法，让我们使用tidyverse函数读取可用于建模的数据。该数据集来自芝加哥市的数据门户，包含该市高架火车站的每日客流量数据。该数据集包含以下列：\n\nthe station identifier (numeric)\nthe station name (character)\nthe date (character in mm/dd/yyyy format)\nthe day of the week (character)\nthe number of riders (numeric)\n\n我们的tidyverse流程将按顺序执行以下任务：\n\n使用tidyverse包中的readr从源网站读取数据并将其转换为tibble格式。要实现这一点，read_csv()函数可以通过读取初始的若干行来确定数据类型。或者，如果列名和类型已知，可以在R中创建列规范并将其传递给read_csv()。\n筛选数据以删除一些不需要的列（例如station ID），并将列stationname更改为station。此操作使用select()函数。筛选时，可以使用列名或dplyr选择器函数。选择名称时，可以使用new_name = old_name的参数格式来声明新的变量名。\n使用lubridate包中的mdy()函数将日期字段转换为R日期格式。我们还将客流量数据转换为以千为单位。这两项计算都是通过dplyr::mutate()函数执行的。\n对每个车站和日期的组合，采用最大的骑行次数。这能缓解少数日期在特定车站出现多条骑行次数记录的问题。我们按车站和日期对骑行数据进行分组，然后在1999个独特组合的每个组合内，用最大值统计量进行汇总。\n\n这些步骤的tidyverse代码如下：\nlibrary(tidyverse)\nlibrary(lubridate)\n\nurl &lt;- \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations &lt;-\n  # Step 1: Read in the data.\n  read_csv(url) %&gt;%\n  # Step 2: filter columns and rename stationname\n  dplyr::select(station = stationname, date, rides) %&gt;%\n  # Step 3: Convert the character date field to a date encoding.\n  # Also, put the data in units of 1K rides\n  mutate(date = mdy(date), rides = rides / 1000) %&gt;%\n  # Step 4: Summarize the multiple records using the maximum.\n  group_by(date, station) %&gt;%\n  summarize(rides = max(rides), .groups = \"drop\")\n这一系列操作流程说明了tidyverse广受欢迎的原因。它采用了一系列数据处理方法，每个转换都有简单易懂的函数；这一系列操作以一种简洁、易读的方式组合在一起。其核心在于用户与软件的交互方式。这种方法让更多人能够学习R语言并实现他们的分析目标，而将这些相同的原则应用于R语言中的建模也能带来同样的好处。",
    "crumbs": [
      "2 A Tidyverse Primer"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/2 A Tidyverse Primer.html#chapter-summary",
    "title": "2 A Tidyverse Primer",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了tidyverse，重点讲解了其在建模方面的应用，以及tidyverse的设计原则如何为tidymodels框架提供指导。可以将tidymodels框架看作是将tidyverse原则应用于模型构建领域。我们阐述了tidyverse与base R在约定上的差异，并介绍了tidyverse系统的两个重要组成部分：tibbles和管道运算符%&gt;%。数据清理和处理有时可能会让人觉得枯燥，但这些任务在现实世界的建模中至关重要；我们通过一个数据导入和处理的示例，说明了如何使用tibbles、管道以及tidyverse函数。",
    "crumbs": [
      "2 A Tidyverse Primer"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/4 The Ames Housing Data.html",
    "href": "Books/Tidy Modeling with R/4 The Ames Housing Data.html",
    "title": "4 The Ames Housing Data",
    "section": "",
    "text": "在本章中，我们将介绍Ames房产数据集（De Cock 2011），该数据集贯穿整本书。探索性数据分析（EDA），就像我们在本章中所进行的那样，是构建可靠模型的重要第一步。该数据集包含Ames Iowa的2930处房产信息，包括与以下方面相关的列：\n我们的建模目标是根据我们掌握的其他信息（例如房屋的特征和位置）来预测房屋的售价。\n原始住房数据由De Cock（2011）提供，但在本书的分析中，我们使用的是modeldata包中经过转换且可用的版本。此版本对数据进行了若干修改和改进。例如，确定了每个房产的经度和纬度值。此外，一些列经过修改，更便于分析。例如：\n加载数据：\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\n# library(modeldata) # This is also loaded by the tidymodels package\ndata(ames)\n\n# or, in one line:\ndata(ames, package = \"modeldata\")\n\ndim(ames)\n#&gt; [1] 2930   74",
    "crumbs": [
      "4 The Ames Housing Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/4 The Ames Housing Data.html#exploring-features-of-homes-in-ames",
    "href": "Books/Tidy Modeling with R/4 The Ames Housing Data.html#exploring-features-of-homes-in-ames",
    "title": "4 The Ames Housing Data",
    "section": "Exploring Features of Homes in Ames",
    "text": "Exploring Features of Homes in Ames\n让我们从关注我们想要预测的结果开始进行探索性数据分析：房屋的最终售价（以美元计）。我们可以创建一个直方图（ Figure 1 ）来查看售价分布。\n\nggplot(ames, aes(x = Sale_Price)) +\n  geom_histogram(bins = 50, col = \"white\")\n\n\n\n\n\n\nFigure 1: Sale prices of houses in Ames, Iowa\n\n\n\n\n这张图表向我们展示，数据呈右偏分布；低价房屋的数量多于高价房屋。销售价格的中位数为160,000美元，最贵的房屋为755,000美元。在对这一结果进行建模时，有充分的理由认为应该对价格进行对数转换。这种转换的优点是，不会预测出房屋的负销售价格，而且在预测高价房屋时出现的误差也不会对模型产生不当影响。此外，从统计学角度来看，对数转换还可能稳定方差，从而使推断更具合理性。我们可以采用类似的步骤来可视化转换后的数据，如 Figure 2 所示。\n\nggplot(ames, aes(x = Sale_Price)) +\n  geom_histogram(bins = 50, col = \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\nFigure 2: Sale prices of houses in Ames, Iowa after a log (base 10) transformation\n\n\n\n\n虽然并不完美，但出于刚才概述的原因，这很可能会比使用未转换的数据得到更好的模型。\n模型系数的单位可能更难解释，性能指标也是如此。例如，均方根误差（RMSE）是回归模型中常用的性能指标。它在计算中使用观测值和预测值之间的差异。如果销售价格处于对数尺度上，这些差异（即残差）也会处于对数尺度上。对于在这样的对数尺度上RMSE为0.15的模型，其质量可能难以理解。\n尽管存在这些缺点，本书中使用的模型仍对这一结果采用对数变换。从这一点来看，结果列在ames数据框中已预先进行了对数处理：\n\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\n这些数据对于我们建模的另一个重要方面是它们的地理位置。这种空间信息在数据中以两种方式呈现：定性的Neighborhood标签以及定量的经度和纬度数据。为了可视化空间信息，让我们将两者结合起来，在 Figure 3 中的地图上绘制数据。\n\n\n\n\n\nFigure 3: Neighborhoods in Ames, IA\n\n\n我们可以看到一些明显的模式。首先，Ames市中心存在数据点的空白区域。这对应于爱荷华州立大学的校园，那里没有住宅。其次，虽然有许多相邻的社区，但其他一些社区在地理上是孤立的。例如，如 Figure 4 所示，Timberland社区几乎与所有其他社区都相隔开来。\n\n\n\n\n\nFigure 4: Locations of homes in Timberland\n\n\n图 Figure 5 展示了Ames市东南部的Meadow村社区是如何像一座房产岛屿一样，坐落在Mitchell社区构成的大片房产海洋之中的。\n\n\n\n\n\nFigure 5: Locations of homes in Meadow Village and Mitchell\n\n\n对地图的详细检查还显示，社区标签并非完全可靠。例如，Figure 6 显示，一些被标记为位于Northridge社区的房产被相邻的Somerset社区的住宅所环绕。\n\n\n\n\n\nFigure 6: Locations of homes in Somerset and Northridge\n\n\n此外，有十处被标记为位于Crawford的孤立住宅，正如你在 Figure 7 中所见，它们并不靠近该社区的大多数其他住宅。\n\n\n\n\n\nFigure 7: Locations of homes in Crawford\n\n\n同样值得注意的是“Iowa Department of Transportation (DOT) and Rail Road”社区，它邻近Ames市东侧的主干道，如 Figure 8 所示。该社区内有几处房屋集群，还有一些纵向的离群房屋；最东边的两栋房屋与其他位置的房屋相互隔离。\n\n\n\n\n\nFigure 8: Homes labeled as Iowa Department of Transportation (DOT) and Rail Road\n\n\n如第1章所述，在开始任何建模之前，进行探索性数据分析至关重要。这些住房数据具有一些特征，对数据的处理和建模方式提出了有趣的挑战。我们将在后面的章节中详细描述其中的许多特征。在这个探索阶段可以研究的一些基本问题包括：\n\n各个预测变量的分布有没有什么奇怪或值得注意的地方？是否存在明显的偏度或任何异常分布？\n预测变量之间是否存在高度相关性？例如，有多个与房屋面积相关的预测变量。其中是否有一些是冗余的？\n预测变量和结果之间存在关联吗？\n\n在本书中使用这些数据时，会重新探讨其中的许多问题。",
    "crumbs": [
      "4 The Ames Housing Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/4 The Ames Housing Data.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/4 The Ames Housing Data.html#chapter-summary",
    "title": "4 The Ames Housing Data",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了Ames房价数据集，并研究了它的一些特征。该数据集将在后续章节中用于演示tidymodels语法。像这样的探索性数据分析（EDA）是任何建模项目的重要组成部分，它能揭示有助于改进建模实践的信息。\n我们将在后续章节中继续使用的、用于准备Ames数据集的重要代码如下：\nlibrary(tidymodels)\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))",
    "crumbs": [
      "4 The Ames Housing Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/index.html",
    "href": "Books/Tidy Modeling with R/index.html",
    "title": "Hello World",
    "section": "",
    "text": "本篇为书籍Tidy Modeling with R的学习笔记。\n原文见：Tidy Modeling with R\ntidymodels R包官方网站：https://www.tidymodels.org\n本书主要包含两方面：\n\n如何使用tidymodels包进行机器学习建模\n统计建模中的实践经验\n\n下面是全文概览：\n\n第1章，概述模型的分类，及优质建模软件的特征。第2章，回顾 tidyverse 的语法和思想。第3章，介绍 base R 中的统计建模及其不足。\n第4~9章：以一个示例介绍tidymodel系列包：recipes，parsnip，workflows，yardstick等。\n第10~15章：模型性能评估与调参。\n第16~21章：介绍特征工程，如降维和高基数预测变量编码；模型的可解释性与可信度。\n\n\n\n\n Back to top",
    "crumbs": [
      "Hello World"
    ]
  },
  {
    "objectID": "Books/quarto/Project Basics.html",
    "href": "Books/quarto/Project Basics.html",
    "title": "Project Basics",
    "section": "",
    "text": "原文：https://quarto.org/docs/projects/quarto-projects.html\nquarto的官方网站提供了许多创建项目的教程，如下图，如何创建项目，这里不再赘述。\n\n\n\n官网教程\n\n\n\n\n当你创建一个项目后，会自动生成一个项目级的YAML文件——_quarto.yml，该文件用来配置整个项目。示例如下：\nproject:\n  output-dir: _output\n\ntoc: true\nnumber-sections: true\nbibliography: references.bib\n\nformat:\n  html:\n    css: styles.css\n    html-math-method: katex\n  pdf:\n    documentclass: report\n    margin-left: 30mm\n    margin-right: 30mm\n随着项目的持续进行，对各部分的调控会显得及其臃肿，最好的办法是将其进行拆分。使用metadata-files可以进行配置，下面是一个混合导航栏的拆分示例，更多导航栏信息见https://quarto.org/docs/websites/website-navigation.html。\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  navbar:\n    left:\n      - text: Books\n        menu:\n          - sidebar:ggplot2\n\nmetadata-files:\n  - ggplot2/_ggplot.yml\n\n\n\n_ggplot.yml\n\nwebsite:\n  sidebar:\n    - id: ggplot2\n      title: ggplot2\n      contents:\n        - ggplot2/index.qmd\n\n与上面导航栏示例不同，quarto提供了另外一种——文件夹级配置文件——_metadata.yml。如下：\n\n\n_metadata.yml\n\nformat:\n  revealjs:\n    menu: false\n    progress: false\nsearch: false\n\nquarto中的配置信息分为三级，按优先级由低到高：\n\n项目级配置文件：_quarto.yml\n文件夹级配置文件：dir/_metadata.yml\n文件级配置文件：dir/file.qmd\n\nquarto会自动对配置文件进行合并或覆盖，示例如下，最终的配置体现在文件级配置文件中。\n\n\n\n\n\n\n\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    toc: true\nbibliography:\n  - refs.bib\n\n\n\ndir/_metadata.yml\n\nformat:\n  html:\n    code-fold: true\nbibliography:\n  - proj.bib\n\n\n\nmerge\n\nformat:\n  html:\n    toc: true\n    code-fold: true\nbibliography:\n  - refs.bib\n  - proj.bib\n\n\n\n\n也可以设置本地配置文件_quarto.yml.local，不进行上传，该文件会自动添加到.gitignore中。\n\n\n_quarto.yml.local\n\nexecute:\n  cache: true\n\n\n\n\n使用quarto 进行渲染时，你可以：\n\n渲染整个项目quarto render\n渲染某个目录quarto render subdir\n渲染为不同格式的文件quarto render --to pdf\n\n如果不想渲染整个项目，只渲染部分文档，可以进行如下配置：\nproject:\n  render:\n    - section1.qmd\n    - section2.qmd\nproject:\n  render:\n    - section*.qmd\n当你想配置某些文件不渲染时，可以进行如下配置，注意一定要包含*.qmd:\nproject:\n  render:\n    - \"*.qmd\"\n    - \"!ignored.qmd\"\n    - \"!ignored-dir/\"",
    "crumbs": [
      "Project Basics"
    ]
  },
  {
    "objectID": "Books/quarto/Project Basics.html#shared-metadata",
    "href": "Books/quarto/Project Basics.html#shared-metadata",
    "title": "Project Basics",
    "section": "",
    "text": "当你创建一个项目后，会自动生成一个项目级的YAML文件——_quarto.yml，该文件用来配置整个项目。示例如下：\nproject:\n  output-dir: _output\n\ntoc: true\nnumber-sections: true\nbibliography: references.bib\n\nformat:\n  html:\n    css: styles.css\n    html-math-method: katex\n  pdf:\n    documentclass: report\n    margin-left: 30mm\n    margin-right: 30mm\n随着项目的持续进行，对各部分的调控会显得及其臃肿，最好的办法是将其进行拆分。使用metadata-files可以进行配置，下面是一个混合导航栏的拆分示例，更多导航栏信息见https://quarto.org/docs/websites/website-navigation.html。\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  navbar:\n    left:\n      - text: Books\n        menu:\n          - sidebar:ggplot2\n\nmetadata-files:\n  - ggplot2/_ggplot.yml\n\n\n\n_ggplot.yml\n\nwebsite:\n  sidebar:\n    - id: ggplot2\n      title: ggplot2\n      contents:\n        - ggplot2/index.qmd\n\n与上面导航栏示例不同，quarto提供了另外一种——文件夹级配置文件——_metadata.yml。如下：\n\n\n_metadata.yml\n\nformat:\n  revealjs:\n    menu: false\n    progress: false\nsearch: false\n\nquarto中的配置信息分为三级，按优先级由低到高：\n\n项目级配置文件：_quarto.yml\n文件夹级配置文件：dir/_metadata.yml\n文件级配置文件：dir/file.qmd\n\nquarto会自动对配置文件进行合并或覆盖，示例如下，最终的配置体现在文件级配置文件中。\n\n\n\n\n\n\n\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    toc: true\nbibliography:\n  - refs.bib\n\n\n\ndir/_metadata.yml\n\nformat:\n  html:\n    code-fold: true\nbibliography:\n  - proj.bib\n\n\n\nmerge\n\nformat:\n  html:\n    toc: true\n    code-fold: true\nbibliography:\n  - refs.bib\n  - proj.bib\n\n\n\n\n也可以设置本地配置文件_quarto.yml.local，不进行上传，该文件会自动添加到.gitignore中。\n\n\n_quarto.yml.local\n\nexecute:\n  cache: true",
    "crumbs": [
      "Project Basics"
    ]
  },
  {
    "objectID": "Books/quarto/Project Basics.html#rendering-projects",
    "href": "Books/quarto/Project Basics.html#rendering-projects",
    "title": "Project Basics",
    "section": "",
    "text": "使用quarto 进行渲染时，你可以：\n\n渲染整个项目quarto render\n渲染某个目录quarto render subdir\n渲染为不同格式的文件quarto render --to pdf\n\n如果不想渲染整个项目，只渲染部分文档，可以进行如下配置：\nproject:\n  render:\n    - section1.qmd\n    - section2.qmd\nproject:\n  render:\n    - section*.qmd\n当你想配置某些文件不渲染时，可以进行如下配置，注意一定要包含*.qmd:\nproject:\n  render:\n    - \"*.qmd\"\n    - \"!ignored.qmd\"\n    - \"!ignored-dir/\"",
    "crumbs": [
      "Project Basics"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNextclade\n\n\n\nnextclade\n\n\n\n\n\n\n\n\n\n2025-12-07\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\n双环网络图\n\n\n\nggraph\n\n\n\n\n\n\n\n\n\n2025-09-06\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\n暴露组学关联分析\n\n\n\n\n\n\n\n\n\n\n\n2025-09-06\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\n替换多对字符串\n\n\n\npurrr\n\ntidyverse\n\n\n\n\n\n\n\n\n\n2025-07-24\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\n绘制点脊图\n\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n2025-07-03\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\n相关性计算\n\n\n\ntriky skill\n\n\n\n\n\n\n\n\n\n2025-06-27\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\nData Masking(1)\n\n\n\nrlang\n\ntidyverse\n\n\n\n\n\n\n\n\n\n2025-04-25\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\nR 中的面向对象(1)\n\n\n\n面向对象\n\n\n\n\n\n\n\n\n\n2025-03-16\n\n\n大番薯本薯\n\n\n\n\n\n\n\n\n\n\n\n\nquarto 中R代码块的设置\n\n\n\nquarto\n\n\n\n\n\n\n\n\n\n2025-03-15\n\n\n大番薯本薯\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html",
    "title": "6 Fitting Models with parsnip",
    "section": "",
    "text": "parsnip包是tidymodels系列中的一个R包，它为各种不同的模型提供了流畅且标准化的接口。在本章中，我们将阐述为什么通用接口有助于在实际操作中理解和构建模型，并展示如何使用parsnip包。\n具体来说，我们将重点介绍如何直接使用parsnip对象进行fit()和predict()操作，这可能适用于一些简单的建模问题。下一章将介绍一种更优的方法，通过将模型和预处理器组合成一个称为workflow的对象，来处理许多建模任务。",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#create-a-model",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#create-a-model",
    "title": "6 Fitting Models with parsnip",
    "section": "Create a Model",
    "text": "Create a Model\n一旦数据被编码成适合建模算法的格式（例如数值矩阵），就可以将其用于模型构建过程中。\n假设我们最初选择的是线性回归模型。这相当于指定结果数据是数值型的，并且预测变量与结果之间的关系可以用简单的斜率和截距来表示：\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_p x_{pi}\\]\n可以使用多种方法来估计模型参数：\n\n普通线性回归采用传统的最小二乘法来求解模型参数。\n正则化线性回归在最小二乘法中加入惩罚项，通过移除预测变量和/或将其系数缩小至零来追求模型的简洁性。这可以通过贝叶斯或非贝叶斯技术来实现。\n\nHeterogeneous Interface\n在R语言中，stats包可用于第一种情况。使用函数lm()进行线性回归的语法为model &lt;- lm(formula, data, ...)，其中...表示要传递给lm()的其他参数。该函数没有单独的x或y的接口，而是直接假定结果是y，预测变量是x。\n对于第二种情况，即使用正则化进行估计，可以通过rstanarm包来拟合贝叶斯模型：model &lt;- stan_glm(formula, data, family = \"gaussian\", ...)。在这种情况下，通过...传递的其他选项将包括参数的先验分布的参数以及有关模型数值方面的具体信息。与lm()一样，只有公式接口可用。\n一种流行的非贝叶斯正则化回归方法是glmnet模型（Friedman, Hastie, Tibshirani 2010）。其语法如下：model &lt;- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)。在这种情况下，预测变量数据必须已经格式化为数值矩阵；这里有x或y的单独接口，没有公式接口。\n请注意，这些接口在数据传递给模型函数的方式或其参数方面存在异质性。第一个问题是，为了在不同的包中拟合模型，数据必须以不同的方式格式化。lm()和stan_glm()只有公式接口，而glmnet()没有，对于其他类型的模型，其接口可能差异更大。对于试图进行数据分析的人来说，这些差异要求他们记住每个包的语法，这可能会非常令人沮丧。\n对于tidymodels而言，指定模型的方法旨在更加统一：\n\n根据模型的数学结构指定其类型（例如，线性回归、随机森林、K近邻等）。\n指定用于拟合模型的引擎。这通常指的是使用的软件包，如Stan或glmnet。这些本身就是模型，而parsnip通过将它们用作建模引擎来提供一致的接口。\n必要时，声明模型的模式。模式反映了预测结果的类型。对于数值型结果，模式为回归；对于定性结果，模式为分类。如果一种模型算法只能处理一种类型的预测结果（如线性回归），则其模式已预先设定。\n\n这些规格的构建没有参考数据。例如，对于我们概述的三个案例：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\n\nlinear_reg() %&gt;% set_engine(\"lm\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nlinear_reg() %&gt;% set_engine(\"glmnet\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: glmnet\n\nlinear_reg() %&gt;% set_engine(\"stan\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: stan\n\nConsistent Interface\n一旦模型的细节确定后，就可以使用fit()函数（用于公式）或fit_xy()函数（当数据已预处理时）来进行模型估计。parsnip包允许用户不必在意底层模型的接口；即使建模包的函数只有x/y接口，你也始终可以使用公式。\ntranslate()函数可以详细说明parsnip如何将用户的代码转换为该包的语法：\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  translate()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm \n#&gt; \n#&gt; Model fit template:\n#&gt; stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nlinear_reg(penalty = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  translate()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   penalty = 1\n#&gt; \n#&gt; Computational engine: glmnet \n#&gt; \n#&gt; Model fit template:\n#&gt; glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n#&gt;     family = \"gaussian\")\n\nlinear_reg() %&gt;%\n  set_engine(\"stan\") %&gt;%\n  translate()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: stan \n#&gt; \n#&gt; Model fit template:\n#&gt; rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n#&gt;     weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n注意，missing_arg()只是尚未提供的数据的占位符。我们为glmnet引擎提供了一个必需的penalty参数。此外，对于Stan和glmnet引擎，family参数作为默认值被自动添加。正如本节后面将展示的，这个选项是可以更改的。\n让我们逐步了解如何仅将经度和纬度作为函数来预测艾姆斯数据中房屋的销售价格：\n\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n\nlm_model &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nlm_form_fit &lt;-\n  lm_model %&gt;%\n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit &lt;-\n  lm_model %&gt;%\n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_form_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -302.974       -2.075        2.710\n\nlm_xy_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -302.974       -2.075        2.710\n\nConsistent Parameters\nparsnip不仅为不同的包提供了一致的模型接口，还在模型参数方面保持了一致性。拟合相同模型的不同函数往往具有不同的参数名称，这是很常见的情况。随机森林模型函数就是一个很好的例子。三个常用的参数分别是集成中的树的数量、在树的每次分裂时随机抽样的预测变量数量，以及进行分裂所需的数据点数量。对于实现该算法的三个不同的R包，这些参数如 Table 1 所示。\n\n\nTable 1: Example argument names for different random forest functions.\n\n\n\n\n\n\n\n\n\nArgument Type\nranger\nrandomForest\nsparklyr\n\n\n\n# sampled predictors\nmtry\nmtry\nfeature_subset_strategy\n\n\n# trees\nnum.trees\nntree\nnum_trees\n\n\n# data points to split\nmin.node.size\nnodesize\nmin_instances_per_node\n\n\n\n\n\n\n为了减轻参数指定的麻烦，parsnip在包内部和包之间使用通用的参数名称。Table 2 展示了parsnip模型在随机森林中所使用的参数。\n\n\nTable 2: Random forest argument names used by parsnip.\n\n\n\nArgument Type\nparsnip\n\n\n\n# sampled predictors\nmtry\n\n\n# trees\ntrees\n\n\n# data points to split\nmin_n\n\n\n\n\n\n\n诚然，这是又一组需要记住的参数。不过，当其他类型的模型具有相同的参数类型时，这些名称仍然适用。例如，梯度提升树集成也会创建大量基于树的模型，因此在那里也会使用trees，min_n也是如此，依此类推。\n一些原始参数名称可能相当专业。例如，在glmnet模型中，为了指定要使用的正则化量，会用到希腊字母lambda。虽然这种数学符号在统计学文献中很常用，但很多人并不清楚lambda代表什么（尤其是那些使用模型结果的人）。由于这是正则化中使用的惩罚项，parsnip将参数名称标准化为penalty。同样，KNN模型中的邻居数量被称为neighbors，而不是k。我们在标准化参数名称时的经验法则是：如果从业者要将这些名称包含在图表或表格中，查看这些结果的人会理解这些名称吗？\n要了解parsnip的参数名称如何对应原始名称，请使用模型的帮助文件（可通过?rand_forest获取）以及translate()函数：\n\nrand_forest(trees = 1000, min_n = 5) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\") %&gt;%\n  translate()\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 5\n#&gt; \n#&gt; Computational engine: ranger \n#&gt; \n#&gt; Model fit template:\n#&gt; ranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n#&gt;     num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n#&gt;     verbose = FALSE, seed = sample.int(10^5, 1))\n\nparsnip中的建模函数将模型参数分为两类：\n\n主要参数更为常用，且往往在不同引擎中都可使用。\n引擎参数要么是特定于某个引擎的，要么使用频率较低。\n\n例如，在之前随机森林代码的转换中，参数num.threads、verbose和seed是默认添加的。这些参数是随机森林模型range实现所特有的，作为主要参数是不合理的。特定于引擎的参数可以在 set_engine()中指定。例如，要让ranger::ranger()函数打印出更多关于拟合的信息：\n\nrand_forest(trees = 1000, min_n = 5) %&gt;%\n  set_engine(\"ranger\", verbose = TRUE) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  translate()\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 5\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   verbose = TRUE\n#&gt; \n#&gt; Computational engine: ranger \n#&gt; \n#&gt; Model fit template:\n#&gt; ranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n#&gt;     num.trees = 1000, min.node.size = min_rows(~5, x), verbose = TRUE, \n#&gt;     num.threads = 1, seed = sample.int(10^5, 1))",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#use-the-model-results",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#use-the-model-results",
    "title": "6 Fitting Models with parsnip",
    "section": "Use the Model Results",
    "text": "Use the Model Results\n一旦模型创建并拟合完成，我们可以通过多种方式使用其结果；我们可能想要绘制、打印或以其他方式检查模型输出。parsnip模型对象中存储了多个量，包括拟合好的模型。这可以在一个名为fit的元素中找到，该元素可通过extract_fit_engine()函数返回：\n\nlm_form_fit %&gt;% extract_fit_engine()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -302.974       -2.075        2.710\n\n常规方法可应用于该对象，例如打印和绘图：\n\nlm_form_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  vcov()\n#&gt;             (Intercept)     Longitude      Latitude\n#&gt; (Intercept)  207.311311  1.5746587743 -1.4239709610\n#&gt; Longitude      1.574659  0.0165462548 -0.0005999802\n#&gt; Latitude      -1.423971 -0.0005999802  0.0325397353\n\n永远不要将parsnip模型的fit元素传递给模型预测函数，也就是说，使用predict(lm_form_fit)，而不是predict(lm_form_fit$fit)。如果数据经过了任何预处理，将会产生错误的预测结果（有时不会出现错误提示）。底层模型的预测函数并不知道在运行模型之前是否对数据进行了任何转换。有关预测的更多内容，请参见第6.3节。\nbase R中一些现有方法存在的一个问题是，结果的存储方式可能并非最实用。例如，针对lm对象的summary()方法可用于打印模型拟合结果，包括一个包含参数值、其不确定性估计以及p值的表格。这些特定结果也可以保存：\n\nmodel_res &lt;-\n  lm_form_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\n# The model coefficient table is accessible via the `coef` method.\nparam_est &lt;- coef(model_res)\nclass(param_est)\n#&gt; [1] \"matrix\" \"array\"\nparam_est\n#&gt;                Estimate Std. Error   t value     Pr(&gt;|t|)\n#&gt; (Intercept) -302.973554 14.3983093 -21.04230 3.640103e-90\n#&gt; Longitude     -2.074862  0.1286322 -16.13019 1.395257e-55\n#&gt; Latitude       2.709654  0.1803877  15.02128 9.289500e-49\n\n关于这个结果，有几点需要注意。首先，该对象是一个数值矩阵。选择这种数据结构很可能是因为所有计算结果都是数值型的，而且矩阵对象比数据框的存储效率更高。这种选择或许是在20世纪70年代末做出的，当时计算效率至关重要。其次，非数值数据（系数的标签）包含在行名中。将参数标签作为行名，这与原始S语言中的约定非常一致。\n合理的下一步可能是创建参数值的可视化。要做到这一点，将参数矩阵转换为数据框是明智的。我们可以将行名作为一列添加进去，这样它们就可以在图表中使用了。然而，请注意，现有的几个矩阵列名对于普通数据框来说不是有效的R列名（例如，\"Pr(&gt;|t|)\"）。另一个复杂之处在于列名的一致性。对于lm对象，p值所在的列是\"Pr(&gt;|t|)\"，但对于其他模型，可能会使用不同的检验，因此列名会有所不同（例如，\"Pr(&gt;|z|)\"），并且检验类型会编码在列名中。\n虽然这些额外的数据格式化步骤并非无法克服，但它们确实是一种阻碍，尤其是因为对于不同类型的模型，这些步骤可能会有所不同。矩阵并非一种可高度复用的数据结构，主要原因是它会将数据限制为单一类型（例如数值型）。此外，将部分数据保存在维度名称中也存在问题，因为这些数据必须经过提取才能具有普遍用途。\n作为一种解决方案，broom包可以将多种类型的模型对象转换为整洁的结构。例如，在线性模型上使用tidy()方法会生成：\n\ntidy(lm_form_fit)\n#&gt; # A tibble: 3 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)  -303.      14.4       -21.0 3.64e-90\n#&gt; 2 Longitude      -2.07     0.129     -16.1 1.40e-55\n#&gt; 3 Latitude        2.71     0.180      15.0 9.29e-49\n\n列名在各个模型中是标准化的，不包含任何额外数据（例如统计检验的类型）。以前包含在行名中的数据现在位于一个名为term的列中。tidymodels生态系统中的一个重要原则是，函数返回的值应该具有可预测性、一致性和不出意料的特点。",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#make-predictions",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#make-predictions",
    "title": "6 Fitting Models with parsnip",
    "section": "Make Predictions",
    "text": "Make Predictions\nparsnip与传统R建模函数的另一个不同之处在于predict()返回值的格式。对于预测，parsnip始终遵循以下规则：\n\n结果始终是一个tibble（ tibble 是R语言中一种数据框格式）。\n该tibble的列名始终是可预测的。\n该tibble中的行数始终与输入数据集的行数相同。\n\n例如，在预测数值型数据时：\n\names_test_small &lt;- ames_test %&gt;% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n#&gt; # A tibble: 5 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.22\n#&gt; 2  5.21\n#&gt; 3  5.28\n#&gt; 4  5.27\n#&gt; 5  5.28\n\n预测结果的行顺序始终与原始数据相同。为什么有些列名前面有圆点？一些tidyverse和tidymodels的参数及返回值包含句点。这是为了防止合并具有重复名称的数据。有些数据集包含名为pred的预测变量！\n这三条规则使将预测结果与原始数据合并变得更加容易：\n\names_test_small %&gt;%\n  select(Sale_Price) %&gt;%\n  bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;%\n  # Add 95% prediction intervals to the results:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\"))\n#&gt; # A tibble: 5 × 4\n#&gt;   Sale_Price .pred .pred_lower .pred_upper\n#&gt;        &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1       5.02  5.22        4.91        5.54\n#&gt; 2       5.39  5.21        4.90        5.53\n#&gt; 3       5.28  5.28        4.97        5.60\n#&gt; 4       5.28  5.27        4.96        5.59\n#&gt; 5       5.28  5.28        4.97        5.60\n\n第一条规则的动机源于一些R包的预测函数会生成不同的数据类型。例如，ranger包是计算随机森林模型的出色工具。然而，它返回的不是数据框或向量形式的输出，而是一个专用对象，其中嵌入了多个值（包括预测值）。这给数据分析师在脚本中处理时又增加了一个步骤。再举一个例子，原生的glmnet模型根据模型的具体情况和数据特征，至少可以返回四种不同的预测输出类型。这些类型如 Table 3 所示。\n\n\nTable 3: Different return values for glmnet prediction types.\n\n\n\nType of Prediction\nReturns a:\n\n\n\nnumeric\nnumeric matrix\n\n\nclass\ncharacter matrix\n\n\nprobability (2 classes)\nnumeric matrix (2nd level only)\n\n\nprobability (3+ classes)\n3D numeric array (all levels)\n\n\n\n\n\n\n此外，结果的列名包含编码值，这些编码值对应于glmnet模型对象中一个名为lambda的向量。这种出色的统计方法在实际使用中可能会令人却步，因为分析师可能会遇到各种特殊情况，而这些情况需要额外编写代码才能让该方法发挥作用。\n对于第二个 tidymodels 预测规则，不同预测类型的可预测列名如 Table 4 所示。\n\n\nTable 4: The tidymodels mapping of prediction types and column names.\n\n\n\ntype value\ncolumn name(s)\n\n\n\nnumeric\n.pred\n\n\nclass\n.pred_class\n\n\nprob\n.pred_{class levels}\n\n\nconf_int\n.pred_lower, .pred_upper\n\n\npred_int\n.pred_lower, .pred_upper\n\n\n\n\n\n\n关于输出中行数的第三条规则至关重要。例如，如果新数据的任何行包含缺失值，输出将为这些行填充缺失结果。parsnip中对模型接口和预测类型进行标准化的一个主要优势是，当使用不同的模型时，语法是相同的。假设我们使用决策树对艾姆斯的数据进行建模。在模型规格之外，代码流程没有显著差异：\n\ntree_model &lt;-\n  decision_tree(min_n = 2) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\ntree_fit &lt;-\n  tree_model %&gt;%\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\names_test_small %&gt;%\n  select(Sale_Price) %&gt;%\n  bind_cols(predict(tree_fit, ames_test_small))\n#&gt; # A tibble: 5 × 2\n#&gt;   Sale_Price .pred\n#&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       5.02  5.15\n#&gt; 2       5.39  5.15\n#&gt; 3       5.28  5.32\n#&gt; 4       5.28  5.32\n#&gt; 5       5.28  5.32\n\n这体现了在不同模型间使数据分析流程和语法同质化的好处。它能让用户将时间花在结果和解读上，而非不得不专注于R包之间的语法差异。",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#parsnip-extension-packages",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#parsnip-extension-packages",
    "title": "6 Fitting Models with parsnip",
    "section": "parsnip-Extension Packages",
    "text": "parsnip-Extension Packages\nparsnip包本身包含了多个模型的接口。不过，为了便于包的安装和维护，还有其他tidymodels包提供了针对其他模型集的parsnip模型定义。discrim包包含了一组称为判别分析方法（如线性或二次判别分析）的分类技术的模型定义。通过这种方式，安装parsnip所需的包依赖得以减少。所有可与parsnip配合使用的模型（涵盖CRAN上的不同包）的列表可在 https://www.tidymodels.org/find/ 找到。",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#creating-model-specifications",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#creating-model-specifications",
    "title": "6 Fitting Models with parsnip",
    "section": "Creating Model Specifications",
    "text": "Creating Model Specifications\n编写许多模型规格，或者记住如何编写生成它们的代码，可能会变得很繁琐。parsnip包包含一个RStudio插件，它可以提供帮助。无论是从插件工具栏菜单中选择这个插件，还是运行以下代码：\nparsnip_addin()\n会在RStudio集成开发环境的查看器面板中打开一个窗口，其中包含每种模型模式的可能模型列表。这些模型可以被写入源代码面板。\n模型列表包含来自CRAN上的parsnip包和parsnip扩展包中的模型。",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/6 Fitting Models with parsnip.html#chapter-summary",
    "title": "6 Fitting Models with parsnip",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了parsnip包，该包使用标准语法为多个R包中的模型提供了统一接口。此接口及生成的对象具有可预测的结构。\n我们接下来将要使用的用于对Ames数据进行建模的代码如下：\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")",
    "crumbs": [
      "6 Fitting Models with parsnip"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html",
    "title": "7 A Model Workflow",
    "section": "",
    "text": "在上一章中，我们讨论了parsnip包，该包可用于定义和拟合模型。本章将介绍一个名为模型工作流的新概念。这一概念（以及相应的workflow()对象）的目的是封装建模过程中的主要部分（在1.5节中讨论）。工作流的重要性体现在两个方面：首先，使用工作流概念有助于形成良好的方法体系，因为它是数据分析中估计部分的单一入口；其次，它能让用户更好地组织项目。这两点将在以下各节中详细讨论。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#where-does-the-model-begin-and-end",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#where-does-the-model-begin-and-end",
    "title": "7 A Model Workflow",
    "section": "Where Does the Model Begin and End?",
    "text": "Where Does the Model Begin and End?\n到目前为止，当我们使用“模型”这一术语时，指的是将一些预测变量与一个或多个结果变量相关联的结构方程。让我们再以线性回归为例来思考。结果数据表示为yᵢ，其中训练集中有i=1…n个样本。假设模型中使用了p个预测变量xᵢ₁，…，xᵢₚ。线性回归会生成一个模型方程：$ _i = _0 + 1x{i1} + + px{ip} $ 。虽然这是一个线性模型，但它仅在参数上是线性的。预测变量可以是非线性项（例如log(xi)）。\n关于建模过程的传统思路是，它只包含模型拟合。对于一些简单的数据集，拟合模型本身可能就是整个过程。然而，在拟合模型之前，通常会有多种选择和额外的步骤：\n\n虽然我们的示例模型有p个预测变量，但通常一开始会有多于p个的候选预测变量。通过探索性数据分析或运用领域知识，一些预测变量可能会被排除在分析之外。在其他情况下，可以使用特征选择算法，以数据驱动的方式为模型选择最小的预测变量集。\n在某些情况下，重要预测变量的值会缺失。此时，我们不必将该样本从数据集中剔除，而是可以利用数据中的其他值来填补这个缺失值。例如，如果x₁的值缺失，但它与预测变量x₂和x₃相关，那么一种填补方法就可以根据x₂和x₃的值来估计缺失的x₁观测值。\n转换预测变量的尺度可能是有益的。如果没有关于新尺度应该是什么的先验信息，我们可以使用统计转换技术、现有数据和一些优化准则来估计合适的尺度。其他转换方法，如主成分分析（PCA），会对一组预测变量进行转换，将它们变成新的特征，用作预测变量。\n\n虽然这些例子都与模型拟合前的步骤有关，但在模型创建后也可能会有一些操作。当创建的分类模型其结果为二元时（例如，event和non-event），通常会使用50%的概率阈值来生成离散的类别预测，这也被称为硬预测。例如，某个分类模型可能会估计某一事件发生的概率为62%。按照通常的默认设置，硬预测结果会是event。然而，该模型可能需要更专注于减少假阳性结果（即，将真正的非事件错误地分类为事件的情况）。实现这一点的一种方法是将阈值从50%提高到某个更高的值。这会提高将新样本判定为事件所需的证据级别。虽然这会降低真阳性率（这是不利的），但它在减少假阳性方面可能会产生更显著的效果。阈值的选择应该利用数据进行优化。这是一个后处理步骤的例子，它对模型的运行效果有着重大影响，尽管它并不包含在模型拟合步骤中。\n关注更广泛的建模过程很重要，而不仅仅是拟合用于估计参数的特定模型。这个更广泛的过程包括任何预处理步骤、模型拟合本身以及潜在的后处理活动。在本书中，我们将把这个更全面的概念称为模型工作流(model workflow)，并重点介绍如何处理其所有组成部分以生成最终的模型方程。在其他软件中，例如Python或Spark，类似的步骤集合被称为管道。在tidymodels中，“管道（pipeline）”一词已经表示通过管道运算符（例如magrittr包中的%&gt;%或更新的原生|&gt;）链接在一起的一系列操作。为了避免在此语境中使用模糊的术语，我们将与建模相关的一系列计算操作称为工作流。\n将数据分析的分析组件整合在一起还有另一个重要原因。后续章节将展示如何准确衡量性能，以及如何优化结构参数（即模型调优）。为了正确量化模型在训练集上的性能，第10章提倡使用重采样方法。要正确做到这一点，分析中任何数据驱动的部分都不应被排除在验证之外。为此，工作流程必须包含所有重要的估计步骤。\n举个例子，考虑主成分分析（PCA）信号提取。我们将在第8.4节和第16章中对此进行更详细的讨论；PCA是一种用新的人工特征替代相关预测变量的方法，这些新特征不相关，并且能捕捉原始集合中的大部分信息。这些新特征可以用作预测变量，而最小二乘回归可以用来估计模型参数。\n关于模型工作流有两种思考方式。Figure 1 展示了错误的方法：将主成分分析（PCA）预处理步骤视为不属于建模工作流的一部分。\n\n\n\n\n\nFigure 1: Incorrect mental model of where model estimation occurs in the data analysis process\n\n\n这里的谬误在于，尽管主成分分析（PCA）为生成主成分进行了大量计算，但其运算被假定为不存在任何相关的不确定性。主成分分析的主成分被视为已知的，而且如果不将其纳入模型工作流，就无法充分衡量主成分分析的效果。\nFigure 2 展示了一种恰当的方法。\n\n\n\n\n\nFigure 2: Correct mental model of where model estimation occurs in the data analysis process\n\n\n通过这种方式，主成分分析预处理被视为建模过程的一部分。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#workflow-basics",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#workflow-basics",
    "title": "7 A Model Workflow",
    "section": "Workflow Basics",
    "text": "Workflow Basics\nworkflows包允许用户将建模对象和预处理对象绑定在一起。让我们再次从Ames数据和一个简单的线性模型开始：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n工作流始终需要一个parsnip模型对象：\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model)\n\nlm_wflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: None\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n请注意，我们尚未指定此工作流应如何预处理数据：Preprocessor: None。如果我们的模型非常简单，可以使用标准的R公式作为预处理器：\n\nlm_wflow &lt;-\n  lm_wflow %&gt;%\n  add_formula(Sale_Price ~ Longitude + Latitude)\n\nlm_wflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n工作流有一个fit()方法，可用于创建模型。使用在第6.6节中创建的对象：\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\nlm_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -302.974       -2.075        2.710\n\n我们还可以在拟合的工作流上使用predict()：\n\npredict(lm_fit, ames_test %&gt;% slice(1:3))\n#&gt; # A tibble: 3 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.22\n#&gt; 2  5.21\n#&gt; 3  5.28\n\npredict()方法遵循我们在6.3节中为parsnip包描述的所有相同规则和命名约定。模型和预处理器都可以被移除或更新：\n\nlm_fit %&gt;% update_formula(Sale_Price ~ Longitude)\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n请注意，在这个新对象中，输出显示之前拟合的模型已被移除，因为新公式与之前的模型拟合不一致。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#adding-raw-variables-to-the-workflow",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#adding-raw-variables-to-the-workflow",
    "title": "7 A Model Workflow",
    "section": "Adding Raw Variables to the workflow()\n",
    "text": "Adding Raw Variables to the workflow()\n\n还有另一种向模型传递数据的接口，即add_variables()函数，它使用类dplyr的语法来选择变量。该函数有两个主要参数：outcomes（结果）和predictors（预测变量）。它们采用类似于tidyverse包的tidyselect后端的选择方法，通过c()来捕获多个选择器。\n\nlm_wflow &lt;-\n  lm_wflow %&gt;%\n  remove_formula() %&gt;%\n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\nlm_wflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n也可以使用更通用的选择器来指定预测变量，例如：predictors = c(ends_with(\"tude\"))。一个便利之处在于，任何意外地在predictors参数中指定的结果列都会被悄无声息地移除。这为以下用法提供了便利：predictors = everything()。\n当模型拟合时，该声明会将这些数据原封不动地整合到一个数据框中，并将其传递给底层函数：\n\nfit(lm_wflow, ames_train)\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -302.974       -2.075        2.710\n\n如果你希望底层建模方法按其通常的方式处理数据，add_variables()会是一个有用的接口。正如我们将在第7.4.1节中看到的，它还能促进更复杂的建模规范。不过，正如我们在下一节中提到的，像glmnet和xgboost这样的模型期望用户从因子预测变量中创建指示变量。在这些情况下，recipe或公式接口通常会是更好的选择。我们将在下一章中介绍更强大的预处理工具——recipe，它也可以添加到工作流程中。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#how-does-a-workflow-use-the-formula",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#how-does-a-workflow-use-the-formula",
    "title": "7 A Model Workflow",
    "section": "How Does a workflow() Use the Formula?",
    "text": "How Does a workflow() Use the Formula?\n回顾第3.2节可知，R语言中的公式方法有多种用途（我们将在第8章进一步讨论这一点）。其中之一是将原始数据正确编码为便于分析的格式。这可能包括执行内嵌转换（例如log(x)）、创建虚拟变量列、创建交互项或其他列扩展等。然而，许多统计方法需要不同类型的编码：\n\n大多数基于树的模型的R包使用公式接口，但不会将分类预测变量编码为虚拟变量。\n某些R包会使用特殊的内联函数来告知模型函数在分析中如何处理预测变量。例如，在生存分析模型中，像strata(site)这样的公式项会表明列site是一个分层变量。这意味着它不应被当作常规预测变量来处理，并且在模型中没有相应的位置参数估计。\n有几个R包以base R函数无法解析或执行的方式扩展了公式。在多水平模型（例如混合模型或分层贝叶斯模型）中，诸如(week | subject)这样的模型项表示列week是一个随机效应，其对于subject列的每个值都有不同的斜率参数估计。\n\n工作流是一种通用接口。当使用add_formula()时，工作流应该如何对数据进行预处理？由于预处理取决于模型，工作流会尽可能模仿底层模型的做法。如果无法做到这一点，公式处理不应对公式中使用的列进行任何操作。让我们更详细地看看这一点。\nSpecial formulas and inline functions\n许多多层模型已经采用了lme4包中设计的公式规范。例如，要拟合一个包含受试者随机效应的回归模型，我们会使用下面的公式。其结果是，每个受试者都会有一个针对age的估计截距和斜率参数。\n\nlibrary(lme4)\n#&gt; Loading required package: Matrix\n#&gt; \n#&gt; Attaching package: 'Matrix'\n#&gt; The following objects are masked from 'package:tidyr':\n#&gt; \n#&gt;     expand, pack, unpack\nlibrary(nlme)\n#&gt; \n#&gt; Attaching package: 'nlme'\n#&gt; The following object is masked from 'package:lme4':\n#&gt; \n#&gt;     lmList\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     collapse\nlmer(distance ~ Sex + (age | Subject), data = Orthodont)\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: distance ~ Sex + (age | Subject)\n#&gt;    Data: Orthodont\n#&gt; REML criterion at convergence: 471.1635\n#&gt; Random effects:\n#&gt;  Groups   Name        Std.Dev. Corr \n#&gt;  Subject  (Intercept) 7.3912        \n#&gt;           age         0.6943   -0.97\n#&gt;  Residual             1.3100        \n#&gt; Number of obs: 108, groups:  Subject, 27\n#&gt; Fixed Effects:\n#&gt; (Intercept)    SexFemale  \n#&gt;      24.517       -2.145\n\n标准的R方法无法正确处理这个公式，导致生成一个0行的数据框：\n\nmodel.matrix(distance ~ Sex + (age | Subject), data = Orthodont)\n#&gt; Warning in Ops.ordered(age, Subject): '|' is not meaningful for ordered\n#&gt; factors\n#&gt;      (Intercept) SexFemale age | SubjectTRUE\n#&gt; attr(,\"assign\")\n#&gt; [1] 0 1 2\n#&gt; attr(,\"contrasts\")\n#&gt; attr(,\"contrasts\")$Sex\n#&gt; [1] \"contr.treatment\"\n#&gt; \n#&gt; attr(,\"contrasts\")$`age | Subject`\n#&gt; [1] \"contr.treatment\"\n\n这个特殊公式必须由底层的包代码来处理，而不是标准的model.matrix()方法。即使这个公式可以与model.matrix()一起使用，这仍然会带来问题，因为该公式还指定了模型的统计属性。workflows中的解决方案是使用add_model()补充模型公式——add_variables()明确了预测因子和结果的列名，add_model()中提供实际公式：\n\nlibrary(multilevelmod)\n\nmultilevel_spec &lt;- linear_reg() %&gt;% set_engine(\"lmer\")\n\nmultilevel_workflow &lt;-\n  workflow() %&gt;%\n  # Pass the data along as-is:\n  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %&gt;%\n  add_model(multilevel_spec,\n    # This formula is given to the model\n    formula = distance ~ Sex + (age | Subject)\n  )\n\nmultilevel_fit &lt;- fit(multilevel_workflow, data = Orthodont)\nmultilevel_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Outcomes: distance\n#&gt; Predictors: c(Sex, age, Subject)\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: distance ~ Sex + (age | Subject)\n#&gt;    Data: data\n#&gt; REML criterion at convergence: 471.1635\n#&gt; Random effects:\n#&gt;  Groups   Name        Std.Dev. Corr \n#&gt;  Subject  (Intercept) 7.3912        \n#&gt;           age         0.6943   -0.97\n#&gt;  Residual             1.3100        \n#&gt; Number of obs: 108, groups:  Subject, 27\n#&gt; Fixed Effects:\n#&gt; (Intercept)    SexFemale  \n#&gt;      24.517       -2.145\n\n我们甚至可以使用前面提到的生存分析包（survival）中的strata()函数来进行生存分析：\n\nlibrary(censored)\n#&gt; Loading required package: survival\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;-\n  workflow() %&gt;%\n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;%\n  add_model(parametric_spec,\n    formula = Surv(futime, fustat) ~ age + strata(rx)\n  )\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#creating-multiple-workflows-at-once",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#creating-multiple-workflows-at-once",
    "title": "7 A Model Workflow",
    "section": "Creating Multiple Workflows at Once",
    "text": "Creating Multiple Workflows at Once\n在某些情况下，数据需要多次尝试才能找到合适的模型。例如：\n\n对于预测模型，建议评估多种不同的模型类型。这需要用户创建多个模型规格。\n模型的序贯检验通常从一组扩展的预测变量开始。这种“全模型”会与一系列相同的模型进行比较，这些模型依次移除了每个预测变量。通过基本的假设检验方法或经验验证，可以分离并评估每个预测变量的影响。\n\n在这些情况下以及其他一些情况中，根据不同的预处理程序集和/或模型规格创建大量工作流可能会变得繁琐或繁重。为了解决这个问题，workflowset包会创建工作流组件的组合。一系列预处理程序（例如公式、dplyr选择器，或下一章将讨论的特征工程recipe对象）可以与一系列模型规格相结合，从而生成一套工作流。\n举个例子，假设我们想重点关注Ames数据中房屋位置的不同表示方式。我们可以创建一组公式来捕捉这些预测变量：\n\nlocation &lt;- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\n可以使用workflow_set()函数将这些表示形式与一个或多个模型进行组合。我们将仅使用之前的线性模型规格来进行演示：\n\nlibrary(workflowsets)\nlocation_models &lt;- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n#&gt; # A workflow set/tibble: 4 × 4\n#&gt;   wflow_id        info             option    result    \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\nlocation_models$info[[1]]\n#&gt; # A tibble: 1 × 4\n#&gt;   workflow   preproc model      comment\n#&gt;   &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  \n#&gt; 1 &lt;workflow&gt; formula linear_reg \"\"\nextract_workflow(location_models, id = \"coords_lm\")\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n工作流集主要设计用于处理重采样，这在第10章中会进行讨论。option列和result列必须填充从重采样中得到的特定类型的对象。我们将在第11章和第15章中更详细地演示这一点。\n与此同时，让我们为每个公式创建模型拟合，并将它们保存在一个名为fit的新列中。我们将使用基本的dplyr和purrr操作：\n\nlocation_models &lt;-\n  location_models %&gt;%\n  mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n#&gt; # A workflow set/tibble: 4 × 5\n#&gt;   wflow_id        info             option    result     fit       \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;     &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\nlocation_models$fit[[1]]\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude  \n#&gt;    -184.396       -2.025\n\n我们在这里使用了一个purrr函数来遍历我们的模型，但有一种更简单、更好的方法来拟合工作流集合，这将在第11.1节中介绍。\n总的来说，工作流集还有很多内容值得探讨！虽然我们在这里介绍了基础知识，但工作流集的细微差别和优势要到第15章才会详细阐述。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#evaluating-the-test-set",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#evaluating-the-test-set",
    "title": "7 A Model Workflow",
    "section": "Evaluating the Test Set",
    "text": "Evaluating the Test Set\n假设我们已经完成了模型开发，并确定了最终模型。有一个便捷函数叫做last_fit()，它会用整个训练集对模型进行拟合，并用测试集对其进行评估。\n以lm_wflow为例，我们可以将模型以及初始的训练/测试集拆分传递给该函数：\n\nfinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 × 6\n#&gt;   splits             id               .metrics         .notes   .predictions\n#&gt;   &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;   &lt;list&gt;      \n#&gt; 1 &lt;split [2342/588]&gt; train/test split &lt;tibble [2 × 4]&gt; &lt;tibble&gt; &lt;tibble&gt;    \n#&gt; # ℹ 1 more variable: .workflow &lt;list&gt;\n\n请注意，last_fit()接受的数据是已分割的，而不是原始数据框。该函数使用此分割好的数据集来生成用于最终拟合和评估的训练集和测试集。\n.workflow列包含拟合的工作流，可以使用以下代码从结果中提取出来：\n\nfitted_lm_wflow &lt;- extract_workflow(final_lm_res)\n\n同样地，collect_metrics()和collect_predictions()分别提供了获取性能指标和预测结果的途径。\n\ncollect_metrics(final_lm_res)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimator .estimate .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 rmse    standard       0.164 pre0_mod0_post0\n#&gt; 2 rsq     standard       0.189 pre0_mod0_post0\ncollect_predictions(final_lm_res) %&gt;% slice(1:5)\n#&gt; # A tibble: 5 × 5\n#&gt;   .pred id               Sale_Price  .row .config        \n#&gt;   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          \n#&gt; 1  5.22 train/test split       5.02     2 pre0_mod0_post0\n#&gt; 2  5.21 train/test split       5.39     4 pre0_mod0_post0\n#&gt; 3  5.28 train/test split       5.28     5 pre0_mod0_post0\n#&gt; 4  5.27 train/test split       5.28     8 pre0_mod0_post0\n#&gt; 5  5.28 train/test split       5.28    10 pre0_mod0_post0\n\n我们将在16.6节中进一步了解last_fit()的实际应用以及如何再次使用它。\n在使用验证集时，last_fit()有一个名为add_validation_set的参数，用于指定最终模型是仅在训练集上训练（默认设置），还是在训练集和验证集的组合上训练。",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/7 A Model Workflow.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/7 A Model Workflow.html#chapter-summary",
    "title": "7 A Model Workflow",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n在本章中，你了解到建模过程不仅仅包括估计将预测变量与结果联系起来的算法参数。这个过程还包括预处理步骤以及模型拟合后的操作。我们引入了一个名为模型工作流的概念，它可以捕捉建模过程中的重要组成部分。多个工作流也可以在一个工作流集合中创建。last_fit()函数便于将最终模型拟合到训练集并使用测试集进行评估。\n对于Ames数据集，其使用的相关代码如下：\nlibrary(tidymodels)\ndata(ames)\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_fit &lt;- fit(lm_wflow, ames_train)",
    "crumbs": [
      "7 A Model Workflow"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html",
    "title": "8 Feature Engineering with recipes",
    "section": "",
    "text": "特征工程(Feature Engineering)需要重新格式化预测变量的值，使其更易于模型有效使用。这包括对数据进行转换(transformation)和编码(encoding)，以最佳方式呈现其重要特征。想象一下，在一个数据集中有两个预测变量，将它们以比率的形式在模型中呈现会更有效；根据这两个原始变量的比率创建一个新的预测变量，就是特征工程的一个简单例子。\n以Ames市一栋房屋的位置为例，有多种方式可以将这种空间信息提供给模型，包括所在社区（一种定性衡量标准）、经度/纬度、到最近学校或Iowa State University的距离等等。在选择如何在建模中对这些数据进行编码时，我们可能会选择一个我们认为与结果关联性最强的选项。数据的原始格式，例如数值型（如距离）与分类型（如社区），也是特征工程选择中的一个驱动因素。\n特征工程的其他预处理示例包括：\n不同的模型有不同的预处理要求，有些模型，例如基于树的模型，几乎不需要任何预处理。附录A包含一个小表格，列出了针对不同模型的推荐预处理技术。在本章中，我们将介绍recipes包，你可以使用它将不同的特征工程和预处理任务组合成一个单一对象，然后将这些转换应用于不同的数据集。与用于模型的parsnip类似，recipes包是tidymodels核心包之一。\n本章使用了Ames房价数据以及本书到目前为止创建的R对象，如第7.7节所概述的那样。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#a-simple-recipe-for-the-ames-housing-data",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#a-simple-recipe-for-the-ames-housing-data",
    "title": "8 Feature Engineering with recipes",
    "section": "A Simple recipe() for the Ames Housing Data",
    "text": "A Simple recipe() for the Ames Housing Data\n在本节中，我们将重点关注Ames房产数据中可用预测变量的一小部分：\n\n社区（定性的，训练集中有29个社区）\n地上总居住面积（连续变量，命名为Gr_Liv_Area）\n建造年份（Year_Built）\n建筑物类型（Bldg_Type，其中OneFam(n = 1,936)，TwoFmCon(n = 50)，Duplex(n = 88)，Twnhs(n = 77)，TwnhsE(n = 191)）\n\n假设最初的普通线性回归模型是根据这些数据拟合的。回想一下，在第4章中，销售价格已经预先进行了对数转换，一个标准的lm()调用可能如下所示：\nlm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)\n当执行此函数时，数据会从数据框转换为数值型的设计矩阵（也称为模型矩阵），然后使用最小二乘法来估计参数。在3.2节中，我们列出了R模型公式的多种用途；现在我们仅关注数据处理方面。该公式的作用可以分解为一系列步骤：\n\n销售价格被定义为结果，而社区、总居住面积、建造年份和建筑类型等变量都被定义为预测因子。\n对总居住面积这一预测变量进行了对数转换。\n社区和建筑类型列从非数字型转换为数字型（因为最小二乘法需要数字型预测变量）。\n\n如第3章所述，~公式会将这些数据处理操作应用于任何传入predict()函数的数据，包括新数据。\n与建模函数中的~公式不同，recipe通过step_*()函数来定义步骤，但不会立即执行这些步骤，而是定义一个包含一系列数据处理步骤的对象；它仅仅是一种对应该做什么的规范说明。以下是一个与前面的~公式等效的recipe对象，其基于第5.5节中的代码摘要构建而成：\n\nlibrary(tidymodels) # Includes the recipes package\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\ndata(ames)\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\nsimple_ames &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n    data = ames_train\n  ) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_dummy(all_nominal_predictors())\nsimple_ames\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Dummy variables from: all_nominal_predictors()\n\n让我们来详细分析一下：\n\n带有公式的recipe()调用会告知“配方”的“原料”（例如，预测变量、结果变量）。它仅使用ames_train数据来确定各列的数据类型。\nstep_log()声明应对Gr_Liv_Area进行对数转换。\nstep_dummy()指定了哪些变量应该从定性型转换为定量型，在这种情况下，会使用虚拟变量或指示变量。指示变量或虚拟变量是一种二元数值变量（由1和0组成的列），用于编码定性信息；我们将在第8.4.1节更深入地探讨这类变量。\n\n函数all_nominal_predictors()会捕获当前为因子型或字符型（即名义型）的任何预测变量列的名称。这是一个类似dplyr的选择器函数，与starts_with()或matches()类似，但只能在recipe对象内部使用。recipe包特有的其他选择器包括：all_numeric_predictors()、all_numeric()、all_predictors()和all_outcomes()。与dplyr类似，可以使用一个或多个未加引号的表达式（用逗号分隔）来选择每个步骤所作用的列。\n与公式或原始预测变量相比，使用recipe有几个优势，包括：\n\n由于这些计算与建模函数并非紧密耦合，因此它们可以在不同模型之间重复使用。\n与~公式相比，recipe能提供更广泛的数据处理选择。\n语法可以非常简洁。例如，all_nominal_predictors()可用于捕获许多变量以进行特定类型的处理，而公式则需要明确列出每个变量。\n所有数据处理都可以被捕获在一个单独的R对象中，而不是在重复的脚本中，甚至不会分散在不同的文件里。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#using-recipes",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#using-recipes",
    "title": "8 Feature Engineering with recipes",
    "section": "Using Recipes",
    "text": "Using Recipes\n正如我们在第7章中讨论的那样，预处理选项和特征工程通常应被视为建模工作流的一部分，而非一项独立任务。workflows包包含用于处理不同类型预处理器的高级函数。我们之前的工作流（lm_wflow）使用了一组简单的dplyr选择器：\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\n为了通过更复杂的特征工程改进这种方法，让我们使用simple_ames对象对建模数据进行预处理。此对象可附加到工作流中：\n\nlm_wflow %&gt;%\n  add_recipe(simple_ames)\n#&gt; Error in `add_recipe()`:\n#&gt; ! A recipe cannot be added when variables already exist.\n\n直接添加行不通，因为工作流只能有一种预处理方法，所以在添加recipe对象之前，需要先移除现有的预处理器。\n\nlm_wflow &lt;-\n  lm_wflow %&gt;%\n  remove_variables() %&gt;%\n  add_recipe(simple_ames)\nlm_wflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_log()\n#&gt; • step_dummy()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n让我们通过一个简单的fit()调用同时估计recipe对象和模型：\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\npredict()方法会先对新数据应用与训练集相同的预处理，然后再将其传递给模型的predict()方法：\n\npredict(lm_fit, ames_test %&gt;% slice(1:3))\n#&gt; Warning in predict.lm(object = object$fit, newdata = new_data, type =\n#&gt; \"response\", : prediction from rank-deficient fit; consider predict(.,\n#&gt; rankdeficient=\"NA\")\n#&gt; # A tibble: 3 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.08\n#&gt; 2  5.32\n#&gt; 3  5.28\n\n如果我们只需要模型对象或recipe对象，可以使用extract_*函数来获取它们：\n\n# Get the recipe after it has been estimated:\nlm_fit %&gt;%\n  extract_recipe(estimated = TRUE)\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area | Trained\n#&gt; • Dummy variables from: Neighborhood Bldg_Type | Trained\n\n# To tidy the model fit:\nlm_fit %&gt;%\n  # This returns the parsnip object:\n  extract_fit_parsnip() %&gt;%\n  # Now tidy the linear model object:\n  tidy() %&gt;%\n  slice(1:5)\n#&gt; # A tibble: 5 × 5\n#&gt;   term                       estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)                -0.669    0.231        -2.90 3.80e-  3\n#&gt; 2 Gr_Liv_Area                 0.620    0.0143       43.2  2.63e-299\n#&gt; 3 Year_Built                  0.00200  0.000117     17.1  6.16e- 62\n#&gt; 4 Neighborhood_College_Creek  0.0178   0.00819       2.17 3.02e-  2\n#&gt; 5 Neighborhood_Old_Town      -0.0330   0.00838      -3.93 8.66e-  5\n\n在工作流对象之外使用（和调试）recipe对象的工具在第16.4节中有描述。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#how-data-are-used-by-the-recipe",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#how-data-are-used-by-the-recipe",
    "title": "8 Feature Engineering with recipes",
    "section": "How Data Are Used by the recipe()\n",
    "text": "How Data Are Used by the recipe()\n\n数据在不同阶段被传递给recipe对象。\n\n首先，在调用recipe(..., data)时，数据集用于确定每列的数据类型，以便可以使用诸如all_numeric()或all_numeric_predictors()之类的选择器。\n其次，在使用fit(workflow, data)准备数据时，训练集会用于所有估计操作，包括可能作为workflow一部分的recipe对象，从确定因子水平到计算主成分分析（PCA）组件，以及介于两者之间的所有操作。所有预处理和特征工程步骤都仅使用训练数据。否则，信息泄露可能会在模型处理新数据时对其性能产生负面影响。\n最后，在使用predict(workflow, new_data)时，不会利用new_data中的值重新估计任何模型或预处理器参数（如来自recipe对象的那些参数）。以使用step_normalize()进行中心化和标准化为例，在调用predict()时，预测时的新样本会使用从训练集中确定的相应列的均值和标准差进行标准化。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#examples-of-recipe-steps",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#examples-of-recipe-steps",
    "title": "8 Feature Engineering with recipes",
    "section": "Examples of Recipe Steps",
    "text": "Examples of Recipe Steps\n在继续之前，让我们深入了解一些recipe包中的函数，并探讨一些最重要的step_*()函数。这些步骤函数各自指定了特征工程过程中一个特定的可能步骤，并且不同的步骤函数会对数据列产生不同的影响。\nEncoding qualitative data in a numeric format\n特征工程中最常见的任务之一是对分类数据或定性数据（因子或字符）进行转换，以便对其进行编码或以数值形式表示。有时，在进行此类转换之前，我们可以以有用的方式修改定性列的因子水平。例如，step_unknown()可用于将缺失值更改为一个专用的因子水平。同样，如果我们预计在未来的数据中可能会遇到新的因子水平，step_novel()可以为此分配一个新的水平。\n此外，step_other()可用于分析训练集中因子水平的频率，并将不常出现的值转换为一个通用的“other”水平，其阈值可以指定。我们数据中的Neighborhood预测变量就是一个很好的例子，如 Figure 1 所示。\n\nggplot(ames_train, aes(y = Neighborhood)) +\n  geom_bar() +\n  labs(y = NULL)\n\n\n\n\n\n\nFigure 1: Frequencies of neighborhoods in the Ames training set.\n\n\n\n\n在这里我们可以看到，有两个社区在训练集中的房产数量少于5处（Landmark和Green Hills）；在这种情况下，测试集中完全没有包含Landmark社区的房屋。对于某些模型而言，列中存在只有一个非零项的虚拟变量可能会产生问题。至少，这些特征不太可能对模型起到重要作用。如果我们在recipe对象中添加step_other(Neighborhood, threshold = 0.01)，那么占比低于1%的社区将会被归为一个名为“other”的新类别。在这个训练集中，这将涵盖7个社区。\n对于Ames数据，我们可以修改recipe对象以使用：\n\nsimple_ames &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n    data = ames_train\n  ) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n许多（但并非所有）基础模型计算都要求将预测变量值编码为数字，但基于树的模型、基于规则的模型和朴素贝叶斯模型除外。\n将因子预测变量转换为数值格式最常用的方法是创建虚拟变量或指示变量。让我们以Ames数据中关于建筑类型的预测变量为例，这是一个具有五个水平的因子变量（见 Table 1 ）。对于虚拟变量，单个Bldg_Type列将被四个数值列取代，这些列的值要么是0要么是1，这些二元变量代表特定的因子水平值。在R语言中，惯例是排除第一个因子水平所对应的列（在这种情况下是OneFam）。Bldg_Type列将被一个名为TwoFmCon的列取代，当行中是该值时，该列的值为1，否则为0。其他三个列也以类似的方式创建：\n\n\nTable 1: Illustration of binary encodings (i.e., dummy variables) for a qualitative predictor.\n\n\n\nRaw Data\nTwoFmCon\nDuplex\nTwnhs\nTwnhsE\n\n\n\nOneFam\n0\n0\n0\n0\n\n\nTwoFmCon\n1\n0\n0\n0\n\n\nDuplex\n0\n1\n0\n0\n\n\nTwnhs\n0\n0\n1\n0\n\n\nTwnhsE\n0\n0\n0\n1\n\n\n\n\n\n\n为什么不是五个都用呢？最基本的原因是简洁性：如果你知道这四个列的值，你就能确定最后一个的值，因为这些类别是互斥的。从更专业的角度来说，经典的理由是，包括普通线性回归在内的许多模型，当列之间存在线性相关性时，会出现数值问题。如果将所有五个建筑类型指示列都包含进来，它们加起来就会等于截距列（如果有的话）。这会在底层的矩阵代数中会导致问题，甚至可能是直接的错误。\n完整的编码集可用于某些模型。这传统上被称为独热编码，可通过step_dummy()的one_hot参数来实现。\nstep_dummy()的一个实用功能是，能更好地控制生成的虚拟变量的命名方式。在基础R中，虚拟变量的名称会将变量名和水平值合并，形成类似NeighborhoodVeenker这样的名称。而recipes默认使用下划线作为名称和水平值之间的分隔符（例如，Neighborhood_Veenker），并且还提供了自定义名称格式的选项。Recipes中的默认命名规则，使得在后续步骤中更容易使用选择器捕获这些新列，比如starts_with(\"Neighborhood_\")。\n传统的虚拟变量要求必须已知所有可能的类别，才能创建一整套数值特征，有些方法不需要，可以直接将其转换为数值格式。如：特征哈希方法仅根据类别的值将其分配到预定义的虚拟变量池中；效应编码或似然编码用一个单一的数值列替换原始数据，该数值列用于衡量这些数据的效应。特征哈希和效应编码都能无缝处理数据中出现新因子水平的情况。第17章探讨了这些以及其他用于编码分类数据的方法，这些方法超越了简单的虚拟变量或指示变量。\n不同的recipe步骤函数应用于数据中的变量时，表现会有所不同。例如，step_log()会在不更改名称的情况下就地修改列。其他步骤，如step_dummy()，会删除原始数据列，并替换为一个或多个名称不同的列。recipe步骤函数的效果取决于所进行的特征工程转换类型。\nInteraction terms\n交互效应涉及两个或多个预测变量。当一个预测变量对结果的影响取决于一个或多个其他预测变量时，就会产生这种效应。例如，如果你试图预测通勤期间的交通流量，两个潜在的预测变量可能是你通勤的具体时间和天气。然而，交通流量与恶劣天气之间的关系在一天中的不同时间是不同的。在这种情况下，你可以在模型中添加这两个预测变量之间的交互项，以及原来的两个预测变量（称为主效应）。从数值上讲，预测变量之间的交互项被编码为它们的乘积。交互是根据它们对结果的影响来定义的，并且可以是不同类型数据（如数值型、分类型等）的组合。第7章更详细地讨论了交互效应以及如何检测它们。\n在探究Ames训练集后，我们可能会发现，不同建筑类型的总居住面积的回归斜率存在差异，如 Figure 2 所示。\n\nggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +\n  geom_point(alpha = .2) +\n  facet_wrap(~Bldg_Type) +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n\n\n\n\n\n\nFigure 2: Gross living area (in log-10 units) versus sale price (also in log-10 units) for five different building types\n\n\n\n\n在recipe对象中如何指定交互项？基础R公式会使用:来表示交互项，所以我们会这样使用：\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + log10(Gr_Liv_Area):Bldg_Type\n# or\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type\n其中，*会将这些列扩展为主效应项和交互项。同样，~公式方法会同时处理多项任务，它知道因子变量（例如Bldg_Type）应首先扩展为虚拟变量，并且交互项应涉及所有生成的二进制列。\nrecipe更为明确且具有顺序性，能让你获得更多控制权。在当前recipe对象中，step_dummy()已经创建了虚拟变量。我们该如何将这些变量组合起来进行交互呢？额外的步骤会像step_interact(~ interaction terms)这样，其中波浪号右侧的项就是交互项。这些项可以包含选择器，因此使用以下方式是合适的：\n\nsimple_ames &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n    data = ames_train\n  ) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\"))\n\n可以在该公式中通过用+分隔来指定额外的交互项。另请注意，此规则仅适用不同变量之间的交互项；如果公式中使用了var_1:var_1，则该术语将被忽略。\n假设在一个recipe对象中，我们尚未对建筑类型创建虚拟变量。在构建交互项时包含一个因子列是不合适的，例如：step_interact(~ Gr_Liv_Area:Bldg_Type)，此时step_interact()所使用的底层（基础R）代码会自动创建虚拟变量然后形成交互项。实际上，如果出现这种情况，会有一个警告提示这可能会产生意外结果。\n与命名虚拟变量一样，recipe为交互项提供了更连贯的名称。在这种情况下，交互项被命名为Gr_Liv_Area_x_Bldg_Type_Duplex，而不是Gr_Liv_Area:Bldg_TypeDuplex（这不是数据框的有效列名）。\n总之，recipe为你提供了更多的控制权，但它与R的标准模型公式不同，且步骤的顺序很重要。例如，总居住面积在形成交互项之前会进行对数转换，此后与该变量的交互也将使用对数尺度。\nSpline functions\n当预测变量与结果之间存在非线性关系时，某些类型的预测模型能够在训练过程中自适应地逼近这种关系。然而，通常来说，越简单越好，尝试使用简单模型（如线性拟合）并为可能需要的预测变量（例如Ames房价数据中的经度和纬度）添加特定的非线性特征的情况并不少见。实现这一点的一种常用方法是使用样条函数来表示数据。样条函数会用一组列替换现有的数值预测变量，使模型能够模拟灵活的非线性关系。随着向数据中添加的样条项增多，非线性表示这种关系的能力也会增强。但遗憾的是，这也可能增加捕捉到偶然出现的数据趋势的可能性（即过拟合）。\n如果你曾经在ggplot中使用过geom_smooth()，那么你很可能使用过数据的样条表示。例如， Figure 3 中的每个面板都对纬度预测变量使用了不同数量的平滑样条：\n\nlibrary(patchwork)\nlibrary(splines)\n\nplot_smoother &lt;- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) +\n    geom_point(alpha = .2) +\n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(\n      title = paste(deg_free, \"Spline Terms\"),\n      y = \"Sale Price (USD)\"\n    )\n}\n\n(plot_smoother(2) + plot_smoother(5)) / (plot_smoother(20) + plot_smoother(100))\n\n\n\n\n\n\nFigure 3: Sale price versus latitude, with trend lines using natural splines with different degrees of freedom\n\n\n\n\n“splines”包中的ns()函数(“natural splines”)可以生成特征列。\nFigure 3 中的一些面板明显拟合效果不佳；n=2时对数据存在欠拟合，而n=100时则存在过拟合。n=5和n=20的面板看起来拟合得相当平滑，捕捉到了数据的主要模式。这表明适当程度的“非线性”很重要。样条项的数量可以被视为该模型的一个调优参数。这类参数将在第12章中探讨。\n在recipe中，多个步骤函数可以创建“样条”，要添加自然样条可以：\n\nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n  data = ames_train\n) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, deg_free = 20)\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 5\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Collapsing factor levels for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude\n\n用户需要确定“社区”和“纬度”是否都应该包含在模型中，因为它们以不同的方式代表着相同的基础数据。\nFeature extraction\n另一种同时表示多个特征的常用方法是特征提取。这些技术大多会从预测变量中创建新的特征，以整体捕捉更广泛集合中的信息。例如，主成分分析（PCA）试图使用更少数量的特征，尽可能多地提取预测变量集合中的原始信息。PCA是一种线性提取方法，这意味着每个新特征都是原始预测变量的线性组合。PCA的一个优点是，每个新特征（称为主成分或PCA得分）彼此不相关。正因为如此，PCA在降低预测变量之间的相关性方面非常有效。需要注意的是，PCA只关注预测变量新的PCA特征可能与结果无关。\n在Ames数据中，有几个预测变量用于衡量房产的面积，例如总地下室面积（Total_Bsmt_SF）、一楼面积（First_Flr_SF）、总居住面积（Gr_Liv_Area）等等。主成分分析（PCA）可能是将这些潜在冗余变量表示为更小特征集的一种选择。除了总居住面积外，这些预测变量的名称中都带有后缀SF（代表平方英尺），因此用于主成分分析的一个recipe步骤可能如下：\n# Use a regular expression to capture house size predictors:\nstep_pca(matches(\"(SF$)|(Gr_Liv)\"))\n请注意，所有这些列的测量单位都是平方英尺。主成分分析（PCA）假设所有预测变量都在同一尺度上。在这种情况下确实如此，但通常在此步骤之前会使用step_normalize()对每一列进行中心化和标准化处理。\n对于其他提取方法，已有现成的recipe步骤，例如：独立成分分析（ICA）、非负矩阵分解（NNMF）、多维尺度分析（MDS）、均匀流形逼近与投影（UMAP）等。\nRow sampling steps\nrecipe中也有影响数据集行的步骤函数。例如，用于类别不平衡的子采样技术会改变提供给模型的数据中的类别比例；这些技术通常不会提高整体性能，但可以生成表现更好的预测类别概率分布。以下是在处理类别不平衡数据时尝试子采样的一些方法：\n\n下采样数据会保留少数类，并对多数类进行随机抽样，以使类频率达到平衡。\n过采样通过复制少数类的样本来平衡类别。一些技术通过合成与少数类数据相似的新样本来实现这一点，而另一些方法则只是简单地重复添加相同的少数类样本。\n混合方法则是将两者结合起来。\n\n“themis”包提供一些recipe类步骤函数，可通过子采样来解决类别不平衡问题。对于简单的下采样，我们会使用：step_downsample(outcome_column_name)\n只有训练集才应该受到这些技术的影响。在使用该流程处理时，测试集或其他保留样本应保持原样。因此，所有子采样步骤都将skip参数的默认值设为TRUE（第8.5节）。\n其他基于行的步骤函数：step_filter()、step_sample()、step_slice()以及step_arrange()。在这些步骤函数的几乎所有使用场景中，skip参数都应设置为TRUE。\nGeneral transformations\n与原始的dplyr操作类似，step_mutate()可用于对数据执行各种基本操作。它最适合用于简单的转换，例如计算两个变量的比率，比如Bedroom_AbvGr / Full_Bath，即Ames住房数据中卧室与浴室的比率。\n使用这个灵活的步骤时，要格外小心，避免在预处理过程中出现数据泄露。例如，考虑这样一种转换：x = w &gt; mean(w)。当将其应用于新数据或测试数据时，这种转换会使用w在新数据中的均值，而不是w在训练数据中的均值。\nNatural language processing\nrecipe也能处理那些并非传统结构（即列代表特征）的数据。例如，“textrecipes”包可以对数据应用自然语言处理方法。输入列通常是一个文本字符串，我们可以通过不同的步骤对数据进行分词（例如，将文本拆分成单独的词语）、过滤掉某些词，并创建适合建模的新特征。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#skipping-steps-for-new-data",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#skipping-steps-for-new-data",
    "title": "8 Feature Engineering with recipes",
    "section": "Skipping Steps for New Data",
    "text": "Skipping Steps for New Data\n有些对训练集的预处理步骤，需要在测试集中避免使用。\n第一类：log转换\nAmes数据框中的销售价格数据已经过对数转换。为什么不使用：step_log(Sale_Price, base = 10)\n当该recipe步骤应用于售价未知的新房产时，这会导致失败。由于价格是我们要预测的内容，数据中可能不会有这个变量的列。事实上，为了避免信息泄露，许多tidymodels包在进行任何预测时都会隔离所使用的数据。这意味着在预测时，训练集和任何结果列都无法使用。\n对于结果列的简单转换，我们强烈建议这些操作在recipe之外进行。\n第二类：行处理\n然而，在其他情况下，这并非一个恰当的解决方案。例如，在存在严重类别不平衡的分类模型中，对提供给建模函数的数据进行子抽样是很常见的做法。举例来说，假设有两个类别，事件发生率为10%。一种简单但存在争议的方法是对数据进行下采样，即向模型提供所有的事件样本以及10%随机选取的非事件样本。\n问题在于，不应将相同的子采样过程应用于待预测的数据。因此，在使用recipe时，我们需要一种机制来确保某些操作仅应用于提供给模型的数据。每个步骤函数都有一个名为skip的选项，当该选项设置为TRUE时，predict()函数会忽略该步骤。通过这种方式，你可以隔离那些影响建模数据的步骤，而不会在应用于新样本时产生错误。不过，在使用fit()时，所有步骤都会被应用。\n在撰写本文时，recipes和themis包中应仅用于训练数据的步骤函数有：step_adasyn()、step_bsmote()、step_downsample()、step_filter()、step_naomit()、step_nearmiss()、step_rose()、step_sample()、step_slice()`。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#tidy-a-recipe",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#tidy-a-recipe",
    "title": "8 Feature Engineering with recipes",
    "section": "Tidy a recipe()\n",
    "text": "Tidy a recipe()\n\n在第3.3节中，我们介绍了用于统计对象的tidy()动词。对于recipe对象以及各个recipe步骤，也有一种tidy()方法。在继续之前，让我们使用本章讨论过的一些新步骤，为Ames数据创建一个扩展recipe：\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\n当用recipe对象调用tidy()方法时，会给出recipe步骤的摘要：\n\ntidy(ames_rec)\n#&gt; # A tibble: 5 × 6\n#&gt;   number operation type     trained skip  id            \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n#&gt; 1      1 step      log      FALSE   FALSE log_NSTJP     \n#&gt; 2      2 step      other    FALSE   FALSE other_5HSyx   \n#&gt; 3      3 step      dummy    FALSE   FALSE dummy_DMSaM   \n#&gt; 4      4 step      interact FALSE   FALSE interact_ULMsR\n#&gt; 5      5 step      ns       FALSE   FALSE ns_HMcZA\n\n这一结果有助于识别各个步骤，或许之后就能对某一特定步骤执行tidy()方法。\n我们可以在任何步骤函数调用中指定id参数；否则，它会使用随机后缀生成。如果同一类型的步骤被多次添加到recipe中，设置这个值会很有帮助。让我们提前为step_other()指定id，因为我们想要对其执行tidy()：\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\n我们将用这个新recipe重新调整工作流程：\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\n可以再次调用tidy()方法，并结合我们指定的id标识符，以获取应用step_other()后的结果：\n\nestimated_recipe &lt;-\n  lm_fit %&gt;%\n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe, id = \"my_id\")\n#&gt; # A tibble: 22 × 3\n#&gt;   terms        retained           id   \n#&gt;   &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt; 1 Neighborhood North_Ames         my_id\n#&gt; 2 Neighborhood College_Creek      my_id\n#&gt; 3 Neighborhood Old_Town           my_id\n#&gt; 4 Neighborhood Edwards            my_id\n#&gt; 5 Neighborhood Somerset           my_id\n#&gt; 6 Neighborhood Northridge_Heights my_id\n#&gt; # ℹ 16 more rows\n\n我们在这里看到的使用tidy()得到的结果显示了哪些因子水平被保留了下来，也就是说，没有被添加到新的“其他”类别中。\n如果我们需要知道recipe中的哪一步，tidy()方法也可以与number标识符一起调用：\n\ntidy(estimated_recipe, number = 2)\n#&gt; # A tibble: 22 × 3\n#&gt;   terms        retained           id   \n#&gt;   &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt; 1 Neighborhood North_Ames         my_id\n#&gt; 2 Neighborhood College_Creek      my_id\n#&gt; 3 Neighborhood Old_Town           my_id\n#&gt; 4 Neighborhood Edwards            my_id\n#&gt; 5 Neighborhood Somerset           my_id\n#&gt; 6 Neighborhood Northridge_Heights my_id\n#&gt; # ℹ 16 more rows\n\n每个tidy()方法都会返回关于该步骤的相关信息。例如，tidy()方法用于step_dummy()时，会返回一列包含被转换为哑变量的变量，以及另一列包含每列所有已知水平的信息。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#column-roles",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#column-roles",
    "title": "8 Feature Engineering with recipes",
    "section": "Column Roles",
    "text": "Column Roles\n当在初次调用recipe()使用公式时，它会根据各列位于波浪号的哪一侧，为每一列分配角色。这些角色要么是\"predictor\"，要么是\"outcome\"。不过，也可以根据需要分配其他角色。\n例如，在我们的Ames数据集中，原始数据包含一个地址列。保留该列可能是有用的，这样在做出预测后，就可以对有问题的结果进行详细调查。换句话说，即使该列既不是预测变量也不是结果变量，它也可能很重要。\n为了解决这个问题，add_role()、remove_role()和update_role()函数会很有帮助。例如，对于房价数据，可以使用以下方式修改街道地址列的角色：\n\names_rec %&gt;% update_role(address, new_role = \"street address\")\n#&gt; Error in `update_role()`:\n#&gt; ! Can't select columns that don't exist.\n#&gt; ✖ Column `address` doesn't exist.\n\n在这一更改之后，数据框中的address列将不再是预测变量，而是根据处理流程，成为一个\"street address\"。任何字符串都可以用作角色。此外，列可以有多个角色（额外的角色通过add_role()添加），这样它们就可以在不止一个情境下被选中。\n当数据被重采样时，这会很有帮助。它有助于将与模型拟合无关的列保留在同一个数据框中（而不是放在外部向量中）。第10章中描述的重采样主要通过行子采样来创建数据的替代版本。如果街道地址在另一个列中，就需要进行额外的子采样，这可能会导致代码更复杂，出错的可能性也更高。\n最后，所有步骤函数都有一个role字段，该字段可以为步骤的结果分配角色。在很多情况下，受步骤影响的列会保留其现有的角色。例如，对我们的ames_rec对象调用step_log()会影响Gr_Liv_Area列。对于该步骤，默认行为是保留该列现有的角色，因为没有创建新列。作为反例，生成样条曲线的步骤会将新列的默认角色设置为\"predictor\"，因为这通常是样条曲线列在模型中的使用方式。大多数步骤都有合理的默认设置，但由于默认设置可能不同，务必查看文档页面以了解将分配哪些角色。",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/8 Feature Engineering with recipes.html#chapter-summary",
    "title": "8 Feature Engineering with recipes",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n在本章中，你学习了如何使用recipe进行灵活的特征工程和数据预处理，包括创建虚拟变量、处理类别不平衡等。特征工程是建模过程中的重要组成部分，在此过程中很容易发生信息泄露，因此必须采用良好的实践方法。在recipes包以及其他扩展recipes的包中，有超过100个可用的步骤。所有可能的recipe步骤都列在tidymodels.org/find上。recipes框架为建模前的数据预处理和转换提供了丰富的数据操作环境，此外，tidymodels.org/learn/develop/recipes/展示了如何创建自定义步骤。\n我们在这里的工作仅在工作流对象内部使用了recipe。对于建模而言，这是推荐的用法，因为特征工程应该与模型一起进行评估。然而，对于可视化和其他活动，工作流可能并不合适，可能需要更多特定于recipe的函数。第16章讨论了用于拟合、使用recipe以及解决recipe问题的低级API。\n我们将在后续章节中使用的代码如下：\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)",
    "crumbs": [
      "8 Feature Engineering with recipes"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html",
    "title": "9 Judging Model Effectiveness",
    "section": "",
    "text": "一旦我们构建好了模型，我们需要知道它的效果如何。定量地评估模型有效性能让我们理解模型、比较不同的模型，或者调整模型以提升性能。在tidymodels中，我们重点关注经验验证方法；这通常意味着以未被用于创建模型的数据为基础来衡量有效性。该数据可以是测试集或者重抽样数据（见第10章），在本章中，我们将通过使用测试集来说明经验验证的必要性。请记住，正如第5.1节中所解释的，测试集只能使用一次。\n在判断模型效果时，指标的选择可能至关重要。在后面的章节中，某些模型参数将通过经验验证进行优化，并且会使用一个主要的性能指标来选择最佳的子模型。选择错误的指标很容易导致意想不到的后果。例如，回归模型的两个常见指标是均方根误差（RMSE）和决定系数（\\(R^2\\)），前者衡量的是准确性，而后者衡量的是相关性，这两者并不一定是一回事。Figure 1 展示了两者之间的区别。\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.1.0     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\nFigure 1: Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination\n针对均方根误差（RMSE）优化的模型变异性更大，但在结果的整个范围内具有相对一致的准确性。针对决定系数（\\(R^2\\)）优化的模型，其观测值和预测值之间的相关性更强，但在尾部表现不佳。\n本章将介绍yardstick包（tidymodels核心包之一），主要用于衡量模型性能。在介绍如何使用之前，让我们探讨一下，当模型侧重于推断而非预测时，使用性能指标进行实证验证是否有价值。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#performance-metrics-and-inference",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#performance-metrics-and-inference",
    "title": "9 Judging Model Effectiveness",
    "section": "Performance Metrics and Inference",
    "text": "Performance Metrics and Inference\n任何模型的有效性取决于该模型的使用方式。推断模型主要用于理解关系，通常会着重强调概率分布的选择（及有效性）以及其他定义该模型的生成特性。相比之下，对于主要用于预测的模型，预测能力至关重要，而对潜在统计特性的其他考量可能没那么重要。预测能力通常由我们的预测与观测数据的接近程度决定，即模型预测与实际结果的一致性。本章将聚焦于可用于衡量预测能力的函数。不过，我们对那些开发推断模型的人的建议是，即便模型的主要目标不是预测，也应使用这些技术。\n推断统计学实践中一个长期存在的问题是，由于纯粹聚焦于推断，没有评估模型的可信度。例如，Craig–Schapiro等人（2011）对333名阿尔茨海默病患者的数据进行分析，以确定影响认知障碍的因素。该分析利用已知的风险因素如年龄、性别和载脂蛋白E基因型（分类变量，该基因有三个主要变体的六种可能组合）等，构建了一个逻辑回归模型，其中结果是二元的（有障碍/无障碍）。已知载脂蛋白E与阿尔茨海默病存在关联（Jungsu、Basak和Holtzman，2009）。\n一种肤浅但并不少见的分析方法是拟合一个包含主效应和交互作用的大型模型，然后使用统计检验来找到在某个预先定义的水平上具有统计显著性的最小模型项集。如果使用包含三个因素及其双向和三向交互作用的完整模型，那么初始阶段将使用序贯似然比检验（sequential likelihood ratio tests）来检验交互作用（Hosmer and Lemeshow，2000）。让我们针对阿尔茨海默病的数据示例逐步介绍这种方法：\n\n当将包含所有双向交互项的模型与额外包含三向交互项的模型进行比较时，似然比检验得出的p值为0.888。这意味着，没有证据表明与三向交互项相关的四个额外模型项能够解释数据中足够多的变异，因此不应将它们保留在模型中。\n接下来，双向交互作用同样会与无交互作用的模型进行比较评估。此处的p值为0.0382。这一数值有些接近临界值，但考虑到样本量较小，谨慎的结论应该是：有证据表明，在10种可能的双向交互作用中，有一些对模型而言是重要的。\n从这里开始，我们将对结果进行一些解释。交互作用的讨论尤为重要，因为它们可能会引发有趣的生理学或神经学假设，供进一步探索。\n\n尽管这种分析策略比较浅显，但在实际应用和文献中都很常见。如果从业者在数据分析方面接受的正规培训有限，情况尤其如此。\n这种方法中缺失的一项信息是该模型与实际数据的拟合程度。利用第10章讨论的重采样方法，我们可以估计该模型的准确率约为73.4%。准确率通常不是衡量模型性能的理想指标，我们在这里使用它是因为它易于理解。如果该模型与数据的契合度为73.4%，我们应该相信它得出的结论吗？或许我们会这么认为，直到我们意识到数据中无障碍患者的基准率为72.7%（无障碍患者的比例）。这意味着，尽管我们进行了统计分析，但这个双因素模型似乎只比一种简单的启发式方法（无论观察到的数据如何，总是预测患者为无障碍）好0.8%。也就是说，对模型统计特征的优化并不意味着该模型能很好地拟合数据。即使对于纯粹的推断模型，推断结果也应附带某种程度的数据保真度衡量指标。借助这一点，分析结果的使用者可以校准他们对结果的预期。\n在本章的剩余部分，我们将讨论通过经验验证来评估模型的一般方法。这些方法按结果数据的性质分组：纯数值型、二元类别型以及三个或更多类别水平型。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#regression-metrics",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#regression-metrics",
    "title": "9 Judging Model Effectiveness",
    "section": "Regression Metrics",
    "text": "Regression Metrics\n回想第6.3节，tidymodels的预测函数会生成包含预测值列的tibble（一种数据框）。这些列有着统一的名称，而yardstick包中用于生成性能指标的函数也具有统一的接口。这些函数基于数据框，而非基于向量，其通用语法如为：function(data, truth, ...)，其中，data是一个数据框或tibble，truth是包含观测结果值的列。省略号或其他参数用于指定包含预测值的列。\n为了说明这一点，让我们以第8.8节中的模型为例。这个名为lm_fit的模型结合了线性回归模型与一个预测因子集，该预测因子集补充了经度和纬度的交互项及样条函数。它是根据一个训练集（名为ames_train）创建的。尽管我们不建议在建模过程的这个阶段使用测试集，但在这里会用它来演示功能和语法。数据框ames_test包含588处房产的数据。首先，让我们生成预测结果：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ rsample      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ tailor       0.1.0\n#&gt; ✔ infer        1.0.9     ✔ workflows    1.3.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflowsets 1.1.1\n#&gt; ✔ parsnip      1.3.3     ✔ yardstick    1.3.2\n#&gt; ✔ recipes      1.3.1\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ scales::discard() masks purrr::discard()\n#&gt; ✖ dplyr::filter()   masks stats::filter()\n#&gt; ✖ recipes::fixed()  masks stringr::fixed()\n#&gt; ✖ dplyr::lag()      masks stats::lag()\n#&gt; ✖ yardstick::spec() masks readr::spec()\n#&gt; ✖ recipes::step()   masks stats::step()\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\names_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.07\n#&gt; 2  5.31\n#&gt; 3  5.28\n#&gt; 4  5.33\n#&gt; 5  5.30\n#&gt; 6  5.24\n#&gt; # ℹ 582 more rows\n\n回归模型的预测数值结果被命名为.pred。让我们将预测值与其对应的观测结果值进行匹配：\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;   .pred Sale_Price\n#&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.07       5.02\n#&gt; 2  5.31       5.39\n#&gt; 3  5.28       5.28\n#&gt; 4  5.33       5.28\n#&gt; 5  5.30       5.28\n#&gt; 6  5.24       5.26\n#&gt; # ℹ 582 more rows\n\n我们可以看到这些数值大多看起来比较接近，但我们尚未对模型的表现有一个定量的了解，因为我们还没有计算任何性能指标。需要注意的是，预测结果和观测结果的单位都是以10为底的对数。即使预测结果是以原始单位报告的，最好还是在转换后的尺度上（如果使用了转换的话）分析这些预测结果。\n在计算指标之前，让我们绘制上面的结果如 Figure 2 中所示：\n\nggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) +\n  # Create a diagonal line:\n  geom_abline(lty = 2) +\n  geom_point(alpha = 0.5) +\n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\n\n\nFigure 2: Observed versus predicted values for an Ames regression model, with log-10 units on both axes\n\n\n\n\n有一处低价房产的预测值严重偏高，也就是说，它的位置远在那条虚线上方。\n让我们使用rmse()函数计算该模型的均方根误差：\n\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0736\n\n这向我们展示了yardstick函数输出的标准格式。对于数值型结果的指标，.estimator列的值通常为“standard”。下一节将展示该列具有不同值的示例。\n为了同时计算多个指标，我们可以创建一个指标集。让我们添加\\(R^2\\)和平均绝对误差：\n\names_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0736\n#&gt; 2 rsq     standard      0.836 \n#&gt; 3 mae     standard      0.0549\n\n这种整洁的数据格式将指标垂直堆叠。均方根误差和平均绝对误差这两个指标都与结果处于同一量级（在我们的示例中是log10(Sale_Price)），它们用于衡量预测值与观测值之间的差异。\\(R^2\\)的值衡量的是预测值与观测值之间的平方相关性，因此该值越接近1越好。\nyardstick包中没有用于计算调整后\\(R^2\\)的函数。这种决定系数的修正形式通常用于使用拟合模型的数据（训练集）来评估模型的情况。tidymodels中没有完全支持这一指标，因为使用与拟合模型不同的独立数据集（测试集）来计算性能总是更好的方法。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#binary-classification-metrics",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#binary-classification-metrics",
    "title": "9 Judging Model Effectiveness",
    "section": "Binary Classification Metrics",
    "text": "Binary Classification Metrics\n为了说明衡量模型性能的其他方法，我们将换一个不同的例子。modeldata包（tidymodels中的另一个包）包含来自一个测试数据集的示例预测结果，该数据集有两个类别（“Class1”和“Class2”）：\n\ndata(two_class_example)\ntibble(two_class_example)\n#&gt; # A tibble: 500 × 4\n#&gt;   truth   Class1   Class2 predicted\n#&gt;   &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n#&gt; 1 Class2 0.00359 0.996    Class2   \n#&gt; 2 Class1 0.679   0.321    Class1   \n#&gt; 3 Class2 0.111   0.889    Class2   \n#&gt; 4 Class1 0.735   0.265    Class1   \n#&gt; 5 Class2 0.0162  0.984    Class2   \n#&gt; 6 Class1 0.999   0.000725 Class1   \n#&gt; # ℹ 494 more rows\n\n第二列和第三列是测试集的预测类别概率，而predicted是离散预测值。\n对于硬类别预测，多种yardstick函数很有帮助：\n\n# A confusion matrix:\nconf_mat(two_class_example, truth = truth, estimate = predicted)\n#&gt;           Truth\n#&gt; Prediction Class1 Class2\n#&gt;     Class1    227     50\n#&gt;     Class2     31    192\n\n# Accuracy:\naccuracy(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n\n# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     binary         0.677\n\n# F1 metric:\nf_meas(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.849\n\n# Combining these three classification metrics together\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n#&gt; 2 mcc      binary         0.677\n#&gt; 3 f_meas   binary         0.849\n\n马修斯相关系数（Matthews correlation coefficient，mcc()）和F1分数（F1 metric，f_meas()）都能对混淆矩阵进行总结，但与衡量正负例质量的mcc()相比，f_meas()指标更侧重于正类，即我们所关注的事件。对于像本示例这样的二分类数据集，yardstick函数有一个名为event_level的标准参数，用于区分正负水平。默认情况下（我们在本代码中使用的就是默认设置），结果因子的第一个水平是我们关注的事件。在这一点上，R函数存在一些不一致性：有些函数使用第一层级，而另一些则使用第二层级来表示关注的事件。我们认为第一层级最为重要，这更符合直觉。将结果编码为0/1（在这种情况下，第二个值代表事件）催生了第二层级的逻辑，遗憾的是，这种逻辑在一些包中仍然存在。然而，tidymodels（以及许多其他R包）要求将分类结果编码为因子，因此，将第二层级作为事件的传统理由就变得无关紧要了。\n举一个第二层级为事件的例子：\n\nf_meas(two_class_example, truth, predicted, event_level = \"second\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.826\n\n在这个输出中，.estimator的值“binary”表明将使用二分类的标准公式。\n有许多分类指标使用预测概率作为输入，而非硬性类别预测。例如，受试者工作特征（ROC）曲线会在一系列不同的事件阈值上计算敏感性和特异性，且不使用预测类别列。这种方法有两个yardstick函数：roc_curve()用于计算构成ROC曲线的数据点，roc_auc()用于计算曲线下面积。\n这些类型的度量函数的接口使用...参数占位符来传入适当的类别概率列。对于二分类问题，感兴趣事件的概率列会被传入函数中：\n\ntwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n#&gt; # A tibble: 502 × 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 -Inf           0                 1\n#&gt; 2    1.79e-7     0                 1\n#&gt; 3    4.50e-6     0.00413           1\n#&gt; 4    5.81e-6     0.00826           1\n#&gt; 5    5.92e-6     0.0124            1\n#&gt; 6    1.22e-5     0.0165            1\n#&gt; # ℹ 496 more rows\n\nroc_auc(two_class_example, truth, Class1)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.939\n\ntwo_class_curve对象可用于ggplot调用中以可视化该曲线，如 Figure 3 所示。存在一个autoplot()方法，可处理相关细节：\n\nautoplot(two_class_curve)\n\n\n\n\n\n\nFigure 3: Example ROC curve\n\n\n\n\n如果曲线接近对角线，那么该模型的预测不会比随机猜测好。由于曲线位于左上角，我们可以看出我们的模型在不同阈值下都表现良好。\n还有许多其他使用概率估计的函数，包括gain_curve()、lift_curve()和pr_curve()。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#multiclass-classification-metrics",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#multiclass-classification-metrics",
    "title": "9 Judging Model Effectiveness",
    "section": "Multiclass Classification Metrics",
    "text": "Multiclass Classification Metrics\n那么具有三个或更多类别的数据该如何处理呢？为了说明这一点，让我们来探讨一个不同的示例数据集，该数据集有四个类别：\n\ndata(hpc_cv)\ntibble(hpc_cv)\n#&gt; # A tibble: 3,467 × 7\n#&gt;   obs   pred     VF      F       M          L Resample\n#&gt;   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#&gt; 2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#&gt; 3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#&gt; 4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#&gt; 5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#&gt; 6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#&gt; # ℹ 3,461 more rows\n\n和之前一样，存在观测结果列和预测结果列，以及每个类别的预测概率列。（这些数据还包括一个Resample列。这些hpc_cv结果是与10折交叉验证相关的样本外预测。目前，这一列将被忽略，我们会在第10章深入讨论重抽样。）\n使用离散类别预测的指标函数与其二元对应函数相同：\n\naccuracy(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     multiclass     0.515\n\n请注意，在这些结果中，列出了一个“multiclass”.estimator。与“binary”类似，这表明使用了适用于具有三个或更多类别水平的结果的公式。马修斯相关系数最初是为两类情况设计的，但已被扩展到具有更多类别水平的情况。\n有一些方法可以将旨在处理仅有两个类别的结果的指标扩展到用于有两个以上类别的结果。例如，像灵敏度这样用于衡量真阳性率的指标，根据定义，它是特定于两个类别的（即“事件”和“非事件”）。在我们的示例数据中，如何使用这个指标呢？\n有一些包装方法可用于将敏感性应用于我们的四分类结果。这些方法包括宏平均、加权宏平均和微平均：\n\n宏平均法使用标准的二分类统计数据计算一组一对一的指标，然后对这些指标进行平均。\n宏加权平均法的做法相同，但平均值会按每个类别的样本数量进行加权。\n微平均会计算每个类别的贡献，对这些贡献进行汇总，然后从汇总结果中计算出一个单一的指标。\n\n有关将分类指标扩展到具有两个以上类别的结果的更多信息，请参见Wu和Zhou（2017）以及Opitz和Burst（2019）。\n以灵敏度为例，常见的二分类计算方法是正确预测的事件数量除以实际事件的数量。这些平均方法的手动计算如下：\n\nclass_totals &lt;-\n  count(hpc_cv, obs, name = \"totals\") %&gt;%\n  mutate(class_wts = totals / sum(totals))\nclass_totals\n#&gt;   obs totals  class_wts\n#&gt; 1  VF   1769 0.51023940\n#&gt; 2   F   1078 0.31093164\n#&gt; 3   M    412 0.11883473\n#&gt; 4   L    208 0.05999423\n\ncell_counts &lt;-\n  hpc_cv %&gt;%\n  group_by(obs, pred) %&gt;%\n  count() %&gt;%\n  ungroup()\n\n# Compute the four sensitivities using 1-vs-all\none_versus_all &lt;-\n  cell_counts %&gt;%\n  filter(obs == pred) %&gt;%\n  full_join(class_totals, by = \"obs\") %&gt;%\n  mutate(sens = n / totals)\none_versus_all\n#&gt; # A tibble: 4 × 6\n#&gt;   obs   pred      n totals class_wts  sens\n#&gt;   &lt;fct&gt; &lt;fct&gt; &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 VF    VF     1620   1769    0.510  0.916\n#&gt; 2 F     F       647   1078    0.311  0.600\n#&gt; 3 M     M        79    412    0.119  0.192\n#&gt; 4 L     L       111    208    0.0600 0.534\n\n# Three different estimates:\none_versus_all %&gt;%\n  summarize(\n    macro = mean(sens),\n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n#&gt; # A tibble: 1 × 3\n#&gt;   macro macro_wts micro\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.560     0.709 0.709\n\n值得庆幸的是，无需手动实现这些平均方法。相反，yardstick函数可以通过estimator参数自动应用这些方法：\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator     .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity micro          0.709\n\n在处理概率估计时，存在一些具有多类别类似物的指标。例如，Hand and Till（2001）确定了一种用于ROC曲线的多类别技术。在这种情况下，必须将所有类别概率列提供给该函数：\n\nroc_auc(hpc_cv, obs, VF, F, M, L)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.829\n\n宏加权平均也可作为将此指标应用于多类别结果的一个选项：\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator     .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 roc_auc macro_weighted     0.868\n\n最后，所有这些性能指标都可以使用dplyr分组来计算。回想一下，这些数据有一个用于重抽样分组的列。我们尚未详细讨论重抽样，但请注意，我们可以将分组数据框传递给metric函数，以计算每个组的指标：\n\nhpc_cv %&gt;%\n  group_by(Resample) %&gt;%\n  accuracy(obs, pred)\n#&gt; # A tibble: 10 × 4\n#&gt;   Resample .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Fold01   accuracy multiclass     0.726\n#&gt; 2 Fold02   accuracy multiclass     0.712\n#&gt; 3 Fold03   accuracy multiclass     0.758\n#&gt; 4 Fold04   accuracy multiclass     0.712\n#&gt; 5 Fold05   accuracy multiclass     0.712\n#&gt; 6 Fold06   accuracy multiclass     0.697\n#&gt; # ℹ 4 more rows\n\n这些分组也适用于autoplot()方法，结果如 Figure 4 所示。\n\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %&gt;%\n  group_by(Resample) %&gt;%\n  roc_curve(obs, VF, F, M, L) %&gt;%\n  autoplot()\n\n\n\n\n\n\nFigure 4: Resampled ROC curves for each of the four outcome classes\n\n\n\n\n这种可视化向我们展示，不同组的表现大致相同，但VF类别的预测效果优于F类或M类别，因为VF的ROC曲线更靠近左上角。此示例使用重采样作为分组，但可以使用数据中的任何分组方式。这种autoplot()方法可以作为一种快速的可视化方法，用于展示模型在不同结果类别和/或组间的有效性。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/9 Judging Model Effectiveness.html#chapter-summary",
    "title": "9 Judging Model Effectiveness",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n不同的指标衡量模型拟合的不同方面，例如，RMSE衡量准确性，而\\(R^2\\)衡量相关性。即使某个给定模型不会主要用于预测，衡量模型性能也很重要；预测能力对于推断性模型或描述性模型而言同样重要。yardstick包中的函数利用数据来衡量模型的有效性。主要的tidymodels接口采用tidyverse原则和数据框（而非向量参数）。不同的指标适用于回归和分类指标，并且在这些指标中，有时存在不同的统计量估计方法，例如针对多类别结果的情况。",
    "crumbs": [
      "9 Judging Model Effectiveness"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html",
    "title": "10 Resampling for Evaluating Performance",
    "section": "",
    "text": "我们已经介绍了评估模型性能时必须综合考虑的几个方面。第9章描述了用于衡量模型性能的统计量。第5章引入了数据使用的概念，并且我们建议使用测试集来获得无偏的性能估计。然而，我们通常需要在使用测试集之前了解一个甚至多个模型的性能（测试集只能使用一次?）。通常情况下，在第一次评估模型性能之前，我们无法决定使用哪个最终模型来处理测试集。我们对可靠地衡量性能的需求与我们可用的数据拆分（训练集和测试集）之间存在差距。\n在本章中，我们将介绍一种名为重采样的方法，它能够填补这一空白。重采样得出的性能估计值可以像测试集得出的估计值一样，推广到新数据。下一章将通过展示用于比较重采样结果的统计方法，对本章内容进行补充。\n为了充分理解重抽样的价值，让我们首先来看一下重代入法（这种方法常常会失败）。",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#the-resubstitution-approach",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#the-resubstitution-approach",
    "title": "10 Resampling for Evaluating Performance",
    "section": "The Resubstitution Approach",
    "text": "The Resubstitution Approach\n所谓重带入，就是使用用于训练的相同数据（训练集而非测试集或新数据）来衡量性能。让我们再次使用Ames房价数据来演示这些概念。\n第8.8节总结了Ames数据分析的当前状态：包括一个名为ames_rec的recipe对象、一个线性模型，以及一个使用该recipe对象和模型的工作流，名为lm_wflow，这个工作流在训练集上进行了拟合，得到了lm_fit。\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\n为了与这个线性模型进行比较，我们可以拟合另一种类型的模型——随机森林。随机森林是一种树集成方法，其运作方式是从训练集的略有不同的版本（下采样获得）中训练大量决策树（Breiman，2001a），组合这些树构成集成模型。在预测新样本时，每个决策树都会做出独立的预测，这些预测会被平均，以形成新数据点的最终集成预测。随机森林模型非常强大，它们能够非常精准地模拟潜在的数据模式。虽然这种模型在计算上可能较为密集，但维护成本极低；几乎不需要进行预处理（如附录A中所记载）。\n使用与线性模型相同的预测变量集（不包含额外的预处理步骤），我们可以通过”ranger”引擎（来自 ranger R包）将随机森林模型拟合到训练集。该模型无需预处理，因此可以使用一个简单的公式：\n\nrf_model &lt;-\n  rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wflow &lt;-\n  workflow() %&gt;%\n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n      Latitude + Longitude\n  ) %&gt;%\n  add_model(rf_model)\n\nrf_fit &lt;- rf_wflow %&gt;% fit(data = ames_train)\n\n我们应该如何比较线性模型和随机森林模型呢？为了演示，我们将对训练集进行预测，以生成所谓的表观度量或重代入度量。以下函数用于生成预测并格式化结果：\n\nestimate_perf &lt;- function(model, dat) {\n  # Capture the names of the `model` and `dat` objects\n  cl &lt;- match.call()\n  obj_name &lt;- as.character(cl$model)\n  data_name &lt;- as.character(cl$dat)\n  data_name &lt;- gsub(\"ames_\", \"\", data_name)\n\n  # Estimate these metrics:\n  reg_metrics &lt;- metric_set(rmse, rsq)\n\n  model %&gt;%\n    predict(dat) %&gt;%\n    bind_cols(dat %&gt;% select(Sale_Price)) %&gt;%\n    reg_metrics(Sale_Price, .pred) %&gt;%\n    select(-.estimator) %&gt;%\n    mutate(object = obj_name, data = data_name)\n}\n\n重代入统计量计算均方根误差（RMSE）和决定系数（\\(R_2\\)）如下：\n\nestimate_perf(rf_fit, ames_train)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0364 rf_fit train\n#&gt; 2 rsq        0.960  rf_fit train\nestimate_perf(lm_fit, ames_train)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0754 lm_fit train\n#&gt; 2 rsq        0.816  lm_fit train\n\n基于重代入的结果，随机森林在预测销售价格方面的能力要强得多；其均方根误差（RMSE）估计值比线性回归好两倍。如果我们需要在这两个模型中为价格预测问题做出选择，我们可能会选择随机森林。但如果将随机森林模型应用到测试集进行最终验证时：\n\nestimate_perf(rf_fit, ames_test)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0701 rf_fit test \n#&gt; 2 rsq        0.853  rf_fit test\n\n测试集的均方根误差（RMSE）估计值比训练集的差很多！这是为什么呢？\n在统计学中，有一类低偏差模型，它们能够从数据中学习复杂的趋势。所谓偏差是指数据中真实的模式（或关系）与模型能够模拟的模式（或关系）之间的差异。许多黑箱机器学习模型具有低偏差特征，它们能够再现复杂的关系；其他模型（如线性/逻辑回归、判别分析等）的适应性较差，被认为是高偏差模型。对于低偏差模型而言，其高度的预测能力有时会导致模型几乎记住训练集数据（过拟合）。举一个明显的例子，k=1的最近邻模型，无论在其他数据集上实际表现如何，它总能对训练集做出完美的预测；随机森林模型也类似。对训练集重新预测总会得出对性能的人为乐观估计。\n对于这两个模型， Table 1 总结了训练集和测试集的均方根误差（RMSE）估计值。注意，由于线性回归模型的复杂度有限，它在训练集和测试集上的表现是一致的。\n\n\nTable 1: Performance statistics for training and test sets.\n\n\n\n\n\n\n\n\n\n\n\nRMSE Estimates\n\n\n\nobject\ntrain\ntest\n\n\n\n\nlm_fit\n0.0754450\n0.0736297\n\n\nrf_fit\n0.0364254\n0.0700734\n\n\n\n\n\n\n以上的例子说明：重新预测训练集会导致对模型性能的估计过于乐观，这在大多数模型中都不是一个好主意。如果不能立即使用测试集，且重新预测训练集又不是个好主意，那该怎么办呢？重采样方法，如交叉验证或验证集，就是解决办法。",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#resampling-methods",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#resampling-methods",
    "title": "10 Resampling for Evaluating Performance",
    "section": "Resampling Methods",
    "text": "Resampling Methods\n重采样方法是一种经验模拟系统，它模拟使用部分数据进行建模、使用不同数据进行评估的过程。大多数重采样方法都是迭代式的，这意味着该过程会重复多次。 Figure 1 展示了重采样方法的大致运作方式。\n\n\n\n\n\nFigure 1: Data splitting scheme from the initial data split to resampling\n\n\n正如你在 Figure 1 中所看到的，重抽样仅在训练集上进行，测试集不参与其中。在重抽样的每一次迭代中，数据会被划分为两个子样本：\n\n分析集（analysis set）：用来训练模型\n评估集（assessment set）：用来评估模型\n\n这两个子样本在某种程度上类似于训练集和测试集。为了避免混淆，我们使用新的术语——分析集和评估集，两者是互斥的。假设进行20次重抽样迭代，这意味着在分析集上拟合20个独立的模型，而相应的评估集则会产生20组性能统计数据。一个模型的最终性能估计值是这20个统计数据的平均值。这个平均值具有非常好的泛化特性，远优于重新代入估计值。不同的重采样方法会有不同的创建分析集和评估集的方法。下一节将定义几种常用的重采样方法，并讨论它们的优缺点。\nCross-validation\n交叉验证（Cross-validation）是一种成熟的重抽样方法。虽然它有多种变体，但最常见的交叉验证方法是 V 折交叉验证——数据被随机划分为 V 个大小大致相等的集合（称为折）。 Figure 2 展示了 V = 3 的情况：对包含30个训练集样本点的数据，进行折随机分配。图中的数字是样本编号，颜色代表它们被随机分配的折数。分层抽样也是一种分配折数的方法（在5.1节中已讨论过）。\n\n\n\n\n\nFigure 2: V-fold cross-validation randomly assigns data to folds\n\n\n对于三折交叉验证，重采样的三次迭代如 Figure 3 所示。在每次迭代中，留出一折用作评估集，其余折用作分析集，三折迭代三次，产生三个模型和三组性能统计数据。即，当 V = n 时，分析集占训练集的 (n-1)/n，每个评估集都是不同的 1/n ，最终的重抽样性能估计值是 V 次重复的平均值。\n\n\n\n\n\nFigure 3: V-fold cross-validation data usage\n\n\n此处使用 V = 3 仅为说明交叉验证是一个不错的选择，在实践中，最常用的 V 值是5或10。因为较大的 V 值会导致重采样估计的偏差较小，但方差较大；较小的 V 值则相反——偏差较大，但方差较小。我们通常倾向于将10折交叉验证作为默认选择，因为在大多数情况下，它的规模足以产生良好的结果。\n主要输入是训练集和折数 V（默认值为10），生成的对象包含两列：\n\n\nsplits列，包含关于如何分割数据的信息（类似于用于创建初始训练/测试分区的对象）。\n\nid列，包含折的标识符。\n\n虽然splits的每一行都嵌入了整个训练集的副本，但不会在内存中复制数据。打印tibble格式的数据框会显示每一项的频数：[2107/235]表示分析集大约有2000个样本，评估集有235个样本。\n\nset.seed(1001)\names_folds &lt;- rsample::vfold_cv(ames_train, v = 10)\names_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 × 2\n#&gt;   splits             id    \n#&gt;   &lt;list&gt;             &lt;chr&gt; \n#&gt; 1 &lt;split [2107/235]&gt; Fold01\n#&gt; 2 &lt;split [2107/235]&gt; Fold02\n#&gt; 3 &lt;split [2108/234]&gt; Fold03\n#&gt; 4 &lt;split [2108/234]&gt; Fold04\n#&gt; 5 &lt;split [2108/234]&gt; Fold05\n#&gt; 6 &lt;split [2108/234]&gt; Fold06\n#&gt; # ℹ 4 more rows\n\n需要手动检索分区数据时，可以使用analysis()和assessment()函数，会返回相应的数据框：\n\n# For the first fold:\names_folds$splits[[1]] %&gt;%\n  analysis() %&gt;%\n  dim()\n#&gt; [1] 2107   74\n\ntidymodels系列包（例如tune包）包含更高级的用户API，因此像analysis()这样的函数通常不需要用于日常工作。10.3节演示了一个在这些重抽样上拟合模型的函数。\n交叉验证有多种变体，我们将介绍其中最重要的几种。\nRepeated cross-validation\n交叉验证最重要的变体是重复 V 折交叉验证。根据数据规模或其他特征，V折交叉验证产生的重采样估计可能会有过大的噪声。与许多统计问题一样，减少噪声的一种方法是收集更多数据。对于交叉验证来说，这意味着要获取超过 V 个统计量，然后取平均值。为此，我们只需要重复执行 R 次 V 折交叉验证，就可以获取 V × R 个统计量来得出最终的重抽样估计值。根据中心极限定理，只要我们拥有相对于 V × R 而言足够多的数据，每个模型的汇总统计量就会趋向于正态分布。\n考虑Ames数据集，如果选择均方根误差（RMSE）作为统计量，我们可以将该估计值的标准差记为 \\(\\sigma\\) 。对于简单的10折交叉验证，平均RMSE的标准误为 \\(\\sigma/\\sqrt{10}\\) 。如果这一结果噪声过大，重复交叉验证可以将标准误降至 \\(\\sigma/\\sqrt{10R}\\) 。对于具有 R 次重复的10折交叉验证， Figure 4 展示了标准误随重复次数增加而快速降低的情况。\n\n\n\n\n\n\n\nFigure 4: Relationship between the relative variance in performance estimates versus the number of cross-validation repeats\n\n\n\n\n更多的重复次数对标准误差的影响往往较小。然而，如果\\(\\sigma\\)的基线值大得不切实际，那么重复次数增加所带来的边际效益递减可能仍然值得额外的计算成本。\n创建重复项，可以使用vfold_v()中的参数repeats：\n\nvfold_cv(ames_train, v = 10, repeats = 5)\n#&gt; #  10-fold cross-validation repeated 5 times \n#&gt; # A tibble: 50 × 3\n#&gt;   splits             id      id2   \n#&gt;   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n#&gt; 1 &lt;split [2107/235]&gt; Repeat1 Fold01\n#&gt; 2 &lt;split [2107/235]&gt; Repeat1 Fold02\n#&gt; 3 &lt;split [2108/234]&gt; Repeat1 Fold03\n#&gt; 4 &lt;split [2108/234]&gt; Repeat1 Fold04\n#&gt; 5 &lt;split [2108/234]&gt; Repeat1 Fold05\n#&gt; 6 &lt;split [2108/234]&gt; Repeat1 Fold06\n#&gt; # ℹ 44 more rows\n\nLeave-one-out cross-validation\n交叉验证的一种变体是留一法（leave-one-out，LOO）交叉验证。如果有n个训练集样本，就会使用训练集中的n-1行数据拟合n个模型。每个模型都会对那个被排除的单一数据点进行预测。在重采样结束时，这n个预测结果会被汇总，以生成一个单一的性能统计量。留一法与几乎所有其他方法相比都存在不足，除了样本量极小的情况外，留一法的计算量过大，而且可能不具备良好的统计特性。尽管rsample包中包含一个loo_cv()函数，但这些对象通常并未整合到更广泛的tidymodels框架中。\nMonte Carlo cross-validation\n另一种V折交叉验证的变体是蒙特卡洛交叉验证（Monte Carlo cross-validation，MCCV，Xu和Liang（2001））。与V折交叉验证类似，它将固定比例的数据分配给评估集。不同之处在于，每次分配都是随机选择，导致评估集间并非相互排斥。使用mc_cv()函数创建该重采样对象：\n\nmc_cv(ames_train, prop = 9 / 10, times = 20)\n#&gt; # Monte Carlo cross-validation (0.9/0.1) with 20 resamples \n#&gt; # A tibble: 20 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [2107/235]&gt; Resample01\n#&gt; 2 &lt;split [2107/235]&gt; Resample02\n#&gt; 3 &lt;split [2107/235]&gt; Resample03\n#&gt; 4 &lt;split [2107/235]&gt; Resample04\n#&gt; 5 &lt;split [2107/235]&gt; Resample05\n#&gt; 6 &lt;split [2107/235]&gt; Resample06\n#&gt; # ℹ 14 more rows\n\nValidation sets\n在第5.2节中，我们简要讨论过，验证集是一个单独划分出来的数据集，用于独立于测试集评估模型性能，如 Figure 5 。\n\n\n\n\n\nFigure 5: A three-way initial split into training, testing, and validation sets\n\n\n当原始数据量非常大时，通常会使用验证集，因为在这种情况下，一个大型的单一分区可能足以描述模型性能，而无需进行多次重采样迭代。借助rsample包，验证集可以像其他任何重抽样对象一样被调用；不同之处仅在于它只有一次迭代。Figure 6 展示了这种方案。\n\n\n\n\n\nFigure 6: A two-way initial split into training and testing with an additional validation set split on the training set\n\n\n使用validation_set()函数，可以将第5.2节代码中initial_validation_split()的结果转换为一个与vfold_cv()等函数生成的结果类似的对象：\n\n# Previously:\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;1758/586/586/2930&gt;\n\n# Object used for resampling:\nval_set &lt;- validation_set(ames_val_split)\nval_set\n#&gt; # A tibble: 1 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1758/586]&gt; validation\n\n正如你将在第10.3节中看到的，fit_resamples()函数将用于通过重采样计算准确的性能估计。val_set对象可以在该函数和其他函数中使用，尽管它只是数据的一次“重采样”。\nBootstrapping\n自助重采样最初是用来近似（难以理论推导）统计量的抽样分布（Davison 和 Hinkley，1997），将其用于估计模型性能是该方法的次要应用。\n训练集的自助重采样结果是一个与训练集大小相同但通过有放回抽样得到的样本。这意味着一些训练集数据点会被多次选入分析集，且每个数据点至少被选入分析集一次的概率为63.2%；评估集包含所有未被选入分析集的训练集样本（平均而言，占训练集的36.8%）。在自助重采样中，评估集通常被称为袋外样本。\n\n\n\n\n\n\nNote63.8%的由来\n\n\n\n假设有 n 个样本，则每个样本被抽到的概率是 \\(\\frac{1}{n}\\)，没有被抽到概率是 \\(1-\\frac{1}{n}\\)，重复抽取 n 次都没抽到某个样本的概率为 \\((1-\\frac{1}{n})^n\\)。推导极限：\n\\(lim_{n\\to\\infty} (1-\\frac{1}{n})^n = \\frac{1}{e} \\approx 0.3679\\)\n可以看到一次都没抽到的概率是36.8%，至少一次被抽中的概率是63.2%。\n\n\n对于一个包含30个样本的训练集，Figure 7 展示了三个自助抽样样本的示意图。请注意，评估集的大小各不相同。\n\n\n\n\n\nFigure 7: Bootstrapping data usage\n\n\n创建自助法重抽样可以使用rsample::bootstraps()函数：\n\nbootstraps(ames_train, times = 5)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [2342/867]&gt; Bootstrap1\n#&gt; 2 &lt;split [2342/869]&gt; Bootstrap2\n#&gt; 3 &lt;split [2342/859]&gt; Bootstrap3\n#&gt; 4 &lt;split [2342/858]&gt; Bootstrap4\n#&gt; 5 &lt;split [2342/873]&gt; Bootstrap5\n\n自助重抽样产生的性能估计值的方差非常小（与交叉验证不同），但存在显著的悲观偏差。这意味着，如果一个模型的真实准确率是90%，自助重抽样得出的估计值会低于90%。这种偏差的大小无法通过经验来精度确定，同时，偏差的大小会随着性能指标的范围而变化。例如，当准确率为90%时，其偏差很可能与准确率为70%时的偏差不同。\n自助法也被用于许多模型内部。例如，前面提到的随机森林模型包含1000棵独立的决策树，每棵树都是训练集不同自助样本的产物。\nRolling forecasting origin resampling\n当数据具有很强的时间成分时，重采样方法需要考虑到，模型要估计数据中的季节性和其他时间趋势。从训练集中随机抽样的技术可能会破坏模型估计这些模式的能力。\n滚动预测起点重采样（Rolling forecast origin resampling，Hyndman 和 Athanasopoulos，2018）是一种解决上述问题的方法，该方法模拟了时间序列数据在实际中通常的划分方式，即使用历史数据估计模型，并使用最新数据评估模型。对于这种类型的重采样，需要指定初始分析集和评估集的大小以及每次迭代的偏移量，然后程序从序列的头部开始按照指定大小生成分析集和评估集，然后按照偏移量向尾部移动。\n举例来说，对一个包含15个样本的训练集进行重采样，其中分析集为8个样本，评估集为3个样本，每次偏移1个样本。这种配置会产生5个重采样样本，如@fig-10.8 所示。\n\n\n\n\n\nFigure 8: Data usage for rolling forecasting origin resampling\n\n\n以下是该方法的两种不同配置：\n\n分析集可以累积增长（而非保持相同大小）。在第一个初始分析集之后，可以积累新样本，而不丢弃早期数据。\n偏移量不必为1。例如，对于大型数据集，偏移量可以是一周或一个月，而非一天。\n\n对于一年的数据，假设分析集大小为6组30天的数据，评估集大小为30天的数据，偏移量为29天。rolling_origin()函数的设置如下：\n\ntime_slices &lt;-\n  tibble(x = 1:365) %&gt;%\n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range &lt;- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~ analysis(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1   180\n#&gt; 2    31   210\n#&gt; 3    61   240\n#&gt; 4    91   270\n#&gt; 5   121   300\n#&gt; 6   151   330\nmap_dfr(time_slices$splits, ~ assessment(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1   181   210\n#&gt; 2   211   240\n#&gt; 3   241   270\n#&gt; 4   271   300\n#&gt; 5   301   330\n#&gt; 6   331   360",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#estimating-performance",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#estimating-performance",
    "title": "10 Resampling for Evaluating Performance",
    "section": "Estimating Performance",
    "text": "Estimating Performance\n上面的重采样的流程可以总结为：\n\n根据分析数据生成预处理，将预处理应用于分析集，并使用这些经过处理的数据来拟合模型。\n将预处理应用于评估集，生成评估集的预测，用以估计模型在新数据上的性能。\n重复1，2两步，最终取所有性能的平均值。\n\n其中第2步的执行需要使用tune::fit_resamples()。该函数类似于fit()，但它没有data参数，类似的参数是resamples，需要提供一个包含多个重采样样本的rset对象。有分别针对model_spec和workflow的接口：\nmodel_spec %&gt;% fit_resamples(formula,  resamples, ...)\nmodel_spec %&gt;% fit_resamples(recipe,   resamples, ...)\nworkflow   %&gt;% fit_resamples(          resamples, ...)\n其他可选参数：\n\nmetrics：yardstick::metric_set()定义的一组性能统计指标。默认情况下，回归模型使用RMSE和\\(R^2\\)，而分类模型计算ROC曲线下面积和总体准确率。请注意，此选择还定义了在模型评估过程中生成哪些预测。对于分类，如果只要求准确率，则不会为评估集生成类别概率估计值（因为不需要）。\ncontrol：由control_resamples()创建的包含各种选项的列表。\n\ncontrol_resamples()的控制参数包括：\n\nverbose：用于打印日志的逻辑值。\nevent_level：二分类时定义那一个类被视为“事件”或“阳性”。\nextract：用于从每个模型迭代中保留对象的函数（本章后续会讨论）。\nsave_pred：用于保存评估集预测结果的逻辑值。\nsave_workflow：用于保存工作流的逻辑值。\nallow_par： 用于启用并行执行的逻辑值。\npkgs：并行运行时需要加载的包列表。\nparallel_over：控制并行处理的执行方式，可选“resamples”或“everything”。\n\n让我们重新考虑一下包含在rf_wflow对象中的随机森林模型，设置保存这些预测结果，以便可视化模型拟合情况和残差：\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;-\n  rf_wflow %&gt;%\n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;   splits             id     .metrics         .notes           .predictions\n#&gt;   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt; 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;    \n#&gt; # ℹ 4 more rows\n\n返回值是一个与输入的重抽样数据类似的tibble，并附带一些额外的列：\n\n.metrics：保存评估集性能统计数据。\n.notes：记录重抽样过程中产生的任何警告或错误。请注意，错误不会终止后续的重抽样执行。\n.predictions：保存评估集预测结果，在save_pred = TRUE时存在。\n\n虽然这些列表列可能看起来令人望而生畏，但可以使用tidyr或tidymodels提供的便捷函数轻松重构它们。例如，collect_metrics()可以返回性能指标结果：\n\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 rmse    standard   0.0720    10 0.00306 pre0_mod0_post0\n#&gt; 2 rsq     standard   0.832     10 0.0107  pre0_mod0_post0\n\n这些是对各个重采样的性能评估结果取平均值后的计值。要获取每个重采样的性能评估结果，请使用选项summarize = FALSE。请注意，这些性能估计值比第10.1节中的重代入估计值要现实得多！\ncollect_predictions()可以获取评估集预测结果：\n\nassess_res &lt;- collect_predictions(rf_res)\nassess_res\n#&gt; # A tibble: 2,342 × 5\n#&gt;   .pred id     Sale_Price  .row .config        \n#&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          \n#&gt; 1  5.10 Fold01       5.09    10 pre0_mod0_post0\n#&gt; 2  4.92 Fold01       4.90    27 pre0_mod0_post0\n#&gt; 3  5.21 Fold01       5.08    47 pre0_mod0_post0\n#&gt; 4  5.13 Fold01       5.10    52 pre0_mod0_post0\n#&gt; 5  5.13 Fold01       5.10    59 pre0_mod0_post0\n#&gt; 6  5.13 Fold01       5.11    63 pre0_mod0_post0\n#&gt; # ℹ 2,336 more rows\n\n预测列名遵循第6章中讨论的parsnip模型约定，以确保一致性和易用性。原始结果列始终使用源数据中的原始列名。.row列是一个整数，与原始训练集的行相匹配，这样这些结果就可以正确排列并与原始数据连接起来。\n对于某些重抽样方法，例如自助法或重复交叉验证，原始训练集中的每一行会有多个预测结果。要获得汇总值（重复预测的平均值），请使用collect_predictions(object, summarize = TRUE)。由于本分析采用了10折交叉验证，因此每个训练集样本都有一个独特的预测值。这些数据可以生成有用的模型图表，以了解模型可能在哪些地方出现了问题。例如，Figure 9 对比了原始值和预测值（类似于图9.2）：\n\nassess_res %&gt;%\n  ggplot(aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") +\n  coord_obs_pred() +\n  ylab(\"Predicted\")\n\n\n\n\n\n\nFigure 9: Out-of-sample observed versus predicted values for an Ames regression model, using log-10 units on both axes\n\n\n\n\n训练集中有两栋房屋的实际售价较低，但模型对它们的预测价格却明显过高。这两栋房屋是哪两栋呢？让我们从assess_res的结果中找找答案：\n\nover_predicted &lt;-\n  assess_res %&gt;%\n  mutate(residual = Sale_Price - .pred) %&gt;%\n  arrange(desc(abs(residual))) %&gt;%\n  slice(1:2)\nover_predicted\n#&gt; # A tibble: 2 × 6\n#&gt;   .pred id     Sale_Price  .row .config         residual\n#&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1  4.96 Fold09       4.11    32 pre0_mod0_post0   -0.857\n#&gt; 2  4.93 Fold08       4.12   317 pre0_mod0_post0   -0.813\n\names_train %&gt;%\n  slice(over_predicted$.row) %&gt;%\n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n#&gt; # A tibble: 2 × 5\n#&gt;   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n#&gt;         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n#&gt; 1         832 Old_Town                     1923             2         1\n#&gt; 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\n识别出这类表现极其糟糕的样本，有助于我们跟进并调查这些特定预测为何会如此糟糕。\n让我们回到整体的房屋情况。我们如何使用验证集而不是交叉验证呢？从我们之前的rsample对象来看：\n\nval_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = val_set)\nval_res\n#&gt; # Resampling results\n#&gt; # Validation Set (0.75/0.25) \n#&gt; # A tibble: 1 × 4\n#&gt;   splits             id         .metrics         .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 &lt;split [1758/586]&gt; validation &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt;\n\ncollect_metrics(val_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 rmse    standard   0.0724     1      NA pre0_mod0_post0\n#&gt; 2 rsq     standard   0.824      1      NA pre0_mod0_post0\n\n这些结果也比性能的重代入估计值更接近测试集结果。\n在这些分析中，重抽样结果与测试集结果非常接近。这两种估计值往往具有良好的相关性。然而，这可能是随机因素导致的。在创建重抽样之前，55这个种子值固定了随机数。试着更改这个值并重新运行分析，以研究重抽样估计值是否也与测试集结果匹配。",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#parallel-processing",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#parallel-processing",
    "title": "10 Resampling for Evaluating Performance",
    "section": "Parallel Processing",
    "text": "Parallel Processing\n重抽样的每一次评估彼此独立，这类计算极易并行化，都可以毫无问题地同时运行。tune包使用foreach包来实施并行计算。根据所选技术的不同，这些计算可以分配到同一台计算机的不同处理器上，也可以分配到不同的计算机上。\n对于在单台计算机上进行的计算，可能的工作进程数量由parallel包决定：\n\n# The number of physical cores in the hardware:\nparallel::detectCores(logical = FALSE)\n#&gt; [1] 12\n\n# The number of possible independent processes that can\n# be simultaneously used:\nparallel::detectCores(logical = TRUE)\n#&gt; [1] 24\n\n这两个数值之间的差异与计算机的处理器有关。例如，大多数英特尔处理器采用超线程技术，即为每个物理核心创建两个虚拟核心。虽然这些额外的资源能够提升性能，但并行处理所带来的大部分速度提升都出现在处理过程使用的核心数量少于物理核心数量的情况下。\n对于fit_resamples()以及tune中的其他函数，当用户注册了并行后端包时，就会进行并行处理。这些R包定义了如何执行并行处理。在Unix和macOS操作系统上，并行计算的一种方法是通过分叉线程（forking threads）。要启用此功能，请加载doMC包，并使用foreach注册并行核心的数量：\n# Unix and macOS only\nlibrary(doMC)\nregisterDoMC(cores = 2)\n\n# Now run fit_resamples()...\n这会指示fit_resamples()在两个核心上各运行一半的计算。要将计算重置为顺序处理可以使用registerDoSEQ()。\n另一种并行化计算的方法是使用网络套接字。doParallel包支持这种方法（所有操作系统都可使用）：\n# All operating systems\nlibrary(doParallel)\n\n# Create a cluster object and then register:\ncl &lt;- makePSOCKcluster(2)\nregisterDoParallel(cl)\n\n# Now run fit_resamples()`...\n\nstopCluster(cl)\n另一个并行处理的R包是future包。与foreach包类似，它提供了一个并行处理框架。该包通过doFuture包与foreach结合使用。（为foreach提供并行后端的R包都以”do”为前缀开头）。\n使用tune包进行并行处理时，在前几个核心的情况下往往能带来线性的速度提升。这意味着，使用两个核心时，计算速度会快一倍。根据数据和模型类型的不同，在使用四到五个核心之后，线性速度提升的效果会减弱。使用更多的核心仍然会减少完成任务所需的时间，只是额外核心带来的回报会递减。\n让我们用关于并行性的最后一点说明来结束。对于这些技术中的每一种，内存需求会随着所使用的额外核心数量而倍增。例如，如果当前数据集在内存中为2GB，且使用了3个核心，那么总内存需求就是8GB（每个工作进程2GB，再加上原始的2GB）。使用过多的核心可能会导致计算（以及计算机）显著变慢。",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#saving-the-resampled-objects",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#saving-the-resampled-objects",
    "title": "10 Resampling for Evaluating Performance",
    "section": "Saving the Resampled Objects",
    "text": "Saving the Resampled Objects\n重抽样过程中创建的模型不会被保留。这些模型的训练目的是评估性能，在我们计算出性能统计数据后，通常就不再需要它们了。如果某种特定的建模方法被证明是我们数据集的最佳选择，那么最好的做法是再次对整个训练集进行拟合，这样就能利用更多的数据来估计模型参数。\n虽然重抽样过程中创建的这些模型不会被保存，但有一种方法可以保留它们或其部分组件——为control_resamples()函数的extract参数提供一个接受单个参数的函数。执行时，无论你是否向fit_resamples()提供了工作流，x都会生成一个拟合好的工作流对象。回想一下，workflows包中包含一些函数，能够提取这些对象的不同组件（例如模型、配方等）。\n让我们使用第8章中的recipe对象来拟合一个线性回归模型：\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(ames_rec) %&gt;%\n  add_model(linear_reg() %&gt;% set_engine(\"lm\"))\n\nlm_fit &lt;- lm_wflow %&gt;% fit(data = ames_train)\n\n# Select the recipe:\nextract_recipe(lm_fit, estimated = TRUE)\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Collapsing factor levels for: Neighborhood | Trained\n#&gt; • Dummy variables from: Neighborhood Bldg_Type | Trained\n#&gt; • Interactions with: Gr_Liv_Area:Bldg_Type_TwoFmCon, ... | Trained\n#&gt; • Natural splines on: Latitude Longitude | Trained\n\n我们可以从工作流中保存拟合模型对象的线性模型系数：\n\nget_model &lt;- function(x) {\n  extract_fit_parsnip(x) %&gt;% tidy()\n}\n\n# Test it using:\n# get_model(lm_fit)\n\n现在让我们将这个函数应用到这10个重抽样拟合结果上。提取函数的结果被包装在一个列表对象中，并以tibble的形式返回：\n\nctrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow %&gt;% fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;   splits             id     .metrics         .notes           .extracts\n#&gt;   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;   \n#&gt; 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n#&gt; # ℹ 4 more rows\n\n现在有一个包含嵌套 tibble 的.extracts 列。这些包含什么内容呢？让我们通过子集化来一探究竟。\n\nlm_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts         .config        \n#&gt;   &lt;list&gt;            &lt;chr&gt;          \n#&gt; 1 &lt;tibble [73 × 5]&gt; pre0_mod0_post0\n\n# To get the results\nlm_res$.extracts[[1]][[1]]\n#&gt; [[1]]\n#&gt; # A tibble: 73 × 5\n#&gt;   term                        estimate  std.error statistic   p.value\n#&gt;   &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)                 1.48     0.320         4.62   4.11e-  6\n#&gt; 2 Gr_Liv_Area                 0.000158 0.00000476   33.2    9.72e-194\n#&gt; 3 Year_Built                  0.00180  0.000149     12.1    1.57e- 32\n#&gt; 4 Neighborhood_College_Creek -0.00163  0.0373       -0.0438 9.65e-  1\n#&gt; 5 Neighborhood_Old_Town      -0.0757   0.0138       -5.47   4.92e-  8\n#&gt; 6 Neighborhood_Edwards       -0.109    0.0310       -3.53   4.21e-  4\n#&gt; # ℹ 67 more rows\n\n这看起来可能是一种复杂的保存模型结果的方法。然而，extract十分灵活，它并不假设用户每个重抽样只保存一个tibble。例如，tidy()方法既可以在配方上运行，也可以在模型上运行。在这种情况下，会返回一个包含两个 tibble 的列表。\n对于我们这个更简单的示例，所有结果都可以使用以下方式进行扁平化处理和收集：\n\nall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Show the replicates for a single predictor:\nfilter(all_coef, term == \"Year_Built\")\n#&gt; # A tibble: 10 × 5\n#&gt;   term       estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Year_Built  0.00180  0.000149      12.1 1.57e-32\n#&gt; 2 Year_Built  0.00180  0.000151      12.0 6.45e-32\n#&gt; 3 Year_Built  0.00185  0.000150      12.3 1.00e-33\n#&gt; 4 Year_Built  0.00183  0.000147      12.5 1.90e-34\n#&gt; 5 Year_Built  0.00184  0.000150      12.2 2.47e-33\n#&gt; 6 Year_Built  0.00180  0.000150      12.0 3.35e-32\n#&gt; # ℹ 4 more rows\n\n第13章和第14章讨论了一套用于模型调优的函数。它们的接口与fit_resamples()类似，并且这里描述的许多功能也适用于这些函数。",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/10 Resampling for Evaluating Performance.html#chapter-summary",
    "title": "10 Resampling for Evaluating Performance",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了数据分析的基本工具之一，即衡量模型结果的性能和变异性的能力。重抽样使我们能够在不使用测试集的情况下确定模型的运行效果。\n介绍了tune包中的一个重要函数，名为fit_resamples()。该函数的接口在后续描述模型调优工具的章节中也会用到。\n到目前为止，针对Ames数据的分析代码如下：\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nrf_model &lt;-\n  rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wflow &lt;-\n  workflow() %&gt;%\n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n      Latitude + Longitude) %&gt;%\n  add_model(rf_model)\n\nset.seed(1001)\names_folds &lt;- vfold_cv(ames_train, v = 10)\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = ames_folds, control = keep_pred)",
    "crumbs": [
      "10 Resampling for Evaluating Performance"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html",
    "title": "11 Comparing Models with Resampling",
    "section": "",
    "text": "一旦我们创建了两个或更多模型，下一步就是对它们进行比较，以了解哪个模型最好。在某些情况下，“比较”可能在模型内进行——使用不同的特征或预处理方法对同一个模型进行评估。另外，模型间的比较更为常见，比如我们在第10章中对线性回归模型和随机森林模型进行的比较。\n无论是哪种情况，结果都是每个模型的一组重采样汇总统计量（例如，均方根误差、准确率等）。在本章中，我们首先演示如何使用工作流集来拟合多个模型。然后，我们讨论重采样统计量的重要方面。最后，我们探讨如何规范地比较模型（使用假设检验或贝叶斯方法）。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#creating-multiple-models-with-workflow-sets",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#creating-multiple-models-with-workflow-sets",
    "title": "11 Comparing Models with Resampling",
    "section": "Creating Multiple Models with Workflow Sets",
    "text": "Creating Multiple Models with Workflow Sets\n在第7.5节中，我们介绍了工作流集合的概念，即可以组合生成不同的预处理器和/或模型。在第10章，我们使用了针对Ames数据的recipe对象——包含了一个交互项，以及用于经度和纬度的样条函数。为了更深入地展示工作流集合的应用，让我们依次逐步添加这些预处理步骤，创建三个不同的线性模型，随后检验这些新增项是否能提升模型的表现。\n首先，创建三个recipe对象，再将它们整合进一个工作流集合中：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\ndata(ames)\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\nbasic_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors())\n\ninteraction_rec &lt;-\n  basic_rec %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\"))\n\nspline_rec &lt;-\n  interaction_rec %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 50)\n\npreproc &lt;-\n  list(\n    basic = basic_rec,\n    interact = interaction_rec,\n    splines = spline_rec\n  )\n\nlm_models &lt;- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE)\nlm_models\n#&gt; # A workflow set/tibble: 3 × 4\n#&gt;   wflow_id    info             option    result    \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n我们希望依次对这些模型进行重新采样。为此，我们将使用一个purrr-like函数，名为workflow_map()。该函数首先接受一个要应用于工作流的函数作为初始参数，随后是该函数的相关选项。此外，我们还设置了verbose参数，用于打印进度信息；以及seed参数，确保每种模型都使用与其他模型相同的随机数种子。\n\names_folds &lt;- vfold_cv(ames_train, v = 10)\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nlm_models &lt;-\n  lm_models %&gt;%\n  workflow_map(\"fit_resamples\",\n    # Options to `workflow_map()`:\n    seed = 1101, verbose = TRUE,\n    # Options to `fit_resamples()`:\n    resamples = ames_folds, control = keep_pred\n  )\n#&gt; i 1 of 3 resampling: basic_lm\n#&gt; ✔ 1 of 3 resampling: basic_lm (3.6s)\n#&gt; i 2 of 3 resampling: interact_lm\n#&gt; ✔ 2 of 3 resampling: interact_lm (3.7s)\n#&gt; i 3 of 3 resampling: splines_lm\n#&gt; ✔ 3 of 3 resampling: splines_lm (5.8s)\nlm_models\n#&gt; # A workflow set/tibble: 3 × 4\n#&gt;   wflow_id    info             option    result   \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n\n请注意，option列和result列现已填满内容。前者包含了之前提供的fit_resamples()函数的选项（以确保可重复性），而后者则列出了由fit_resamples()生成的结果。\n有一些适用于工作流集合的便捷函数，包括用于汇总性能统计信息的collect_metrics()。我们还可以使用filter()来筛选出感兴趣的特定指标：\n\ncollect_metrics(lm_models) %&gt;%\n  filter(.metric == \"rmse\")\n#&gt; # A tibble: 3 × 9\n#&gt;   wflow_id    .config         preproc model      .metric .estimator   mean\n#&gt;   &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 basic_lm    pre0_mod0_post0 recipe  linear_reg rmse    standard   0.0800\n#&gt; 2 interact_lm pre0_mod0_post0 recipe  linear_reg rmse    standard   0.0796\n#&gt; 3 splines_lm  pre0_mod0_post0 recipe  linear_reg rmse    standard   0.0786\n#&gt; # ℹ 2 more variables: n &lt;int&gt;, std_err &lt;dbl&gt;\n\n上一章中的随机森林模型呢？我们可以先将其转换为独立的工作流集，然后再绑定行，从而将其加入到集合中。需要注意的是，当对该模型进行重采样时，必须在control_resamples()函数中设置save_workflow = TRUE。\n\nrf_model &lt;-\n  rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wflow &lt;-\n  workflow() %&gt;%\n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n      Latitude + Longitude\n  ) %&gt;%\n  add_model(rf_model)\n\nrf_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = ames_folds, control = keep_pred)\n\nfour_models &lt;-\n  as_workflow_set(random_forest = rf_res) %&gt;%\n  bind_rows(lm_models)\nfour_models\n#&gt; # A workflow set/tibble: 4 × 4\n#&gt;   wflow_id      info             option    result   \n#&gt;   &lt;chr&gt;         &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 random_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 basic_lm      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 3 interact_lm   &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 4 splines_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n\nautoplot()方法的输出如 Figure 1 所示，它按模型从优到劣的顺序展示了各模型的置信区间。在本章中，我们将重点关注决定系数（又称\\(R^2\\)），并在调用时使用参数metric = \"rsq\"来设置我们的图表：\n\nlibrary(ggrepel)\nautoplot(four_models, metric = \"rsq\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1 / 8, nudge_y = 1 / 100) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 1: Confidence intervals for the coefficient of determination using four different models\n\n\n\n\n从这一组\\(R^2\\)置信区间图中，我们可以看出随机森林方法表现最佳，而随着我们逐步增加配方步骤，线性模型也呈现出微小的改进。\n现在，我们已为四个模型中的每一个模型获得了10个重新采样的性能估计值，这些汇总统计量可用于进行模型间的比较。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#comparing-resampled-performance-statistics",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#comparing-resampled-performance-statistics",
    "title": "11 Comparing Models with Resampling",
    "section": "Comparing Resampled Performance Statistics",
    "text": "Comparing Resampled Performance Statistics\n上述三种线性模型的结果似乎表明，新增的项并未显著改善线性模型的平均RMSE或\\(R^2\\)统计量。尽管差异较小，但可能已超出系统中的实验噪声范围，即在统计上具有显著性意义。我们可以进行正式的检验假设来说明：新增项确实提高了\\(R^2\\)值。\n在进行模型间比较之前，我们有必要先讨论一下重采样统计量的样本内相关性。每个模型都使用相同的重采样数据集进行评估，这通往往会导致重采样的结果较为接近。换句话说，有些重采样情况下，各模型的表现往往较低；而在另一些重采样情况下，则倾向于较高。在统计学中，这被称为重采样间（resample-to-resample）的变异成分。\n举例来说，让我们汇总线性模型和随机森林的各个重采样统计量。我们将重点关注每个模型的\\(R^2\\)统计量，该统计量用于衡量每栋房屋的实际销售价格与预测销售价格之间的相关性。接下来，我们通过filter()函数仅保留这些\\(R^2\\)指标，然后重塑结果，并计算这些指标彼此之间的相关性。\n\nrsq_indiv_estimates &lt;-\n  collect_metrics(four_models, summarize = FALSE) %&gt;%\n  filter(.metric == \"rsq\")\n\nrsq_wider &lt;-\n  rsq_indiv_estimates %&gt;%\n  select(wflow_id, .estimate, id) %&gt;%\n  pivot_wider(id_cols = \"id\", names_from = \"wflow_id\", values_from = \".estimate\")\n\ncorrr::correlate(rsq_wider %&gt;% select(-id), quiet = TRUE)\n#&gt; # A tibble: 4 × 5\n#&gt;   term          random_forest basic_lm interact_lm splines_lm\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 random_forest        NA        0.929       0.922      0.879\n#&gt; 2 basic_lm              0.929   NA           0.996      0.943\n#&gt; 3 interact_lm           0.922    0.996      NA          0.956\n#&gt; 4 splines_lm            0.879    0.943       0.956     NA\n\n结果表明在不同模型之间，每次重采样内的相关性很大。为了更直观地体现这一点，每个模型的\\(R^2\\)统计量都以连线的方式标注了各次重采样的结果，如 Figure 2 所示：\n\nrsq_indiv_estimates %&gt;%\n  mutate(wflow_id = reorder(wflow_id, .estimate)) %&gt;%\n  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) +\n  # ggplot(aes(x = id, y = .estimate, group = wflow_id, color = wflow_id)) +\n  geom_line(alpha = .5, linewidth = 1.25) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 2: Resample statistics across models\n\n\n\n\n如果重采样间的效应不存在，就不会出现任何平行趋势。存在一种用于检验相关性的统计方法，旨在评估这些相关性的大小是否仅仅是随机噪声。对于线性模型而言：\n\nrsq_wider %&gt;%\n  with(cor.test(basic_lm, splines_lm)) %&gt;%\n  tidy() %&gt;%\n  select(estimate, starts_with(\"conf\"))\n#&gt; # A tibble: 1 × 3\n#&gt;   estimate conf.low conf.high\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1    0.943    0.772     0.987\n\n相关性检验的结果estimate列（相关性估计值及置信区间）表明，样本内相关性似乎确实存在。\n额外的相关性对我们的分析有何影响？请考虑两个变量之差的方差：\n\\[\n\\operatorname{Var}[X - Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y]  - 2 \\operatorname{Cov}[X, Y]\n\\]\n最后的项是两个变量之间的协方差。如果存在显著的正协方差，那么在比较两个模型差异时，任何针对这一差异的统计检验都将严重缺乏检验力。换句话说，忽略重采样间的效应，可能会导致我们的模型比较倾向于得出“模型间无明显差异”的结论。这种重采样统计的特性将在接下来的两个小节中详细讨论。\n在进行模型比较或查看重采样结果之前，我们需要明确一个相关的实际效应大小。例如，上述这些分析聚焦于\\(R^2\\)统计量，因此实际效应大小即为我们认为具有实际意义且值得关注的\\(R^2\\)值变化幅度。例如，我们可能认为，如果两个模型的\\(R^2\\)值相差不超过\\(\\pm 2\\)%，则它们在实际意义上并无差异。换言之，即使差异在统计上显著，若小于2%，我们也视其为不具重要性。\n实践意义是主观的；两个人对重要性的阈值可能有着截然不同的看法。不过，我们稍后会说明，在模型选择过程中，这一考量实际上非常有帮助。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#simple-hypothesis-testing-methods",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#simple-hypothesis-testing-methods",
    "title": "11 Comparing Models with Resampling",
    "section": "Simple Hypothesis Testing Methods",
    "text": "Simple Hypothesis Testing Methods\n我们可以使用简单的假设检验来对模型进行正式比较。考虑熟悉的线性统计模型：\n\\[\ny_{ij} = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip} + \\epsilon_{ij}\n\\]\n这种多功能模型既可用于构建回归模型，也是广泛应用的方差分析（ANOVA）技术中比较各组差异的基础。在ANOVA模型中，预测变量（\\(x_{ij}\\)）是用于区分不同组别的二元虚拟变量。通过这些变量，\\(\\beta\\)参数能够借助假设检验方法，判断两个或多个组之间是否存在显著差异。在我们具体的情境中，ANOVA同样可以用于模型比较。\n假设每个重新抽样的\\(R^2\\)统计量作为因变量数据（即\\(y_{ij}\\)），而各模型则作为ANOVA模型中的预测变量。这种数据结构的示例见 Table 1 。\n\n\nTable 1: Model performance statistics as a data set for analysis.\n\n\n\nY = rsq\nmodel\nX1\nX2\nX3\nid\n\n\n\n0.8441590\nbasic_lm\n0\n0\n0\nFold01\n\n\n0.8441893\ninteract_lm\n1\n0\n0\nFold01\n\n\n0.8717934\nrandom_forest\n0\n1\n0\nFold01\n\n\n0.8378595\nsplines_lm\n0\n0\n1\nFold01\n\n\n0.7903836\nbasic_lm\n0\n0\n0\nFold02\n\n\n0.7974093\ninteract_lm\n1\n0\n0\nFold02\n\n\n\n\n\n\n表格中的X1、X2和X3列是model列中数值的指示器。它们的排列顺序与R语言定义这些列的方式相同，即按model列中名称的字母顺序排列。\n在我们的模型比较中，具体的方差分析模型是：\n\\[\ny_{ij} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\n\\]\n其中：\n\n\\(\\beta_0\\) 是basic_lm模型（即不含样条或交互作用项）的 \\(R^2\\) 均值的估计值；\n\\(\\beta_1\\) 表示当向basic_lm模型中引入交互作用时， \\(R^2\\) 均值的变化量；\n\\(\\beta_2\\) 则是basic_lm模型与随机森林模型之间 \\(R^2\\) 均值的变化量；\n\\(\\beta_3\\) 描述的是basic_lm模型与同时包含交互作用和样条函数的模型相比，\\(R^2\\) 均值的变化量。\n\n根据这些模型参数，我们生成假设检验和p值，以对模型进行统计比较；但同时，我们必须解决重采样间效应的问题。过去，研究者通常将这些重复抽样的组别视为区块效应，并在模型中加入相应的项。另一种方法是将重采样效应视为随机效应，即认为这些特定的样本是从更大的潜在样本总体中随机抽取的。然而，实际上我们并不真正关注这些效应——我们只是希望在模型中加以调整，从而确保能够准确估计出那些真正感兴趣的差异的方差。\n将重抽样间的效应视为随机效应，在理论上颇具吸引力。用于拟合具有此类随机效应的方差分析模型的方法，包括线性混合模型（Faraway, 2016）或和叶斯层次模型（如下一节所示）。\n一种简单快捷的两模型比较方法，是将两个模型的\\(R^2\\)值之差作为ANOVA模型中的响应数据。由于这些响应数据通过重采样进行匹配，因此差异中不包含重采样间的效应，正因如此，标准的ANOVA模型完全适用。例如，以下对lm()函数的调用正是用于检验两个线性回归模型之间的差异：\n\ncompare_lm &lt;-\n  rsq_wider %&gt;%\n  mutate(difference = splines_lm - basic_lm)\n\nlm(difference ~ 1, data = compare_lm) %&gt;%\n  tidy(conf.int = TRUE) %&gt;%\n  select(estimate, p.value, starts_with(\"conf\"))\n#&gt; # A tibble: 1 × 4\n#&gt;   estimate p.value conf.low conf.high\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1  0.00730   0.130 -0.00262    0.0172\n\n# Alternatively, a paired t-test could also be used:\nrsq_wider %&gt;%\n  with(t.test(splines_lm, basic_lm, paired = TRUE)) %&gt;%\n  tidy() %&gt;%\n  select(estimate, p.value, starts_with(\"conf\"))\n#&gt; # A tibble: 1 × 4\n#&gt;   estimate p.value conf.low conf.high\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1  0.00730   0.130 -0.00262    0.0172\n\n我们可以用这种方式评估每一对的差异。需要注意的是，p值表明存在统计学上显著的信号；经由样条函数构建的经度和纬度项确实似乎产生了影响。然而，\\(R^2\\)的差异仅估计为0.91%。如果我们的实际效应大小为2%，那么可能就不会认为这些项值得纳入模型中了。\n我们之前已简要提及过p值，但究竟什么是p值呢？根据 Wasserstein 和 Lazar（2016年）的解释：非正式地说，p值是在特定统计模型下，数据的统计量（例如，两组比较时样本均值的差异）等于或超过其观测值的概率。换句话说，如果在“无差异”的零假设下重复进行这种分析多次，p值将反映我们所观察到的结果究竟有多极端。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#bayesian-methods",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#bayesian-methods",
    "title": "11 Comparing Models with Resampling",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\n我们刚刚通过假设检验来比较模型，但也可以采用更通用的方法，利用随机效应和贝叶斯统计来进行比较（McElreath 2020）。尽管这种方法比方差分析更为复杂，但其解释却比p值法更加简单明了。\n此前的方差分析模型中，残差 \\(\\epsilon_{ij}\\) 被假定为相互独立，并服从均值为0、标准差恒定为 \\(\\sigma\\) 的正态分布。基于这一假设，统计理论表明，所估计的回归参数也遵循多变量正态分布，进而可据此计算出p值和置信区间。\n\\[\ny_{ij} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\n\\]\n贝叶斯线性模型做出了额外的假设。除了为残差指定分布外，我们还需要为模型参数（\\(\\beta_j\\) 和 \\(\\sigma\\)）设定先验分布。这些分布是模型在接触观测数据之前所假定的参数分布。例如，上述模型的一组简单先验分布可能是：\n\\[\n\\begin{align}\n\\epsilon_{ij} &\\sim N(0, \\sigma) \\notag \\\\\n\\beta_j &\\sim N(0, 10) \\notag \\\\\n\\sigma &\\sim \\text{exponential}(1) \\notag\n\\end{align}\n\\]\n这些先验分布设定了模型参数的可能范围或合理范围，且不包含任何未知参数。例如，关于 \\(\\sigma\\) 的先验表明，其取值必须大于零，呈现明显的右偏分布，并且通常小于3或4。\n请注意，回归参数的先验分布相当宽泛，标准差为10。在许多情况下，我们对这一先验可能并没有强烈的看法，只需确保其呈对称且钟形即可。较大的标准差意味着这是一个信息量较少的先验——它并未对参数可能取值范围施加过于严格的限制，从而让数据在参数估计过程中发挥更大的影响力。\n根据观测数据和先验分布的设定，模型参数便可被估计出来。这些参数的最终分布（后验分布）是先验分布与似然估计的组合，完整地体现了模型对参数的概率描述，正是我们所关注的关键分布。\nA random intercept model\n为了使我们的贝叶斯方差分析模型能够充分模拟重采样过程，我们考虑采用随机截距模型（random intercept model）。在此模型中，我们假设重采样仅通过改变截距来影响模型。需要注意的是，这一假设限制了重采样对回归参数\\(\\beta_j\\)的影响存在差异——即假定这些参数在不同重采样之间保持相同的关系。该模型的方程如下：\n\\[\ny_{ij} = (\\beta_0 + b_{i}) + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\n\\]\n这对于重新抽样的统计方法来说并非是不合理的模型，当这些方法如 Figure 2 所示在不同模型间绘制时，往往表现出较为平行的效果（即各条线几乎不会交叉）。\n对于这种模型配置，我们对随机效应的先验分布做出了一个额外假设。合理的假设是采用另一种对称分布，例如另一条钟形曲线。鉴于我们的汇总统计数据显示有效样本量仅为10，我们将选择一个比标准正态分布更宽的先验分布。具体来说，我们将使用自由度为1的t分布（即 \\(b_i \\sim t(1)\\)），其尾部比相应的高斯分布更为厚重。\ntidyposterior包提供了用于拟合此类贝叶斯模型的函数，以便比较重新采样的模型。其中，主函数名为perf_mod()，它被设计为能“无缝兼容”不同类型的对象：\n\n对于工作流集合workflwo_set，它会创建一个ANOVA模型，其中各组对应于不同的工作流。我们现有的模型并未优化任何调参参数（详见接下来的三章）。如果集合中的某个工作流包含调参数据，则在贝叶斯分析中将采用该工作流的最佳调参设置。换句话说，尽管存在调参参数，perf_mod()的重点仍在于进行工作流间的比较。\n对于包含单个经重采样调优的模型的对象tune_results，perf_mod()会进行模型内部的比较。在这种情况下，贝叶斯 ANOVA 模型中所检验的分组变量正是由调参参数定义的子模型。\nperf_mod() 函数还可以接受由rsample生成的数据框，该数据框包含与两个或多个模型/工作流结果相关的性能指标列。这些指标可能是通过非标准方式生成的。\n\n从这些类型的对象中，perf_mod()函数会确定一个合适的贝叶斯模型，并利用重采样统计量对其进行拟合。在我们的示例中，它将对与工作流相关的四组 \\(R^2\\) 统计量进行建模。\ntidyposterior 包通过rstanarm包调用“Stan software”指定并拟合模型。rstanarm包中的函数具有默认先验分布（详细信息请参阅 ?rstanarm::priors）。以下模型对所有参数均采用默认先验，但随机截距除外——它遵循“t-分布”。由于估计过程涉及随机数生成，因此在函数调用中设置了随机种子。估计过程采用迭代方式，并在称为“链”的多个集合中重复进行多次。iter参数用于指定每个链中估计过程的迭代次数。当使用多条链时，其结果将被合并（前提是已通过诊断评估验证）。\n\nlibrary(tidyposterior)\nlibrary(rstanarm)\n#&gt; Loading required package: Rcpp\n#&gt; \n#&gt; Attaching package: 'Rcpp'\n#&gt; The following object is masked from 'package:rsample':\n#&gt; \n#&gt;     populate\n#&gt; This is rstanarm version 2.32.2\n#&gt; - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#&gt; - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#&gt; - For execution on a local, multicore CPU with excess RAM we recommend calling\n#&gt;   options(mc.cores = parallel::detectCores())\n\n# The rstanarm package creates copious amounts of output; those results\n# are not shown here but are worth inspecting for potential issues. The\n# option `refresh = 0` can be used to eliminate the logging.\nrsq_anova &lt;-\n  perf_mod(\n    four_models,\n    metric = \"rsq\",\n    prior_intercept = rstanarm::student_t(df = 1),\n    chains = 4,\n    iter = 5000,\n    seed = 1102\n  )\n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 6.4e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration: 1500 / 5000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration: 2000 / 5000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 2500 / 5000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 2501 / 5000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 3.815 seconds (Warm-up)\n#&gt; Chain 1:                2.254 seconds (Sampling)\n#&gt; Chain 1:                6.069 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 2.4e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration: 1500 / 5000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration: 2000 / 5000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 2500 / 5000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 2501 / 5000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 3.258 seconds (Warm-up)\n#&gt; Chain 2:                2.484 seconds (Sampling)\n#&gt; Chain 2:                5.742 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 2.3e-05 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 5000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  500 / 5000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 5000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration: 1500 / 5000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration: 2000 / 5000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 2500 / 5000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 2501 / 5000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 5000 / 5000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 3.956 seconds (Warm-up)\n#&gt; Chain 3:                2.204 seconds (Sampling)\n#&gt; Chain 3:                6.16 seconds (Total)\n#&gt; Chain 3: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n#&gt; Chain 4: \n#&gt; Chain 4: Gradient evaluation took 6e-05 seconds\n#&gt; Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.6 seconds.\n#&gt; Chain 4: Adjust your expectations accordingly!\n#&gt; Chain 4: \n#&gt; Chain 4: \n#&gt; Chain 4: Iteration:    1 / 5000 [  0%]  (Warmup)\n#&gt; Chain 4: Iteration:  500 / 5000 [ 10%]  (Warmup)\n#&gt; Chain 4: Iteration: 1000 / 5000 [ 20%]  (Warmup)\n#&gt; Chain 4: Iteration: 1500 / 5000 [ 30%]  (Warmup)\n#&gt; Chain 4: Iteration: 2000 / 5000 [ 40%]  (Warmup)\n#&gt; Chain 4: Iteration: 2500 / 5000 [ 50%]  (Warmup)\n#&gt; Chain 4: Iteration: 2501 / 5000 [ 50%]  (Sampling)\n#&gt; Chain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)\n#&gt; Chain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)\n#&gt; Chain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)\n#&gt; Chain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)\n#&gt; Chain 4: Iteration: 5000 / 5000 [100%]  (Sampling)\n#&gt; Chain 4: \n#&gt; Chain 4:  Elapsed Time: 3.546 seconds (Warm-up)\n#&gt; Chain 4:                1.945 seconds (Sampling)\n#&gt; Chain 4:                5.491 seconds (Total)\n#&gt; Chain 4:\n\n生成的对象包含重采样过程的信息，同时还嵌入了Stan对象（存储在一个名为stan的元素中）。我们最感兴趣的是回归参数的后验分布。tidyposterior包提供了一个tidy()方法，可将这些后验分布提取到一个tibble中：\n\nmodel_post &lt;-\n  rsq_anova %&gt;%\n  # Take a random sample from the posterior distribution\n  # so set the seed again to be reproducible.\n  tidy(seed = 1103)\n\nglimpse(model_post)\n#&gt; Rows: 40,000\n#&gt; Columns: 2\n#&gt; $ model     &lt;chr&gt; \"random_forest\", \"basic_lm\", \"interact_lm\", \"splines_lm\",…\n#&gt; $ posterior &lt;dbl&gt; 0.8317200, 0.7925903, 0.7946385, 0.7987719, 0.8412325, 0.…\n\n四个后验分布如 Figure 3 所示。\n\nmodel_post %&gt;%\n  mutate(model = forcats::fct_inorder(model)) %&gt;%\n  ggplot(aes(x = posterior)) +\n  geom_histogram(bins = 50, color = \"white\", fill = \"blue\", alpha = 0.4) +\n  facet_wrap(~model, ncol = 1)\n\n\n\n\n\n\nFigure 3: Posterior distributions for the coefficient of determination using four different models\n\n\n\n\n这些直方图描述了每个模型中均值\\(R^2\\)值的估计概率分布。其中存在一定重叠，尤其是在三个线性模型之间。\n此外，模型结果还提供了一个基本的autoplot()方法，如 Figure 4 所示，以及经过整理后的对象，其中叠加展示了密度图。\n\nautoplot(rsq_anova) +\n  geom_text_repel(aes(label = workflow), nudge_x = 1 / 8, nudge_y = 1 / 100) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 4: Credible intervals derived from the model posterior distributions\n\n\n\n\n使用贝叶斯模型进行重采样的一大优势在于，一旦我们获得了参数的后验分布，便能轻松得到这些参数组合的后验分布。例如，若要比较两个线性回归模型，我们关注的是两者的均值差异。计算这一差异的后验分布时，只需分别从每个模型的后验中独立采样，并取其差值即可。contrast_models()函数便可完成这一任务。要指定需要进行的比较，list_1和list_2参数需接收字符型向量，并计算这两个列表中模型之间的差异（具体表达为list_1 - list_2）。\n我们可以比较两个线性模型，并将结果可视化于 Figure 5 中。\n\nrqs_diff &lt;-\n  contrast_models(rsq_anova,\n    list_1 = \"splines_lm\",\n    list_2 = \"basic_lm\",\n    seed = 1104\n  )\n\nrqs_diff %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = difference)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_histogram(bins = 50, color = \"white\", fill = \"red\", alpha = 0.4)\n\n\n\n\n\n\nFigure 5: Posterior distribution for the difference in the coefficient of determination\n\n\n\n\n后验结果显示，分布的中心大于零（表明使用样条函数的模型通常具有较大值），但同时也与零存在一定程度的重叠。该对象的summary()方法不仅计算了分布的均值，还生成了可信区间——这是贝叶斯方法中对应于传统置信区间的概念。\n\nsummary(rqs_diff, size = 0.02) %&gt;%\n  select(contrast, starts_with(\"pract\"))\n#&gt; # A tibble: 1 × 4\n#&gt;   contrast               pract_neg pract_equiv pract_pos\n#&gt;   &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 splines_lm vs basic_lm         0        1.00    0.0002\n\nprobability列反映了后验分布中大于零的比例。这正是正差异真实存在的概率。该数值并不接近零，有力地表明了统计显著性——即从统计学角度来看，实际差异确实不为零。\n然而，均值差的估计值非常接近零。回想一下，我们之前建议的实际效应大小为2%。借助后验分布，我们还可以计算出达到实际显著性的概率。在贝叶斯分析中，这被称为ROPE估计（即“实际等效区域”，Kruschke和Liddell，2018年）。要估算这一概率，可使用summary函数中的size选项：\n\nsummary(rqs_diff, size = 0.02) %&gt;%\n  select(contrast, starts_with(\"pract\"))\n#&gt; # A tibble: 1 × 4\n#&gt;   contrast               pract_neg pract_equiv pract_pos\n#&gt;   &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 splines_lm vs basic_lm         0        1.00    0.0002\n\npract_equiv列表示后验分布中位于[-size, size]区间内的比例（而pract_neg和pract_pos列则分别表示低于和高于该区间的比例）。这一较大数值表明，对于我们的效应大小而言，两个模型在实际意义上几乎完全相同的可能性极高。尽管之前的图表显示，我们观察到的差异很可能不为零，但等效性检验却提示，这种差异小到不足以在实际应用中产生意义。\n同样的流程也可用于将随机森林模型与经过重采样的一个或两个线性回归模型进行比较。实际上，当perf_mod()函数与工作流集合一起使用时，autoplot()方法能够展示实际等效性结果，直观地对比每个工作流与当前最佳模型（在本例中即随机森林模型）。\n\nautoplot(rsq_anova, type = \"ROPE\", size = 0.02) +\n  geom_text_repel(aes(label = workflow)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 6: Probability of practical equivalence for an effect size of 2%\n\n\n\n\nFigure 6 表明，在采用2%的实际效应大小时，没有任何一个线性模型能与随机森林模型相媲美。\nThe effect of the amount of resampling\n这些类型的正式贝叶斯比较中，重抽样的次数有何影响？更多的重抽样会提高整体重抽样估计的精度；而这种精度又会进一步传递到此类分析中。为了说明这一点，我们采用了重复交叉验证的方法增加了重抽样的次数。那么，后验分布发生了怎样的变化呢？ Figure 7 展示了在最多进行100次重抽样（由10次10折交叉验证的重复生成）情况下的90%可信区间。\n\n# 源数据见 https://github.com/tidymodels/TMwR/blob/main/RData/post_intervals.RData\nload(\"Rdata/post_intervals.RData\")\ny_lab &lt;- expression(Mean ~ difference ~ \"in\" ~ italic(R^2))\n\nggplot(\n  intervals,\n  aes(x = resamples, y = mean)\n) +\n  geom_path() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"red\", alpha = .1) +\n  labs(\n    y = y_lab,\n    x = \"Number of Resamples (repeated 10-fold cross-validation)\"\n  )\n\n\n\n\n\n\nFigure 7: Probability of practical equivalence to the random forest model\n\n\n\n\n随着重抽样的次数增加，区间宽度逐渐缩小。显然，从10次重抽样增至30次的效果，比从80次增加到100次更为显著。然而，使用“大量”重抽样所带来的收益正在递减——当然，“大量”的定义会因具体数据集而有所不同。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/11 Comparing Models with Resampling.html#chapter-summary",
    "title": "11 Comparing Models with Resampling",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了用于检验模型间性能差异的正式统计方法。我们展示了重采样内部效应——即同一重采样的结果往往较为相似；这一重采样汇总统计量的特点，需要进行恰当的分析，才能确保模型比较的有效性。此外，尽管统计显著性和实际意义都是模型比较中至关重要的概念，但二者并不相同。",
    "crumbs": [
      "11 Comparing Models with Resampling"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "",
    "text": "为了使用模型进行预测，必须先估计出该模型的参数。其中一些参数可以直接从训练数据中估算出来，而另一些被称为调优参数（tuning parameters）或超参数（hyperparameters）的参数，则无法直接从训练数据中获得，需事先指定。这些未知的结构化或其他类型数值对模型有重要影响，但无法通过数据直接推算。本章将提供调优参数的示例，并展示如何利用tidymodels系函数来创建和处理调优参数。此外，我们还将说明错误地选择这些值会导致过拟合（overfitting）现象，并介绍几种寻找最优调优参数值的策略。第13章和第14章将更深入地探讨用于调优的具体优化方法。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#model-parameters",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#model-parameters",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "Model Parameters",
    "text": "Model Parameters\n在普通的线性回归中，模型有两个参数 \\(\\beta_0\\) 和 \\(\\beta_1\\) :\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n当我们有结果 \\(y\\) 和 预测变量 \\(x\\) 的数据时，我们可以估计出 \\(\\beta_0\\) 和 \\(\\beta_1\\) ：\n\\[\n\\hat \\beta_1 = \\frac{\\sum_i (y_i-\\bar{y})(x_i-\\bar{x})}{\\sum_i(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat \\beta_0 = \\bar{y}-\\hat \\beta_1 \\bar{x}\n\\]\n对于这个示例模型，我们可以直接从数据中估计这些值，因为它们在解析上是可处理的；但在许多情况下，模型的参数无法直接从数据中估计出来。例如，对于KNN模型，给定某个输入 \\(x_0\\) ，它的预测值方程为：\n\\[\n\\hat y = \\frac{1}{K}\\sum_{\\ell = 1}^K x_\\ell^*\n\\]\n其中 \\(K\\) 是邻居的数量，而 \\(x_\\ell^*\\) 是训练集中与 \\(x_0\\) 最接近的 \\(K\\) 个值。KNN模型由预测方程来定义，而不是模型方程。这一特性，再加上距离度量可能难以处理，使得无法创建一组可求解 \\(K\\) 的方程（无论是迭代求解还是通过其他方式）。邻居的数量对模型有着深远的影响，它决定了类别边界的灵活性。当 \\(K\\) 值较小时，边界会非常复杂；而当 \\(K\\) 值较大时，边界可能会相当平滑。\n最近邻的数量 \\(K\\) 是一个很好的调优参数或超参数的例子，它无法直接从数据中估计出来。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#tuning-parameters-for-different-types-of-models",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#tuning-parameters-for-different-types-of-models",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "Tuning Parameters for Different Types of Models",
    "text": "Tuning Parameters for Different Types of Models\n在不同的统计模型和机器学习模型中，存在许多调优参数或超参数的例子：\n\nboosting是一种集成方法，它结合了一系列基础模型，每个基础模型都是依次创建的，并且依赖于之前的模型。boosting迭代次数是一个重要的调优参数，通常需要进行优化。\n在经典的单层人工神经网络（又名多层感知器）中，预测变量通过两个或多个隐藏单元进行组合。隐藏单元是预测变量的线性组合，这些组合被包含在一个激活函数（通常是一种非线性函数，如“sigmoid”函数）中。然后，隐藏单元与输出单元相连；回归模型使用一个输出单元，而分类则需要多个输出单元。隐藏单元的数量和激活函数的类型是重要的结构调优参数。\n现代梯度下降方法通过找到合适的优化参数得到了改进。这类超参数的例子包括学习率、动量以及优化迭代次数/轮数（Goodfellow，Bengio 和 Courville，2016）。神经网络和一些集成模型利用梯度下降来估计模型参数。虽然与梯度下降相关的调优参数并非结构参数，但它们通常需要进行调优。\n\n在某些情况下，预处理步骤也需要调优：\n\n在主成分分析（principal component analysis）或者其有监督的“近亲”——偏最小二乘（partial least squares）中，预测变量会被替换为新的、人工构建的特征，这些特征在共线性方面具有更优的性质。提取的成分数量是可以调整的。\n插补方法利用一个或多个预测变量的完整值来估计缺失的预测变量值。一种有效的插补工具是使用完整列的 \\(K\\)-近邻 来预测缺失值。邻居的数量可以进行调整。\n\n一些经典的统计模型也具有结构参数：\n\n在二元回归中，通常使用“logit”链接函数（即逻辑回归）。也可以使用其他链接函数，如“probit”和“complementary log-log”（Dobson，1999）。第12.3节对该示例进行了更详细的描述。\n非贝叶斯纵向和重复测量模型需要明确数据的协方差或相关结构。可选的结构包括复合对称（compound symmetric）（也称为可交换）、自回归（autoregressive）、托普利茨（Toeplitz）等（Littell，Pendergast 和 Natarajan，2000）。\n\n一个不适合调整参数的例子是贝叶斯分析所需的先验分布。先验分布概括了分析师在考虑证据或数据之前对某个量的分布的信念。例如，在11.4节中，我们使用了贝叶斯方差分析模型，并且不确定回归参数的先验分布应该是什么（除了是对称分布之外）。我们选择了自由度为1的t分布作为先验分布，因为它具有更厚的尾部；这反映了我们额外的不确定性。我们的先验信念不应受到优化的影响。调优参数通常是为了优化性能而进行调整的，而先验分布不应为了得到“正确的结果”而被修改。另一个（或许更有争议的）不需要调整参数的例子是随机森林或装袋模型（bagging model）中树的数量。这个值应该选得足够大，以确保结果的数值稳定性；并且只要该值足够大，就能够产生可靠的结果，无需调整它来提升性能。随机森林通常需要数千棵树，而装袋模型所需的数量大约在50到100棵之间。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#what-do-we-optimize",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#what-do-we-optimize",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "What do we Optimize?",
    "text": "What do we Optimize?\n在优化调优参数时，我们应该如何评估模型呢？这取决于模型本身以及模型的用途。\n对于调优参数的统计特性易于处理的情况，可以将常见的统计特性用作目标函数。例如，在二元逻辑回归中，可以通过最大化似然函数或信息准则来选择链接函数。然而，这些统计特性可能与使用面向准确性的特性所得到的结果不一致。例如，Friedman（2001）对提升树集成中的树的数量进行了优化，发现在最大化似然函数和最大化准确性时得到了不同的结果：通过过拟合降低似然性实际上会提高错误分类率，尽管这可能违反直觉，但并不矛盾；似然性和错误率是拟合质量在不同方面的描述。\n为了说明这一点，考虑 Figure 1 中所示的分类数据，其中包含两个预测变量、两个类别，共593个数据点的训练集。\n\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\n\n\n\n\n\n\nFigure 1: An example two-class classification data set with two predictors\n\n\n\n\n我们可以先为这些数据拟合一个线性分类边界。最常用的方法是使用广义线性模型，其形式为逻辑回归。该模型通过“logit”函数将样本属于类别1的对数几率联系起来：\n\\[\n\\log\\left(\\frac{\\pi}{1 - \\pi}\\right) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\]\n在广义线性模型的背景下，“logit”函数是结果（\\(\\pi\\)）和预测变量之间的连接函数。还有其他连接函数，包括“probit”函数，其中 \\(\\Phi\\) 是累积标准正态函数，：\n\\[\n\\Phi^{-1}(\\pi) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\]\n还有“complementary log-log”函数：\n\\[\n\\log(-\\log(1-\\pi)) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\]\n上述模型都会产生线性分类边界，我们应该使用哪一个呢？因为上述模型参数的数量是不变的，我们可以计算每个模型在统计学上的（对数）似然值，并确定具有最大似然值的模型。传统上，似然值的计算需要使用与估计参数相同的数据，而不是使用像第5章和第10章中的数据拆分或重抽样之类的方法获得的数据。\n对于数据框training_set，我们来创建一个函数，用于计算不同的模型并提取训练集的似然统计量（使用broom::glance()）：\n\nllhood &lt;- function(...) {\n  logistic_reg() %&gt;%\n    set_engine(\"glm\", ...) %&gt;%\n    fit(Class ~ ., data = training_set) %&gt;%\n    glance() %&gt;%\n    select(logLik)\n}\n\nbind_rows(\n  llhood(),\n  llhood(family = binomial(link = \"probit\")),\n  llhood(family = binomial(link = \"cloglog\"))\n) %&gt;%\n  mutate(link = c(\"logit\", \"probit\", \"c-log-log\")) %&gt;%\n  arrange(desc(logLik))\n#&gt; # A tibble: 3 × 2\n#&gt;   logLik link     \n#&gt;    &lt;dbl&gt; &lt;chr&gt;    \n#&gt; 1  -258. logit    \n#&gt; 2  -262. probit   \n#&gt; 3  -270. c-log-log\n\n根据这些结果，逻辑回归模型具有最佳的统计特性。但从对数似然值的量级来看，很难判断这些差异是重要的还是可以忽略的。改进这种分析的一种方法是对统计数据进行重采样，并将建模数据与用于性能评估的数据分开。对于这个小数据集，重复10折交叉验证是重采样的一个不错选择。在yardstick包中，mn_log_loss()函数用于估计负对数似然，我们的结果如 Figure 2 所示。\n\nset.seed(1201)\nrs &lt;- vfold_cv(training_set, repeats = 10)\n\n# Return the individual resampled performance estimates:\nlloss &lt;- function(...) {\n  perf_meas &lt;- metric_set(roc_auc, mn_log_loss)\n\n  logistic_reg() %&gt;%\n    set_engine(\"glm\", ...) %&gt;%\n    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %&gt;%\n    collect_metrics(summarize = FALSE) %&gt;%\n    select(id, id2, .metric, .estimate)\n}\n\nresampled_res &lt;-\n  bind_rows(\n    lloss() %&gt;% mutate(model = \"logistic\"),\n    lloss(family = binomial(link = \"probit\")) %&gt;% mutate(model = \"probit\"),\n    lloss(family = binomial(link = \"cloglog\")) %&gt;% mutate(model = \"c-log-log\")\n  ) %&gt;%\n  # Convert log-loss to log-likelihood:\n  mutate(.estimate = ifelse(.metric == \"mn_log_loss\", -.estimate, .estimate)) %&gt;%\n  group_by(model, .metric) %&gt;%\n  summarize(\n    mean = mean(.estimate, na.rm = TRUE),\n    std_err = sd(.estimate, na.rm = TRUE) / sqrt(n()),\n    .groups = \"drop\"\n  )\n#&gt; → A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n#&gt; There were issues with some computations   A: x1\n#&gt; There were issues with some computations   A: x1\n#&gt; \n\nresampled_res %&gt;%\n  filter(.metric == \"mn_log_loss\") %&gt;%\n  ggplot(aes(x = mean, y = model)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),\n    width = .1\n  ) +\n  labs(y = NULL, x = \"log-likelihood\")\n\n\n\n\n\n\nFigure 2: Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions\n\n\n\n\n这些值的规模与之前的值不同，因为它们是在更小的数据集上计算得出的；broom::glance()产生的值是总和，而yardstick::mn_log_loss()产生的值是平均值。\n换一个不同的指标怎么样？我们还计算了每个重采样的ROC曲线下面积。这些结果反映了模型在众多概率阈值下的判别能力，在 Figure 3 中显示出没有差异。\n\nresampled_res %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  ggplot(aes(x = mean, y = model)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),\n    width = .1\n  ) +\n  labs(y = NULL, x = \"area under the ROC curve\")\n\n\n\n\n\n\nFigure 3: Means and approximate 90% confidence intervals for the resampled area under the ROC curve with three different link functions\n\n\n\n\n考虑到区间的重叠以及x轴的刻度，这些选项中的任何一个都可以使用。当三个模型的类别边界叠加在 Figure 4 中的198个数据点的测试集上时，我们再次看到了这一点。\n\ndata_grid &lt;- crossing(A = seq(0.4, 4, length = 200), B = seq(.14, 3.9, length = 200))\n\nlogit_pred &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(Class ~ A + B, data = training_set) %&gt;%\n  predict(data_grid, type = \"prob\") %&gt;%\n  bind_cols(data_grid) %&gt;%\n  mutate(link = \"logit\")\n\nprobit_pred &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\", family = binomial(link = \"probit\")) %&gt;%\n  fit(Class ~ A + B, data = training_set) %&gt;%\n  predict(data_grid, type = \"prob\") %&gt;%\n  bind_cols(data_grid) %&gt;%\n  mutate(link = \"probit\")\n\ncloglog_pred &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\", family = binomial(link = \"cloglog\")) %&gt;%\n  fit(Class ~ A + B, data = training_set) %&gt;%\n  predict(data_grid, type = \"prob\") %&gt;%\n  bind_cols(data_grid) %&gt;%\n  mutate(link = \"c-log-log\")\n\nlink_grids &lt;-\n  bind_rows(logit_pred, probit_pred, cloglog_pred) %&gt;%\n  mutate(link = factor(link, levels = c(\"logit\", \"probit\", \"c-log-log\")))\n\nlink_grids %&gt;%\n  ggplot(aes(x = A, y = B)) +\n  geom_point(\n    data = testing_set, aes(color = Class, pch = Class),\n    alpha = 0.7, show.legend = FALSE\n  ) +\n  geom_contour(aes(z = .pred_Class1, lty = link), breaks = 0.5, color = \"black\") +\n  scale_color_manual(values = c(\"#CC6677\", \"#88CCEE\")) +\n  coord_equal() +\n  labs(x = \"Predictor A\", y = \"Predictor B\")\n\n\n\n\n\n\nFigure 4: The linear class boundary fits for three link functions\n\n\n\n\n本练习强调，不同的指标可能会导致在选择调优参数值时做出不同的决策。在这种情况下，一个指标表明这些模型存在一定差异，而另一个指标则显示它们完全没有差异。Thomas 和 Uminsky（2020）对指标优化进行了深入探讨，他们研究了几个问题，包括指标的博弈。他们警告说：当前人工智能方法中，指标优化的不合理有效性是该领域面临的一项根本性挑战，并且由此产生了一个内在矛盾：仅仅优化指标会导致结果远非最优。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#the-consequences-of-poor-parameter-estimates",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#the-consequences-of-poor-parameter-estimates",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "The consequences of poor parameter estimates",
    "text": "The consequences of poor parameter estimates\n许多调优参数会调节模型复杂度的大小，更高的复杂度通常意味着模型在模拟模式时具有更强的可塑性（malleability）。例如，如第8.4.3节所示，在样条函数中增加自由度会提高预测方程的复杂性。虽然当数据中有潜在的复杂模式时，这是一个优势，但它也可能导致对偶然模式的过度解读，而这些偶然模式在新数据中不会重现。过拟合指的是模型过度适应训练数据的情况：它在用于构建模型的数据上表现良好，但在新数据上表现不佳。由于调优模型参数会增加模型的复杂度，不当的选择可能会导致过拟合。\n回顾12.2节中描述的单层神经网络模型。对于分类任务的神经网络，当它只有一个隐藏单元且采用“sigmoidal”激活函数时，实际上就等同于逻辑回归。然而，随着隐藏单元数量的增加，模型的复杂度也会随之提高。事实上，当网络模型使用“sigmoidal”激活单元时，Cybenko（1989）证明，只要有足够多的隐藏单元，该模型就是一个通用函数逼近器。\n我们将神经网络分类模型应用于上一节中的数据，并改变隐藏单元的数量。以ROC曲线下面积作为性能指标，随着添加的隐藏单元数量增多，模型在训练集上的效果会提升。该网络模型会彻底且细致地学习训练集。如果模型根据训练集的ROC值来评判自身，它会倾向于选择大量隐藏单元，以便几乎消除误差。\n第5章和第10章表明，仅对训练集进行重新预测是一种糟糕的模型评估方法。在这里，神经网络会很快开始过度解读它在训练集中看到的模式。请比较 Figure 5 中这三个示例分类边界（基于训练集开发）在训练集和测试集上的叠加情况。\ntwo_class_rec &lt;-\n  recipe(Class ~ ., data = two_class_dat) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nmlp_mod &lt;-\n  mlp(hidden_units = tune(), epochs = 1000) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\nmlp_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(two_class_rec) %&gt;%\n  add_model(mlp_mod)\n\nmlp_res &lt;-\n  tibble(\n    hidden_units = 1:20,\n    train = NA_real_,\n    test = NA_real_,\n    model = vector(mode = \"list\", length = 20)\n  )\n\nfor (i in 1:nrow(mlp_res)) {\n  set.seed(27)\n  tmp_mod &lt;-\n    mlp_wflow %&gt;%\n    finalize_workflow(mlp_res %&gt;% slice(i) %&gt;% select(hidden_units)) %&gt;%\n    fit(training_set)\n  mlp_res$train[i] &lt;-\n    roc_auc_vec(training_set$Class, predict(tmp_mod, training_set, type = \"prob\")$.pred_Class1)\n  mlp_res$test[i] &lt;-\n    roc_auc_vec(testing_set$Class, predict(tmp_mod, testing_set, type = \"prob\")$.pred_Class1)\n  mlp_res$model[[i]] &lt;- tmp_mod\n}\n\nte_plot &lt;-\n  mlp_res %&gt;%\n  slice(c(1, 4, 20)) %&gt;%\n  mutate(\n    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = \"prob\")))\n  ) %&gt;%\n  dplyr::select(hidden_units, probs) %&gt;%\n  unnest(cols = c(probs)) %&gt;%\n  mutate(\n    label = paste(format(hidden_units), \"units\"),\n    label = ifelse(label == \" 1 units\", \" 1 unit\", label)\n  ) %&gt;%\n  ggplot(aes(x = A, y = B)) +\n  geom_point(\n    data = testing_set, aes(color = Class, pch = Class),\n    alpha = 0.5, show.legend = FALSE\n  ) +\n  geom_contour(aes(z = .pred_Class1), breaks = 0.5, color = \"black\") +\n  scale_color_manual(values = c(\"#CC6677\", \"#88CCEE\")) +\n  facet_wrap(~label, nrow = 1) +\n  coord_equal() +\n  ggtitle(\"Test Set\") +\n  labs(x = \"Predictor A\", y = \"Predictor B\")\n\ntr_plot &lt;-\n  mlp_res %&gt;%\n  slice(c(1, 4, 20)) %&gt;%\n  mutate(\n    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = \"prob\")))\n  ) %&gt;%\n  dplyr::select(hidden_units, probs) %&gt;%\n  unnest(cols = c(probs)) %&gt;%\n  mutate(\n    label = paste(format(hidden_units), \"units\"),\n    label = ifelse(label == \" 1 units\", \" 1 unit\", label)\n  ) %&gt;%\n  ggplot(aes(x = A, y = B)) +\n  geom_point(\n    data = training_set, aes(color = Class, pch = Class),\n    alpha = 0.5, show.legend = FALSE\n  ) +\n  geom_contour(aes(z = .pred_Class1), breaks = 0.5, color = \"black\") +\n  scale_color_manual(values = c(\"#CC6677\", \"#88CCEE\")) +\n  facet_wrap(~label, nrow = 1) +\n  coord_equal() +\n  ggtitle(\"Training Set\") +\n  labs(x = \"Predictor A\", y = \"Predictor B\")\n\ntr_plot\nte_plot\n\n\n\n\n\n\n\n\n\n(a) Training set\n\n\n\n\n\n\n\n\n\n\n\n(b) Test set\n\n\n\n\n\n\nFigure 5: Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets.\n\n\n一个隐藏单元的模型因为受限于线性，不能灵活地适应数据。具有四个隐藏单元的模型开始出现过拟合迹象，对于偏离数据主流的值，其边界不切实际。这是由数据右上角第一类中的单个数据点引起的。到20个隐藏单元时，模型开始记住训练集，在这些数据周围创建小的“孤岛”，以最小化重代入错误率。这些模式在测试集中不会重现。最后一个面板最能说明，必须控制调节复杂度的调优参数，才能使模型有效。对于20单元模型，训练集的AUC为0.944，但测试集的值为0.855。\n当我们有两个可以绘图的预测变量时，这种过拟合现象是很明显的。然而，一般来说，我们必须使用定量方法来检测过拟合。可以使用样本外数据来检测模型是否在训练集上过拟合。同样，我们不能使用测试集，而是需要某种形式的重采样，这可能意味着一种迭代方法（例如，10折交叉验证）或单一数据源（例如，验证集）。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#two-general-strategies-for-optimization",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#two-general-strategies-for-optimization",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "Two general strategies for optimization",
    "text": "Two general strategies for optimization\n调优参数优化通常分为两类：网格搜索（grid search）和迭代搜索（iterative search）。\n网格搜索是指我们预先定义一组要评估的参数值。网格搜索涉及的主要选择是如何构建网格以及要评估多少种参数组合。网格搜索通常被认为效率低下，因为随着“维度灾难”的出现，覆盖参数空间所需的网格点数量可能会变得难以处理。这种担忧有一定道理，在流程未优化时尤为明显。第13章对此有更详细的讨论。\n迭代搜索或顺序搜索是指我们基于先前的结果依次发现新的参数组合。几乎任何非线性优化方法都是适用的，在某些情况下，需要一组初始的一个或多个参数组合的结果来启动优化过程。第14章将对迭代搜索进行更详细的讨论。\nFigure 6 展示了两个面板，这两个面板针对具有两个在0到1之间取值的调优参数的情况，演示了这两种方法。在每个面板中，一组等高线显示了参数与结果之间真实的（模拟的）关系。最优结果位于右上角。\n# load(\"E:/Blog/Books/Tidy Modeling with R/Rdata/search_examples.RData\")\nload(\"./Rdata/search_examples.RData\")\n\ngrid_plot &lt;-\n  ggplot(sfd_grid, aes(x = x, y = y)) +\n  geom_point() +\n  lims(x = 0:1, y = 0:1) +\n  labs(x = \"Parameter 1\", y = \"Parameter 2\", title = \"Space-Filling Grid\") +\n  geom_contour(\n    data = grid_contours,\n    aes(z = obj),\n    alpha = .3,\n    bins = 12\n  ) +\n  coord_equal() +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\nsearch_plot &lt;-\n  ggplot(nm_res, aes(x = x, y = y)) +\n  geom_point(size = .7) +\n  lims(x = 0:1, y = 0:1) +\n  labs(x = \"Parameter 1\", y = \"Parameter 2\", title = \"Global Search\") +\n  coord_equal() +\n  geom_contour(\n    data = grid_contours,\n    aes(x = x, y = y, z = obj),\n    alpha = .3,\n    bins = 12\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\ngrid_plot\nsearch_plot\n\n\n\n\n\n\n\n\n\n(a) Space-Filling Grid\n\n\n\n\n\n\n\n\n\n(b) Global Search\n\n\n\n\n\n\nFigure 6: Examples of pre-defined grid tuning and an iterative search method. The lines represent contours of a performance metric; it is best in the upper-right-hand side of the plot.\n\n\nFigure 6 的左面板展示了一种名为空间填充设计（space-filling design）的网格类型。这是一种为覆盖参数空间的设计，其目的是使调优参数组合彼此之间不会过于接近。这种设计的结果中，没有任何点恰好位于真正的最优位置。不过，有一个点处于大致的附近区域，其性能指标结果很可能在最优值的误差范围内。\nFigure 6 的右侧面板展示了一种全局搜索方法（global search method）的结果——Nelder-Mead单纯形法（奥尔森和纳尔逊，1975）。起始点位于参数空间的左下部分。搜索在空间中蜿蜒前行，直到到达最优位置，在那里它会努力尽可能接近数值上的最佳值。这种特定的搜索方法虽然有效，但并不以效率著称；它需要进行大量的函数评估，尤其是在接近最优值的地方。第14章将讨论更高效的搜索算法。\n混合策略也是一种选择，并且效果很好。在初始网格搜索之后，序贯优化可以从最佳网格组合开始。这些策略的示例将在接下来的两章中详细讨论。在继续之前，让我们学习如何使用dials包在tidymodels中处理调优参数对象。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#tuning-parameters-in-tidymodels",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#tuning-parameters-in-tidymodels",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "Tuning Parameters in tidymodels",
    "text": "Tuning Parameters in tidymodels\n在前面的章节中，我们已经讨论了相当多与recipe对象和模型的调优参数相对应的参数。以下这些参数是可以进行调优的：\n\n将社区合并为“其他”类别的阈值（参数名称为threshold），在第8.4.1节中进行了讨论\n自然样条中的自由度数量（deg_free，第8.4.3节）\n在基于树的模型中执行分割所需的数据点数量（min_n，第6.1节）\n惩罚模型中的正则化量（penalty，第6.1节）\n\n由parsnip包构建的模型函数，有两种参数：\n\n主要参数（main arguments）：为了性能而进行优化且在多个引擎中可用的常见参数，它们是模型函数的顶级参数，例如，rand_forest()函数有主要参数trees、min_n和mtry，因为这些参数是最常被指定或优化的。\n特定于引擎（调优参数）的调优参数：这些参数要么很少被优化，要么只针对某些特定引擎。例如，随机森林ranger包有一些其他包不会使用的参数，比如“增益惩罚”——它在树的归纳过程中对预测因子的选择进行正则化，这个参数有助于调节集成中使用的预测因子数量与性能之间的权衡（Wundervald、Parnell和Domijan，2020）。在ranger()中，这个参数的名称是regularization.factor。要通过parsnip模型规格指定一个值，需将其作为补充参数添加到set_engine()中：\n\nrand_forest(trees = 2000, min_n = 10) %&gt;%                   # &lt;- main arguments\n  set_engine(\"ranger\", regularization.factor = 0.5)         # &lt;- engine-specific\n主要参数使用统一的命名系统，以消除不同引擎之间的不一致性，而特定于引擎的参数则不这样做。\n我们如何向tidymodels函数表明哪些参数需要优化？通过为参数赋值tune()来标记需要调整的参数。对于第12.4节中使用的单层神经网络，隐藏单元的数量通过以下方式指定为需要调整的参数：\n\nneural_net_spec &lt;-\n  mlp(hidden_units = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"keras\")\n\ntune()函数不会执行任何特定的参数值；它只返回一个表达式：\n\ntune()\n#&gt; tune()\n\n将这个tune()值嵌入到参数中，会为待优化的参数添加标签。接下来两章中展示的模型调优函数会解析模型和recipe对象，以发现带有标签的参数。这些函数能够自动配置和处理这些参数，因为它们了解这些参数的特性（例如，值的范围等）。\n\nextract_parameter_set_dials(neural_net_spec)\n#&gt; Collection of 1 parameters for tuning\n#&gt; \n#&gt;    identifier         type    object\n#&gt;  hidden_units hidden_units nparam[+]\n#&gt; \n\n结果显示值为nparam[+]，这表明隐藏单元的数量是一个数值参数。\n有一个可选的标识参数，用于将名称与参数相关联。当同一种参数在不同地方进行调优时，这会很有用。例如，对于10.6节中的Ames房产数据，recipe对象使用样条函数对经度和纬度都进行了编码。如果我们想调优这两个样条函数，使其可能具有不同的平滑度，我们会调用两次step_ns()，每个预测变量调用一次。为了使这些参数可识别，标识参数可以采用任何字符串：\n\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\names_rec &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_other(Neighborhood, threshold = tune()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Longitude, deg_free = tune(\"longitude df\")) %&gt;%\n  step_ns(Latitude, deg_free = tune(\"latitude df\"))\n\nrecipes_param &lt;- extract_parameter_set_dials(ames_rec)\nrecipes_param\n#&gt; Collection of 3 parameters for tuning\n#&gt; \n#&gt;    identifier      type    object\n#&gt;     threshold threshold nparam[+]\n#&gt;  longitude df  deg_free nparam[+]\n#&gt;   latitude df  deg_free nparam[+]\n#&gt; \n\n请注意，identifier列和type列对于这两个样条参数来说并不相同。\n当使用工作流将recipe对象和模型结合起来时，两组参数都会显示：\n\nwflow_param &lt;-\n  workflow() %&gt;%\n  add_recipe(ames_rec) %&gt;%\n  add_model(neural_net_spec) %&gt;%\n  extract_parameter_set_dials()\nwflow_param\n#&gt; Collection of 4 parameters for tuning\n#&gt; \n#&gt;    identifier         type    object\n#&gt;  hidden_units hidden_units nparam[+]\n#&gt;     threshold    threshold nparam[+]\n#&gt;  longitude df     deg_free nparam[+]\n#&gt;   latitude df     deg_free nparam[+]\n#&gt; \n\n神经网络极其擅长模拟非线性模式。向这类模型中添加样条项是没有必要的；我们将此模型与recipe对象结合起来只是为了举例说明。\n每个调优参数在dials包中都有一个对应的函数。在绝大多数情况下，该函数与参数参数同名：\n\nhidden_units()\n#&gt; # Hidden Units (quantitative)\n#&gt; Range: [1, 10]\nthreshold()\n#&gt; Threshold (quantitative)\n#&gt; Range: [0, 1]\n\ndeg_free参数是一个反例；自由度的概念会出现在各种各样不同的语境中。当用于样条函数时，有一个专门的dials函数叫做spline_degree()，默认情况下，它会被用于样条函数：\n\nspline_degree()\n#&gt; Spline Degrees of Freedom (quantitative)\n#&gt; Range: [1, 10]\n\ndials包还提供了一个便捷函数，用于提取特定的参数对象：\n\n# identify the parameter using the id value:\nwflow_param %&gt;% extract_parameter_dials(\"threshold\")\n#&gt; Threshold (quantitative)\n#&gt; Range: [0, 0.1]\n\n在参数集中，参数的范围也可以就地更新：\n\nextract_parameter_set_dials(ames_rec) %&gt;%\n  update(threshold = threshold(c(0.8, 1.0)))\n#&gt; Collection of 3 parameters for tuning\n#&gt; \n#&gt;    identifier      type    object\n#&gt;     threshold threshold nparam[+]\n#&gt;  longitude df  deg_free nparam[+]\n#&gt;   latitude df  deg_free nparam[+]\n#&gt; \n\n由extract_parameter_set_dials()创建的参数集会被tidymodels调优函数使用（在需要时）。如果需要修改调优参数对象的默认值，则将修改后的参数集传递给相应的调优函数。\n一些调优参数取决于数据的维度。例如，最近邻的数量必须在1和数据的行数之间。在某些情况下，很容易为可能的取值范围设定合理的默认值。而在其他情况下，参数范围至关重要，不能随意假设。随机森林模型的主要调优参数是每个树节点分裂时随机抽样的预测变量列的数量，通常记为mtry()。在不知道预测变量数量的情况下，该参数范围无法预先配置，需要进行最终确定。\n\nrf_spec &lt;-\n  rand_forest(mtry = tune()) %&gt;%\n  set_engine(\"ranger\", regularization.factor = tune(\"regularization\")) %&gt;%\n  set_mode(\"regression\")\n\nrf_param &lt;- extract_parameter_set_dials(rf_spec)\nrf_param\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;      identifier                  type    object\n#&gt;            mtry                  mtry nparam[?]\n#&gt;  regularization regularization.factor nparam[+]\n#&gt; \n#&gt; Model parameters needing finalization:\n#&gt; # Randomly Selected Predictors ('mtry')\n#&gt; \n#&gt; See `?dials::finalize()` or `?dials::update.parameters()` for more\n#&gt; information.\n\n完整的参数对象在其摘要中带有[+]；[?]值表示可能范围中至少有一端缺失。有两种处理方法。第一种是使用update()，根据你对数据维度的了解添加范围：\n\nrf_param %&gt;%\n  update(mtry = mtry(c(1, 70)))\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;      identifier                  type    object\n#&gt;            mtry                  mtry nparam[+]\n#&gt;  regularization regularization.factor nparam[+]\n#&gt; \n\n然而，如果一个recipe对象附加到使用了添加或删除列的步骤的工作流中，这种方法可能就不奏效了。如果这些步骤不计划进行调整，那么finalize()函数可以执行一次该recipe对象以获取维度：\n\npca_rec &lt;-\n  recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  # Select the square-footage predictors and extract their PCA components:\n  step_normalize(contains(\"SF\")) %&gt;%\n  # Select the number of components needed to capture 95% of\n  # the variance in the predictors.\n  step_pca(contains(\"SF\"), threshold = .95)\n\nupdated_param &lt;-\n  workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(pca_rec) %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  finalize(ames_train)\nupdated_param\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;      identifier                  type    object\n#&gt;            mtry                  mtry nparam[+]\n#&gt;  regularization regularization.factor nparam[+]\n#&gt; \nupdated_param %&gt;% extract_parameter_dials(\"mtry\")\n#&gt; # Randomly Selected Predictors (quantitative)\n#&gt; Range: [1, 74]\n\n当recipe对象准备好后，finalize()函数会学习将mtry的上限设置为74个预测变量。\n此外，extract_parameter_set_dials()的结果将包含特定于引擎的参数（如果有的话）。这些参数的发现方式与主要参数相同，并被包含在参数集中。dials包包含所有可能可调节的特定于引擎的参数的参数函数：\n\nrf_param\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;      identifier                  type    object\n#&gt;            mtry                  mtry nparam[?]\n#&gt;  regularization regularization.factor nparam[+]\n#&gt; \n#&gt; Model parameters needing finalization:\n#&gt; # Randomly Selected Predictors ('mtry')\n#&gt; \n#&gt; See `?dials::finalize()` or `?dials::update.parameters()` for more\n#&gt; information.\nregularization_factor()\n#&gt; Gain Penalization (quantitative)\n#&gt; Range: (0, 1]\n\n最后，有些调优参数最好与转换相关联。一个很好的例子是与许多正则化回归模型相关的惩罚参数。该参数是非负的，通常以对数单位改变其值。主要的dials参数对象表明默认使用转换：\n\npenalty()\n#&gt; Amount of Regularization (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, 0]\n\n了解这一点很重要，尤其是在更改范围时。新的范围值必须采用转换后的单位：\n\n# correct method to have penalty values between 0.1 and 1.0\npenalty(c(-1, 0)) %&gt;%\n  value_sample(1000) %&gt;%\n  summary()\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.1002  0.1796  0.3284  0.4007  0.5914  0.9957\n\n# incorrect:\npenalty(c(0.1, 1.0)) %&gt;%\n  value_sample(1000) %&gt;%\n  summary()\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.261   2.081   3.437   4.151   5.781   9.996\n\n如果需要，可以通过trans参数更改尺度。你可以使用自然单位，但要保持相同的范围：\n\npenalty(trans = NULL, range = 10^c(-10, 0))\n#&gt; Amount of Regularization (quantitative)\n#&gt; Range: [1e-10, 1]",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/12 Model Tuning and the Dangers of Overfitting.html#chapter-summary",
    "title": "12 Model Tuning and the Dangers of Overfitting",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了模型超参数的调优过程，这些超参数无法直接从数据中估计出来。调整这类参数可能会导致过拟合，这通常是因为模型变得过于复杂，所以使用重采样数据集以及适当的评估指标非常重要。确定合适值有两种通用策略，即网格搜索和迭代搜索，我们将在接下来的两章中深入探讨。在tidymodels中，tune()函数用于确定待优化的参数，而dials包中的函数可以提取调优参数对象并与之交互。",
    "crumbs": [
      "12 Model Tuning and the Dangers of Overfitting"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html",
    "title": "13 Grid Search",
    "section": "",
    "text": "在第12章中，我们演示了如何在预处理recipe对象和模型中，使用tune()函数标记待优化的参数。一旦确定了要优化的内容，接下来就要解决如何优化参数的问题。本章将介绍网格搜索（grid search）方法，这种方法可以先验地指定参数的可能值。第14章将继续讨论，介绍迭代搜索（iterative search）方法。\n让我们先来看两种构建网格的主要方法。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#regular-and-nonregular-grids",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#regular-and-nonregular-grids",
    "title": "13 Grid Search",
    "section": "Regular and Nonregular Grids",
    "text": "Regular and Nonregular Grids\n网格主要有两种类型。规则网格（regular grid）将每个参数（及其对应的一组可能值）以阶乘方式组合，也就是说，使用这些集合的所有组合。另外，非规则网格（nonregular grid）是指参数组合并非由一小组点形成的网格。\n在我们更详细地研究每种类型之前，让我们考虑一个示例模型：多层感知器模型（又名单层人工神经网络）。标记为需要调整的参数如下：\n\nhidden_units：隐藏层单元数\nepochs：模型训练中的拟合轮次/迭代次数\npenalty：权重衰减惩罚的量\n\n从历史上看，时期数是通过早停法来确定的；一个单独的验证集根据错误率来决定训练的时长，因为对训练集进行重复预测会导致过拟合。在我们的案例中，使用权重衰减惩罚应该能防止过拟合，而且调整惩罚项和时期数几乎没有坏处。\n使用parsnip包和nnet包拟合分类模型的示例如下：\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine(\"nnet\", trace = 0) %&gt;%\n  set_mode(\"classification\")\n\n参数trace = 0可防止对训练过程进行额外的日志记录。如第12.6节所示，extract_parameter_set_dials()函数能够提取具有未知值的参数集，并设置它们的dials对象：\n\nmlp_param &lt;- extract_parameter_set_dials(mlp_spec)\nmlp_param %&gt;% extract_parameter_dials(\"hidden_units\")\nmlp_param %&gt;% extract_parameter_dials(\"penalty\")\nmlp_param %&gt;% extract_parameter_dials(\"epochs\")\n\n此输出表明参数对象是完整的，并打印了它们的默认范围。这些值将用于演示如何创建不同类型的参数网格。\nRegular grids\n规则网格是不同参数值集合的组合。首先，用户为每个参数创建一组不同的值。每个参数的可能值数量不必相同。tidyr包中的函数crossing()是创建规则网格的一种方法：\n\ncrossing(\n  hidden_units = 1:3,\n  penalty = c(0.0, 0.1),\n  epochs = c(100, 200)\n)\n#&gt; # A tibble: 12 × 3\n#&gt;   hidden_units penalty epochs\n#&gt;          &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1            1     0      100\n#&gt; 2            1     0      200\n#&gt; 3            1     0.1    100\n#&gt; 4            1     0.1    200\n#&gt; 5            2     0      100\n#&gt; 6            2     0      200\n#&gt; # ℹ 6 more rows\n\n参数对象知道参数的范围。dials包包含一组grid_*()函数，这些函数将参数对象作为输入，以生成不同类型的网格。例如：\n\ngrid_regular(mlp_param, levels = 2)\n#&gt; # A tibble: 8 × 3\n#&gt;   hidden_units      penalty epochs\n#&gt;          &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1            1 0.0000000001     10\n#&gt; 2           10 0.0000000001     10\n#&gt; 3            1 1                10\n#&gt; 4           10 1                10\n#&gt; 5            1 0.0000000001   1000\n#&gt; 6           10 0.0000000001   1000\n#&gt; # ℹ 2 more rows\n\nlevels参数用来设定每个参数创建时的水平数量。它也可以接受一个命名的值向量：\n\nmlp_param %&gt;%\n  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))\n#&gt; # A tibble: 12 × 3\n#&gt;   hidden_units      penalty epochs\n#&gt;          &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1            1 0.0000000001     10\n#&gt; 2            5 0.0000000001     10\n#&gt; 3           10 0.0000000001     10\n#&gt; 4            1 1                10\n#&gt; 5            5 1                10\n#&gt; 6           10 1                10\n#&gt; # ℹ 6 more rows\n\n一些创建规则网格的技术不会使用每个参数集的所有可能值，例如部分因子设计（fractional factorial designs，Box, Hunter, and Hunter，2005）。要了解更多信息，请查阅“CRAN Task View”中的实验设计部分。\n规则网格的一个优点是，调优参数与模型指标之间的关系和模式易于理解。这些设计的因子特性使得可以单独检查每个参数，且参数之间的混淆很少。规则网格的一个缺点是，当存在（中）大量的调优参数时，可能在计算上成本较高。但并非所有模型都这样，如下文13.5节所讨论的，有许多模型的调优时间会随着规则网格而减少！\nIrregular grids\n创建非规则网格有几种选择。第一种是在参数范围内进行随机抽样。grid_random()函数会在参数范围内生成独立的均匀随机数。如果参数对象有相关的转换（比如我们对penalty所做的转换），则随机数会在转换后的尺度上生成。让我们为示例神经网络的参数创建一个随机网格：\n\nset.seed(1301)\nmlp_param %&gt;%\n  grid_random(size = 1000) %&gt;% # 'size' is the number of combinations\n  summary()\n#&gt;   hidden_units       penalty              epochs     \n#&gt;  Min.   : 1.000   Min.   :0.0000000   Min.   : 10.0  \n#&gt;  1st Qu.: 3.000   1st Qu.:0.0000000   1st Qu.:265.8  \n#&gt;  Median : 5.000   Median :0.0000061   Median :497.0  \n#&gt;  Mean   : 5.381   Mean   :0.0437435   Mean   :509.5  \n#&gt;  3rd Qu.: 8.000   3rd Qu.:0.0026854   3rd Qu.:761.0  \n#&gt;  Max.   :10.000   Max.   :0.9814405   Max.   :999.0\n\n对于penalty，随机数在对数（以10为底）尺度上是均匀分布的，但网格中的值是以自然单位表示的。\n随机网格的问题在于，对于中小规模的网格，随机值可能会导致参数组合重叠。此外，随机网格需要覆盖整个参数空间，但良好覆盖的可能性是会随着网格值数量的增加而提高。即使对于15个候选点的样本，Figure 1 也显示了我们示例中的多层感知器的点之间存在一些重叠。\n\nlibrary(ggforce)\nset.seed(1302)\nmlp_param %&gt;%\n  # The 'original = FALSE' option keeps penalty in log10 units\n  grid_random(size = 20, original = FALSE) %&gt;%\n  ggplot(aes(x = .panel_x, y = .panel_y)) +\n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +\n  labs(title = \"Random design with 20 candidates\")\n\n\n\n\n\n\nFigure 1: Three tuning parameters with 15 points generated at random\n\n\n\n\n一种好得多的方法是使用一组称为空间填充设计（space-filling designs）的实验设计。虽然不同的设计方法目标略有不同，但它们通常会找到一种能够覆盖参数空间，且点重叠或值冗余的可能性最小的点配置。这类设计的例子包括：\n\n“Latin hypercubes”（McKay, Beckman, and Conover 1979），\n“maximum entropy designs”（Shewry and Wynn 1987），\n“maximum projection designs”（Joseph, Gul, and Ba 2015），\n\n更多见 Santner等人（2003）的综述。\ndials包包含用于Latin hypercubes和maximum entropy designs的函数。与grid_random()一样，主要输入是参数组合的数量和一个参数对象。让我们在 Figure 2 中比较20个候选参数值的随机设计和Latin hypercubes设计。\n\nset.seed(1303)\nmlp_param %&gt;%\n  grid_latin_hypercube(size = 20, original = FALSE) %&gt;%\n  ggplot(aes(x = .panel_x, y = .panel_y)) +\n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +\n  labs(title = \"Latin Hypercube design with 20 candidates\")\n\n\n\n\n\n\nFigure 2: Three tuning parameters with 20 points generated using a space-filling design\n\n\n\n\n尽管并不完美，但这种Latin hypercubes设计能让各点之间的距离更远，从而可以更好地探索超参数空间。\n空间填充设计在表示参数空间方面可能非常有效。tune包使用的默认设计是最大熵设计。这些设计往往能生成能很好地覆盖候选空间的网格，并极大地增加找到良好结果的几率。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#evaluating-the-grid",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#evaluating-the-grid",
    "title": "13 Grid Search",
    "section": "Evaluating the Grid",
    "text": "Evaluating the Grid\n为了选择最佳的调优参数组合，每个候选参数集都使用未用于训练该模型的数据进行评估。重抽样方法或单个验证集很适合用于此目的。这一过程（以及语法）与10.3节中使用tune包的fit_resamples()函数的方法非常相似。\n重抽样后，用户会选择最合适的候选参数集。选择经验上最佳的参数组合，或者偏向模型拟合的其他方面（如简洁性），可能都是合理的。\n在本章和下一章中，我们将使用一个分类数据集来演示模型调优。这些数据来自Hill等人（2007），他们开发了一种用于癌症研究的自动化显微镜实验室工具。该数据集包含对2019个人类乳腺癌细胞的56项成像测量结果。这些预测变量代表了细胞不同部分（例如细胞核、细胞边界等）的形状和强度特征。预测变量之间存在高度的相关性。例如，有几个不同的预测变量用于测量细胞核和细胞边界的大小与形状。此外，许多预测变量各自具有偏斜分布。\n每个细胞都属于两个类别中的一个。由于这是自动化实验室测试的一部分，重点在于预测能力而非推断。\n这些数据包含在modeldata包中。让我们删除一个分析不需要的列（case）：\n\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\n鉴于数据的维度，我们可以使用10折交叉验证来计算性能指标：\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\n由于预测变量之间具有高度的相关性，使用主成分分析（PCA）进行特征提取来消除预测变量的相关性是合理的。下面的流程包含以下步骤：对预测变量进行变换以提高对称性，将它们标准化到同一尺度，然后进行特征提取。需要保留的主成分分析成分数量会与模型参数一起进行调优。虽然得到的主成分分析（PCA）成分在技术上处于相同的尺度，但低阶成分的范围往往比高阶成分更宽。出于这个原因，我们再次进行标准化，以使预测变量具有相同的均值和方差。\n许多预测变量具有偏态分布。由于主成分分析（PCA）是基于方差的，极端值可能会对这些计算产生不利影响。为了解决这个问题，让我们添加一个recipe步骤，为每个预测变量估计“Yeo-Johnson”变换（Yeo和Johnson，2000年）。虽然它最初旨在对结果进行变换，但也可用于估计有助于获得更对称分布的变换。这个step_YeoJohnson()步骤在recipe对象中出现在通过step_normalize()进行初始标准化之前。然后，让我们将这个特征工程recipe对象与我们的神经网络模型mlp_spec结合起来。\n\nmlp_rec &lt;-\n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = tune()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nmlp_wflow &lt;-\n  workflow() %&gt;%\n  add_model(mlp_spec) %&gt;%\n  add_recipe(mlp_rec)\n\n让我们创建一个参数对象mlp_param来调整一些默认范围。我们可以将轮次数量改为更小的范围（50到200轮）。此外，num_comp()的默认范围非常狭窄（1到4个成分）；我们可以将该范围扩大到40个成分，并将最小值设置为0：\n\nmlp_param &lt;-\n  mlp_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(\n    epochs = epochs(c(50, 200)),\n    num_comp = num_comp(c(0, 40))\n  )\n\n\n\n\n\n\n\nTip\n\n\n\n在step_pca()中，使用零个主成分分析（PCA）组件是跳过特征提取的快捷方式。通过这种方式，原始预测变量可以直接与包含主成分分析组件的结果进行比较。\n\n\ntune_grid()函数是进行网格搜索的主要函数。其功能与第10.3节中的fit_resamples()非常相似，但它有一些与网格相关的额外参数：\n\ngrid：一个整数或数据框。当使用整数时，该函数会创建一个空间填充设计，其中包含grid数量的候选参数组合。如果存在特定的参数组合，则使用grid参数将它们传递给该函数。\nparam_info：一个用于定义参数范围的可选参数。当grid为整数时，该参数最为有用。\n\ntune_grid()的其他参数接口与fit_resamples()相同。第一个参数要么是模型，要么是工作流。当提供模型时，第二个参数可以是recipe对象或公式。另一个必需的参数是rsample重抽样对象（例如cell_folds）。下面的调用还传递了一个指标集，以便在重抽样过程中测量ROC曲线下面积。\n首先，让我们评估一个在重采样中包含三个级别的常规网格：\n\nroc_res &lt;- metric_set(roc_auc)\nset.seed(1305)\nmlp_reg_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = mlp_param %&gt;% grid_regular(levels = 3),\n    metrics = roc_res\n  )\nmlp_reg_tune\n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 4\n#&gt;   splits             id     .metrics          .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n#&gt; 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; # ℹ 4 more rows\n\n我们可以使用一些高级便捷函数来理解结果。首先，用于规则网格的autoplot()方法在 Figure 3 中展示了不同调优参数下的性能概况。\n\nautoplot(mlp_reg_tune) +\n  scale_color_viridis_d(direction = -1) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 3: The regular grid results\n\n\n\n\n对于这些数据，惩罚量对ROC曲线下面积的影响最大。 epoch数似乎对性能没有显著影响。当正则化量较低时，隐藏单元数量的变化影响最大（并且会损害性能）。有几种参数配置的性能大致相当，这可以通过函数show_best()看出：\n\nshow_best(mlp_reg_tune) %&gt;% select(-.estimator)\n#&gt; # A tibble: 5 × 9\n#&gt;   hidden_units penalty epochs num_comp .metric  mean     n std_err\n#&gt;          &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1            5       1    125        0 roc_auc 0.894    10 0.00851\n#&gt; 2            5       1    200        0 roc_auc 0.893    10 0.00808\n#&gt; 3            5       1    125       20 roc_auc 0.892    10 0.0104 \n#&gt; 4            5       1     50        0 roc_auc 0.891    10 0.00868\n#&gt; 5           10       1    125       20 roc_auc 0.891    10 0.00867\n#&gt; # ℹ 1 more variable: .config &lt;chr&gt;\n\n根据这些结果，进行另一轮网格搜索是合理的，此次搜索应采用更大的权重衰减惩罚值。\n要使用空间填充设计，既可以给grid参数赋一个整数，也可以使用某个grid_*()函数生成一个数据框。要使用具有20个候选值的最大熵设计来评估相同的范围：\n\nset.seed(1306)\nmlp_sfd_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = 20,\n    # Pass in the parameter object to use the appropriate range:\n    param_info = mlp_param,\n    metrics = roc_res\n  )\nmlp_sfd_tune\n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 4\n#&gt;   splits             id     .metrics          .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n#&gt; 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 4]&gt;\n#&gt; # ℹ 4 more rows\n\nautoplot()方法也适用于这些设计，不过结果的格式会有所不同。Figure 4 是使用autoplot(mlp_sfd_tune)生成的。\n\nautoplot(mlp_sfd_tune)\n\n\n\n\n\n\nFigure 4: The autoplot() method results when used with a space-filling design\n\n\n\n\n这个边际效应图（ Figure 4 ）展示了每个参数与性能指标之间的关系。查看此图表时要注意：由于未使用规则网格，其他调优参数的值可能会影响每个面板。\n惩罚参数在权重衰减量较小时似乎能带来更好的性能。这与常规网格的结果相反。由于每个面板中的每个点都与其他三个调优参数相关联，因此一个面板中的趋势可能会受到其他面板的影响。使用常规网格时，每个面板中的每个点都会在其他参数上进行均等平均。因此，使用常规网格能更好地分离每个参数的影响。\n与常规网格一样，show_best()可以报告数值上最佳的结果：\n\nshow_best(mlp_sfd_tune) %&gt;% select(-.estimator)\n#&gt; # A tibble: 5 × 9\n#&gt;   hidden_units     penalty epochs num_comp .metric  mean     n std_err\n#&gt;          &lt;int&gt;       &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1            4 0.0264         184        8 roc_auc 0.887    10 0.00928\n#&gt; 2            6 0.298           73       12 roc_auc 0.885    10 0.0107 \n#&gt; 3            7 1              152       29 roc_auc 0.884    10 0.00764\n#&gt; 4            8 0.000000483     57        6 roc_auc 0.875    10 0.00774\n#&gt; 5            2 0.00000546      50       18 roc_auc 0.871    10 0.00931\n#&gt; # ℹ 1 more variable: .config &lt;chr&gt;\n\n一般来说，通过多个指标对模型进行评估是个不错的主意，这样可以考虑到模型拟合的不同方面。此外，选择一个与更简单模型相关联的略次优的参数组合往往是合理的。对于这个模型而言，简单性对应的是更大的惩罚值和/或更少的隐藏单元。\n与fit_resamples()的结果一样，通常没有必要保留不同重抽样和调优参数下的中间模型拟合结果。不过，和之前一样，control_grid()的extract选项允许保留拟合好的模型和/或recipe对象。此外，将save_pred选项设置为TRUE会保留评估集的预测结果，这些结果可以通过collect_predictions()进行访问。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#finalizing-the-model",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#finalizing-the-model",
    "title": "13 Grid Search",
    "section": "Finalizing the Model",
    "text": "Finalizing the Model\n如果通过show_best()找到的某组可能的模型参数对于这些数据来说是一个有吸引力的最终选项，我们可能希望评估它在测试集上的表现。然而，tune_grid()的结果仅为选择合适的调优参数提供了基础。该函数不会拟合最终模型。\n要拟合最终模型，必须确定一组最终的参数值。有两种方法可以做到这一点：\n\n手动选择看起来合适的值，或者\n使用select_*()函数。\n\n例如，select_best()会选择在数值上结果最佳的参数。让我们回到常规网格的结果，看看哪一个是最佳的：\n\nselect_best(mlp_reg_tune, metric = \"roc_auc\")\n#&gt; # A tibble: 1 × 5\n#&gt;   hidden_units penalty epochs num_comp .config         \n#&gt;          &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;           \n#&gt; 1            5       1    125        0 pre1_mod17_post0\n\n回顾 Figure 3 ，我们可以看到，一个具有单个隐藏单元、在原始预测变量上训练125个epochs且带有大量惩罚项的模型，其性能与该选项相当，而且更简单。这本质上就是有惩罚项的逻辑回归！要手动指定这些参数，我们可以创建一个包含这些值的tibble，然后使用最终确定函数将这些值拼接回工作流中：\n\nlogistic_param &lt;-\n  tibble(\n    num_comp = 0,\n    epochs = 125,\n    hidden_units = 1,\n    penalty = 1\n  )\n\nfinal_mlp_wflow &lt;-\n  mlp_wflow %&gt;%\n  finalize_workflow(logistic_param)\nfinal_mlp_wflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: mlp()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; • step_YeoJohnson()\n#&gt; • step_normalize()\n#&gt; • step_pca()\n#&gt; • step_normalize()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Single Layer Neural Network Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   hidden_units = 1\n#&gt;   penalty = 1\n#&gt;   epochs = 125\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   trace = 0\n#&gt; \n#&gt; Computational engine: nnet\n\n在此最终确定的工作流程中，不再包含tune()的更多值。现在可以将模型拟合到整个训练集：\n\nfinal_mlp_fit &lt;-\n  final_mlp_wflow %&gt;%\n  fit(cells)\n\n现在可以使用这个对象对新数据进行未来预测。\n如果您没有使用工作流，则模型和/或recipe对象的最终确定是通过finalize_model()和finalize_recipe()来完成的。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#tools-for-creating-tuning-specifications",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#tools-for-creating-tuning-specifications",
    "title": "13 Grid Search",
    "section": "Tools for Creating Tuning Specifications",
    "text": "Tools for Creating Tuning Specifications\nusemodels包可以接收一个数据框和模型公式，然后生成用于模型调优的R代码。该代码还会创建一个合适的recipe对象，其步骤取决于所请求的模型以及预测变量数据。\n例如，对于Ames房价数据，可以使用以下代码创建xgboost建模代码：\nlibrary(usemodels)\n\nuse_xgboost(\n  Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude,\n  data = ames_train,\n  # Add comments explaining some of the code:\n  verbose = TRUE\n)\n生成的代码如下：\nxgboost_recipe &lt;-\n  recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  ## This model requires the predictors to be numeric. The most common\n  ## method to convert qualitative predictors to numeric is to create\n  ## binary indicator variables (aka dummy variables) from these\n  ## predictors. However, for this model, binary indicator variables can be\n  ## made for each of the levels of the factors (known as 'one-hot\n  ## encoding').\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\nxgboost_spec &lt;-\n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),\n    loss_reduction = tune(), sample_size = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"xgboost\")\n\nxgboost_workflow &lt;-\n  workflow() %&gt;%\n  add_recipe(xgboost_recipe) %&gt;%\n  add_model(xgboost_spec)\n\nset.seed(69305)\nxgboost_tune &lt;-\n  tune_grid(xgboost_workflow,\n            resamples = stop(\"add your rsample object\"),\n            grid = stop(\"add number of candidate points\"))\n根据usemodels对数据的理解，这段代码是所需的最低限度预处理。对于其他模型，会添加像step_normalize()这样的操作来满足模型的基本需求。需要注意的是，作为建模从业者，选择用于调优的resamples以及grid类型是我们的责任。\n通过将参数tune = FALSE进行设置，usemodels包还可用于创建无需调优的模型拟合代码。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#tools-for-efficient-grid-search",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#tools-for-efficient-grid-search",
    "title": "13 Grid Search",
    "section": "Tools for Efficient Grid Search",
    "text": "Tools for Efficient Grid Search\n通过应用一些不同的技巧和优化方法，可以提高网格搜索的计算效率。本节将介绍几种技术。\nSubmodel optimization\n有些模型，只需一次模型拟合，就可以在不重新拟合的情况下评估多个调优参数。\n例如，偏最小二乘（PLS）是主成分分析的一种有监督版本（Geladi 和 Kowalski，1986）。它会创建能最大化预测变量变异的成分（类似主成分分析），但同时还会尝试最大化这些预测变量与结果之间的相关性。我们将在第16章更深入地探讨偏最小二乘。其中一个调优参数是要保留的偏最小二乘成分数量。假设使用偏最小二乘对一个包含100个预测变量的数据集进行拟合，可能保留的成分数量范围可以是1到50。不过，在许多实现中，单次模型拟合就可以计算多个num_comp值对应的预测值。因此，一个包含100个成分的偏最小二乘模型也能对任何num_comp &lt;= 100的情况进行预测。这节省了时间，因为无需进行冗余的模型拟合，一次拟合就可以用于评估多个子模型。\n虽然并非所有模型都能利用这一特性，但许多被广泛使用的模型可以：\n\nBoosting模型通常能够针对多个不同的Boosting迭代次数进行预测。\n正则化方法（例如glmnet模型）能够基于拟合模型时所使用的不同正则化量，同时进行预测。\n多元自适应回归样条（MARS）为线性回归模型添加了一组非线性特征（Friedman，1991）。需要保留的项数是一个调优参数，而且从单个模型拟合中就可以针对该参数的多个值快速进行预测计算。\n\n每当对适用的模型进行调优时，tune包会自动应用这种类型的优化。例如，如果将增强型 “C5.0”分类模型（M. Kuhn and Johnson，2013）应用于细胞数据，我们可以调整增强迭代的次数（trees）。在所有其他参数设为默认值的情况下，我们可以在与之前相同的重采样上评估1到100次的迭代：\nc5_spec &lt;-\n  boost_tree(trees = tune()) %&gt;%\n  set_engine(\"C5.0\") %&gt;%\n  set_mode(\"classification\")\n\nset.seed(1307)\nc5_spec %&gt;%\n  tune_grid(\n    class ~ .,\n    resamples = cell_folds,\n    grid = data.frame(trees = 1:100),\n    metrics = roc_res\n  )\n如果不进行子模型优化，调用tune_grid()来重采样100个子模型需要62.2分钟。而进行优化后，相同的调用仅耗时100秒（速度提升了37倍）。时间的减少源于tune_grid()所拟合的模型数量从1000个变为了10个。\n尽管我们在拟合模型时使用和不使用子模型预测技巧，但这种优化是由parsnip自动应用的。\nParallel processing\n如第10.4节先前所提及的，在对模型进行重抽样时，并行处理是减少执行时间的有效方法。这一优势也体现在通过网格搜索进行模型调优的过程中，不过还存在一些额外的注意事项。\n让我们来考虑两种不同的并行处理方案。\n在通过网格搜索调整模型时，存在两个不同的循环：一个是针对重采样的循环，另一个是针对独特的调整参数组合的循环。用伪代码表示，这个过程如下：\nfor (rs in resamples) {\n  # Create analysis and assessment sets\n  # Preprocess data (e.g. formula or recipe)\n  for (mod in configurations) {\n    # Fit model {mod} to the {rs} analysis set\n    # Predict the {rs} assessment set\n  }\n}\n默认情况下，tune包仅对重采样（外循环）进行并行处理，而不是同时对内外循环进行并行处理。\n当预处理方法成本较高时，这是最佳方案。然而，这种方法存在两个潜在的缺点：\n\n当预处理成本不高时，这会限制可实现的速度提升。\n并行工作进程的数量受重采样数量的限制。例如，在10折交叉验证中，即使计算机的核心数超过10个，也只能使用10个并行工作进程。\n\n为了说明并行处理是如何工作的，我们将使用一个包含7个模型调优参数值且采用5折交叉验证的案例。Figure 5 展示了任务是如何分配给工作进程的。\n\n\n\n\n\n\n\nFigure 5: Worker processes when parallel processing matches resamples to a specific worker process\n\n\n\n\n请注意，每个折都被分配给它自己的工作进程，而且由于只对模型参数进行调优，所以每个折/工作进程会执行一次预处理。如果使用的工作进程少于五个，那么一些工作进程会接收多个折。\n在tune_*()函数的控制函数中，parallel_over参数控制着流程的执行方式。要使用之前的并行化策略，该参数应设置为parallel_over = \"resamples\"。\n不采用并行处理重采样的方式，另一种方案将重采样和模型上的循环合并为一个单一循环。用伪代码表示，这个过程如下：\nall_tasks &lt;- crossing(resamples, configurations)\n\nfor (iter in all_tasks) {\n  # Create analysis and assessment sets for {iter}\n  # Preprocess data (e.g. formula or recipe)\n  # Fit model {iter} to the {iter} analysis set\n  # Predict the {iter} assessment set\n}\n在这种情况下，并行化现在在单个循环上进行。例如，如果我们使用带有 \\(M\\) 个调优参数值的5折交叉验证，那么这个循环将执行 \\(5×M\\) 次迭代。这增加了可使用的潜在工作进程数量。然而，与数据预处理相关的工作会被重复多次。如果这些步骤的成本很高，这种方法就会效率低下。在tidymodels中，验证集被视为单个重采样，这种情况下，这种并行化方案将是最佳的。\nFigure 6 展示了此方案向工作进程分配任务的情况；使用了相同的示例，但有10个工作进程。\n\n\n\n\n\n\n\nFigure 6: Worker processes when preprocessing and modeling tasks are distributed to many workers\n\n\n\n\n在这里，每个工作进程处理多个折数，并且预处理被不必要地重复进行。例如，对于第一个折数，预处理被计算了七次，而不是一次。\n对于这种方案，控制函数的参数是parallel_over = \"everything\"。\nBenchmarking boosted trees\n为了比较不同可能的并行化方案，我们使用xgboost引擎，在包含4000个样本的数据集上对一个提升树进行了调优，采用了5折交叉验证和10个候选模型。这些数据需要一些不需要任何估计的基线预处理。预处理通过三种不同的方式进行：\n\n在建模前使用dplyr管道对数据进行预处理（标记为“none”）。\n通过一个流程执行相同的预处理（标记为“light”）。\n使用recipe对象时，添加一个计算成本高的额外步骤（标记为“expensive”）。\n\n第一种和第二种预处理选项旨在进行比较，以衡量第二种选项中recipe对象的计算成本。第三种选项衡量在parallel_over = \"everything\"的情况下执行冗余计算的成本。\n我们在一台拥有10个物理核心和20个虚拟核心（通过超线程实现）的计算机上，使用不同数量的工作进程以及两种parallel_over选项对这一过程进行了评估。由于只有五次重采样，所以当parallel_over = \"resamples\"时，使用的核心数量被限制为五个。\n结果如 Figure 7 所示：\n\n\n\n\n\n\n\nFigure 7: Execution times for model tuning versus the number of workers using different delegation schemes\n\n\n\n\n比较前两个面板中“none”和“light”的曲线：\n\n这些面板之间的执行时间几乎没有差异。这表明，对于这些数据而言，在recipe对象中执行预处理步骤并不会带来实际的计算代价。\n使用parallel_over = \"everything\"搭配多个核心会有一些好处。然而，如图所示，并行处理的大部分好处出现在前五个工作进程中。\n\n由于存在昂贵的预处理步骤，执行时间存在显著差异。使用parallel_over = \"everything\"会产生问题，因为即使使用所有内核，其执行时间也始终无法达到parallel_over = \"resamples\"仅用五个内核时所达到的水平。这是因为在计算方案中，昂贵的预处理步骤被不必要地重复执行了。\n我们也可以在 Figure 8 中从加速比的角度查看这些数据。\n\n\n\n\n\n\n\nFigure 8: Speed-ups for model tuning versus the number of workers using different delegation schemes. The diagonal black line indicates a linear speedup where the addition of a new worker process has maximal effect.\n\n\n\n\n对于这些数据，最佳的加速效果出现在parallel_over = \"resamples\"且计算量较大的情况下。然而，在后一种情况下，请记住，先前的分析表明整体模型拟合速度会更慢。\n将子模型优化方法与并行处理结合使用有什么好处？第13.5.1节中展示的C5.0分类模型也使用10个工作进程进行了并行运行。并行计算耗时13.3秒，实现了7.5倍的速度提升（两次运行都使用了子模型优化技巧）。与最基础的网格搜索代码相比，子模型优化技巧和并行处理相结合总共实现了282倍的速度提升。\n总的来说，要注意的是，计算节省的增加量会因模型而异，并且还会受到网格大小、重采样次数等因素的影响。一个计算效率非常高的模型可能从并行处理中获益不多。\nAccess to global variables\n使用tidymodels时，可以在模型对象中使用本地环境（通常是全局环境）中的值。这里所说的“环境”是什么意思呢？可以把R语言中的环境看作是一个存储你可以操作的变量的地方。想要了解更多信息，请参阅 Advanced R 中“Environment”章节。\n如果我们定义一个变量用作模型参数，然后将其传递给像linear_reg()这样的函数，该变量通常是在全局环境中定义的。\n\ncoef_penalty &lt;- 0.1\nspec &lt;- linear_reg(penalty = coef_penalty) %&gt;% set_engine(\"glmnet\")\nspec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   penalty = coef_penalty\n#&gt; \n#&gt; Computational engine: glmnet\n\n使用parsnip包创建的模型会将诸如此类的参数保存为准引用（quosures）；这些对象既会记录对象的名称，也会记录其所在的环境：\n\nspec$args$penalty\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^coef_penalty\n#&gt; env:  global\n\n注意，结果有env: global，因为这个变量是在全局环境中创建的。由spec定义的模型规范在用户的常规会话中运行时能正常工作，因为该会话也在使用全局环境；R可以轻松找到coef_penalty对象。当使用并行工作进程评估此类模型时，它可能会失败。这取决于用于并行处理的特定技术，工作进程可能无法访问全局环境。\n在编写将并行运行的代码时，将实际数据插入到对象中而非对象的引用，是个不错的主意。rlang和dplyr包在这方面会很有帮助。例如，!!运算符可以将单个值拼接进一个对象：\n\nspec &lt;- linear_reg(penalty = !!coef_penalty) %&gt;% set_engine(\"glmnet\")\nspec$args$penalty\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^0.1\n#&gt; env:  empty\n\n现在输出结果是^0.1，这表明存在的是值而非对对象的引用。当你有多个外部值要插入到一个对象中时，!!!运算符会有所帮助：\n\nmcmc_args &lt;- list(chains = 3, iter = 1000, cores = 3)\n\nlinear_reg() %&gt;% set_engine(\"stan\", !!!mcmc_args)\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   chains = 3\n#&gt;   iter = 1000\n#&gt;   cores = 3\n#&gt; \n#&gt; Computational engine: stan\n\nrecipe对象选择器是另一个你可能希望访问全局变量的地方。假设你有一个recipe对象步骤，该步骤应该使用细胞数据中所有通过第二个光学通道测量的预测变量。我们可以创建这些列名的向量：\n\nlibrary(stringr)\nch_2_vars &lt;- str_subset(names(cells), \"ch_2\")\nch_2_vars\n#&gt; [1] \"avg_inten_ch_2\"   \"total_inten_ch_2\"\n\n我们可以将这些硬编码到一个recipe对象步骤中，但更好的做法是通过编程方式引用它们，以防数据发生变化。有两种实现方法：\n\n# Still uses a reference to global data (~_~;)\nrecipe(class ~ ., data = cells) %&gt;%\n  step_spatialsign(all_of(ch_2_vars))\n\n# Inserts the values into the step ヽ(•‿•)ノ\nrecipe(class ~ ., data = cells) %&gt;%\n  step_spatialsign(!!!ch_2_vars)\n\n后者更适合并行处理，因为所有所需信息都嵌入在recipe对象对象中。\nRacing methods\n网格搜索的一个问题是，所有模型都需要在所有重采样上进行拟合，之后才能评估任何调优参数。相反，如果能在调优过程中的某个时刻进行中期分析，以排除任何确实糟糕的参数候选者，那将会很有帮助。这类似于临床试验中的无效性分析。如果一种新药的表现极差（或极好），等到试验结束再做决定可能是不道德的。\n在机器学习中，被称为竞赛方法（racing methods）的一系列技术具有类似的功能（Maron and Moore，1994）。在这里，调优过程会在重抽样的初始子集上评估所有模型。根据它们当前的性能指标，某些参数集不会在后续的重抽样中被考虑。\n例如，在本章探讨的使用规则网格的多层感知器调优过程中，仅经过前三个折后的结果会是什么样子？使用与第11章所示类似的技术，我们可以拟合一个模型，其中结果是重采样的ROC曲线下面积，而预测变量是参数组合的指示器。该模型考虑了重采样之间的效应，并为每个参数设置生成点估计和区间估计。模型的结果是单侧95%置信区间，用于衡量相对于当前表现最佳参数的ROC值损失，如 Figure 9 所示。\n\n\n\n\n\n\n\nFigure 9: The racing process for 20 tuning parameters and 10 resamples\n\n\n\n\n任何置信区间包含零的参数组，都缺乏证据表明其性能与最佳结果存在统计学上的差异。我们保留了6种设置，对这些设置进行更多的重抽样。其余14个子模型不再被考虑。\n该过程会在每个重采样中持续进行；在获取下一组性能指标后，会根据这些统计数据拟合一个新模型，并且可能会舍弃更多的子模型。\n只要中期分析速度快且某些参数设置性能较差，竞赛方法就可能比基本的网格搜索更高效。当模型不具备利用子模型预测的能力时，这种方法也最有帮助。\nfinetune包包含用于竞赛的函数。tune_race_anova()函数通过ANOVA模型来检验不同模型配置的统计显著性。重现前面所示筛选过程的语法如下：\n\nlibrary(finetune)\n\nset.seed(1309)\nmlp_sfd_race &lt;-\n  mlp_wflow %&gt;%\n  tune_race_anova(\n    cell_folds,\n    grid = 20,\n    param_info = mlp_param,\n    metrics = roc_res,\n    control = control_race(verbose_elim = TRUE)\n  )\n\n这些参数与tune_grid()的参数相似。control_race()函数具有用于消除过程的选项。\n如上面的动画所示，在评估完完整的重采样集后，有两种调优参数组合在考虑范围内。show_best()返回表现最佳的模型（按性能排名），但只返回从未被淘汰的配置：\n\nshow_best(mlp_sfd_race, n = 10)\n#&gt; # A tibble: 4 × 10\n#&gt;   hidden_units penalty epochs num_comp .metric .estimator  mean     n std_err\n#&gt;          &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1            4 2.64e-2    184        8 roc_auc binary     0.882    10 0.00988\n#&gt; 2            7 1   e+0    152       29 roc_auc binary     0.882    10 0.00937\n#&gt; 3            6 2.98e-1     73       12 roc_auc binary     0.881    10 0.0103 \n#&gt; 4            8 4.83e-7     57        6 roc_auc binary     0.880    10 0.00940\n#&gt; # ℹ 1 more variable: .config &lt;chr&gt;\n\n还有其他用于舍弃设置的中期分析技术。例如， Krueger, Panknin, and Braun（2015）使用传统的序贯分析方法，而 Max Kuhn（2014）则将数据视为体育比赛，并使用“Bradley-Terry”模型（Bradley and Terry，1952）来衡量参数设置的获胜能力。",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/13 Grid Search.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/13 Grid Search.html#chapter-summary",
    "title": "13 Grid Search",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章讨论了可用于模型调优的网格搜索的两个主要类别（常规和非常规），并演示了如何构建这些网格，既可以手动构建，也可以使用grid_*()函数族。tune_grid()函数可以通过重抽样来评估这些候选模型参数集。本章还展示了如何确定模型、recipe对象或工作流的最终版本，以更新最终拟合的参数值。网格搜索可能计算成本很高，但在这类搜索的实验设计中做出深思熟虑的选择可以使其变得易于处理。\n下一章将重用的数据分析代码如下：\nlibrary(tidymodels)\n\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\nroc_res &lt;- metric_set(roc_auc)",
    "crumbs": [
      "13 Grid Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/14 Iterative Search.html",
    "href": "Books/Tidy Modeling with R/14 Iterative Search.html",
    "title": "14 Iterative Search",
    "section": "",
    "text": "第13章展示了网格搜索如何采用一组预定义的候选值，对它们进行评估，然后选择最佳设置。迭代搜索方法则采用不同的策略。在搜索过程中，它们会预测接下来要测试哪些值。当网格搜索不可行或效率低下时，迭代方法是优化调优参数的合理选择。\n本章概述了两种搜索方法：\n为了便于说明，我们使用与前一章相同的细胞特征数据，但更换了模型。本章采用支持向量机模型，因为它能对搜索过程提供清晰的二维可视化效果。",
    "crumbs": [
      "14 Iterative Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/14 Iterative Search.html#a-support-vector-machine-model",
    "href": "Books/Tidy Modeling with R/14 Iterative Search.html#a-support-vector-machine-model",
    "title": "14 Iterative Search",
    "section": "A Support Vector Machine Model",
    "text": "A Support Vector Machine Model\n我们再次使用第13.2节中描述的细胞分割数据进行建模，并使用支持向量机（SVM）模型来演示迭代搜索方法。有关SVM模型的更多信息，请参见 M. Kuhn和Johnson（2013）。需要优化的两个调优参数是SVM的成本值和径向基函数核参数 \\(\\sigma\\)。这两个参数都会对模型的复杂性和性能产生深远影响。\n支持向量机（SVM）模型会用到点积，因此有必要对预测变量进行中心化和缩放处理。与多层感知器模型类似，该模型也能从主成分分析（PCA）特征提取中获益。不过，本章不会使用这第三个调优参数，以便我们能在二维空间中可视化搜索过程。\n除了之前使用的对象（见第13.6节），还需要svm_rec、svm_spec和svm_wflow等tidymodels对象：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\n\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\nroc_res &lt;- metric_set(roc_auc)\n\nsvm_rec &lt;-\n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nsvm_spec &lt;-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\nsvm_wflow &lt;-\n  workflow() %&gt;%\n  add_model(svm_spec) %&gt;%\n  add_recipe(svm_rec)\n\n两个调优参数cost和rbf_sigma的默认参数范围是\n\ncost()\n#&gt; Cost (quantitative)\n#&gt; Transformer: log-2 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, 5]\nrbf_sigma()\n#&gt; Radial Basis Function sigma (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, 0]\n\n为了说明，让我们稍微改变核参数的范围，以改进搜索的可视化效果：\n\nsvm_param &lt;-\n  svm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(rbf_sigma = rbf_sigma(c(-7, -1)))\n\n在讨论迭代搜索的具体细节及其工作原理之前，让我们探究这个特定数据集的两个支持向量机（SVM）调优参数与ROC曲线下面积之间的关系。我们构建了一个非常大的规则网格，由2500个候选值组成，并通过重采样对该网格进行了评估。显然，这在常规数据分析中是不切实际的，而且效率极低。然而，它阐明了搜索过程应遵循的路径以及数值最优值出现的位置。\nFigure 1 展示了对该网格的评估结果，其中颜色越浅表示模型性能越高（越好）。参数空间的左下对角线区域有一大片相对平缓的区域，性能较差。性能最佳的脊线出现在该空间的右上部分。黑点表示最佳设置。从性能不佳的平台区域到性能最佳的脊线区域的过渡非常陡峭。在脊线右侧紧邻的位置，ROC曲线下面积也出现了急剧下降。\n\n\n\n\n\nFigure 1: Heatmap of the mean area under the ROC curve for a high density grid of tuning parameter values. The best point is a solid dot in the upper-right corner.\n\n\n以下搜索过程在继续之前至少需要一些重采样的性能统计数据。为此，下面的代码创建了一个位于参数空间平坦部分的小型规则网格。tune_grid()函数对该网格进行重采样：\n\nset.seed(1401)\nstart_grid &lt;-\n  svm_param %&gt;%\n  update(\n    cost = cost(c(-6, 1)),\n    rbf_sigma = rbf_sigma(c(-6, -4))\n  ) %&gt;%\n  grid_regular(levels = 2)\n\nset.seed(1402)\nsvm_initial &lt;-\n  svm_wflow %&gt;%\n  tune_grid(resamples = cell_folds, grid = start_grid, metrics = roc_res)\n#&gt; maximum number of iterations reached 0.01985804 0.01947119maximum number of iterations reached 0.01951676 0.01912962maximum number of iterations reached 0.01929118 0.01895888maximum number of iterations reached 0.01938124 0.01905198maximum number of iterations reached 0.01967913 0.01930207maximum number of iterations reached 0.02022452 0.01983573maximum number of iterations reached 0.0194954 0.01916201maximum number of iterations reached 0.0204274 0.01999696maximum number of iterations reached 0.01870142 0.01838042maximum number of iterations reached 0.01978898 0.01941516\n\ncollect_metrics(svm_initial)\n#&gt; # A tibble: 4 × 8\n#&gt;     cost rbf_sigma .metric .estimator  mean     n std_err .config        \n#&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 0.0156  0.000001 roc_auc binary     0.864    10 0.00864 pre0_mod1_post0\n#&gt; 2 0.0156  0.0001   roc_auc binary     0.863    10 0.00862 pre0_mod2_post0\n#&gt; 3 2       0.000001 roc_auc binary     0.863    10 0.00867 pre0_mod3_post0\n#&gt; 4 2       0.0001   roc_auc binary     0.866    10 0.00855 pre0_mod4_post0\n\n这个初始网格显示出相当相近的结果，没有任何单个点比其他点好很多。这些结果可以被下一节讨论的迭代调优函数接收，用作初始值。",
    "crumbs": [
      "14 Iterative Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/14 Iterative Search.html#bayesian-optimization",
    "href": "Books/Tidy Modeling with R/14 Iterative Search.html#bayesian-optimization",
    "title": "14 Iterative Search",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\n贝叶斯优化技术会分析当前的重采样结果，并创建一个预测模型，以推荐尚未评估的调参参数值。然后对推荐的参数组合进行重采样。这些结果随后会被用于另一个预测模型，该模型会推荐更多供测试的候选值，依此类推。这个过程会进行设定好的迭代次数，或者直到不再有进一步的改进为止。Shahriari等人（2016）和Frazier（2018）对贝叶斯优化做了很好的介绍。\n在使用贝叶斯优化时，主要关注点是如何创建模型以及如何选择该模型推荐的参数。首先，让我们考虑贝叶斯优化中最常用的技术——高斯过程模型。\nA Gaussian process model\n高斯过程（GP）（Schulz，Speekenbrink，and Krause，2018）模型是著名的统计技术，在空间统计学中有着悠久的历史（以“kriging methods”最为出名），它可以通过多种方式（包括贝叶斯模型）推导得出，详见 Rasmussen 和 Williams（2006）的参考文献。\n从数学角度来讲，高斯过程（GP）是一组随机变量的集合，其联合概率分布为多元高斯分布。在我们的应用场景中，这组随机变量是调优参数候选值对应的性能指标集合。之前的初始网格就提供了四个样本，观测值分别为0.8639, 0.8627, 0.8625, 0.8659。这些值被假设服从多元高斯分布。定义高斯过程模型中自变量（预测变量）的输入是相应的调优参数值（如 Table 1 所示）。\n\n\n\nTable 1: Resampling statistics used as the initial substrate to the Gaussian process model.\n\n\n\n\n\n\n\n\n\n\n\noutcome\n\n\npredictors\n\n\n\nROC\ncost\nrbf_sigma\n\n\n\n\n0.8639\n0.01562\n0.000001\n\n\n0.8627\n0.01562\n0.000100\n\n\n0.8625\n2.00000\n0.000001\n\n\n0.8659\n2.00000\n0.000100\n\n\n\n\n\n\n\n\n高斯过程模型由其均值函数和协方差函数来定义，不过后者对高斯过程模型的性质影响更大。协方差函数通常根据输入值（记为 \\(x\\) ）进行参数化。例如，一种常用的协方差函数是平方指数函数：\n\\[\ncov(x_i, x_j) = \\exp\\left(-\\frac{1}{2}|x_i - x_j|^2\\right) + \\sigma^2_{ij}\n\\]\n其中 \\(\\sigma^2_{ij}\\) 是一个恒定的误差方差项，当 \\(i = j\\) 时，该误差方差项为零。这个方程可转化为：随着两个调优参数组合之间距离的增加，性能指标之间的协方差呈指数增长。该方程的性质还意味着，结果指标的变异在已观测到的点处达到最小（即当 \\(|x_i−x_j|^2\\) 为零时）。这种协方差函数的特性使得高斯过程即便在只有少量数据的情况下，也能够表征模型性能与调优参数之间高度非线性的关系。然而，在某些情况下，拟合这些模型可能会很困难，而且随着调优参数组合数量的增加，模型的计算成本会变得更高。\n该模型的一个重要优点是，由于指定了完整的概率模型，对新输入的预测能够反映结果的整个分布。换句话说，新的性能统计数据可以从均值和方差两方面进行预测。假设正在考虑两个新的调优参数。在 Table 2 中，候选参数A的平均ROC值略高于候选参数B（当前最佳值为0.8659）。然而，其方差是B的四倍。这是好还是坏呢？选择选项A风险更高，但潜在回报也可能更高。方差的增加还反映出，这个新值与现有数据的距离比B更远。下一节将更详细地探讨高斯过程（GP）预测在贝叶斯优化中的这些方面。\n\n\n\nTable 2: Two example tuning parameters considered for further sampling.\n\n\n\n\n\n\n\n\n\n\n\n\nGP Prediction of ROC AUC\n\n\n\ncandidate\nmean\nvariance\n\n\n\n\nA\n0.90\n0.000400\n\n\nB\n0.89\n0.000025\n\n\n\n\n\n\n\n\n贝叶斯优化是一个迭代过程。基于四个结果的初始网格，拟合GP模型，预测候选参数，并选择第五个调优参数组合。我们计算新配置的性能估计值，然后利用这五个现有结果重新拟合GP模型（以此类推）。\nAcquisition functions\n一旦高斯过程拟合了当前数据，它将如何被使用呢？我们的目标是选择下一个最有可能比当前最佳结果“更好”的调优参数组合。实现这一目标的一种方法是创建一个大型候选集（或许可以采用空间填充设计），然后对每个候选参数组合进行均值和方差预测。利用这些信息，我们就能选出最有利的调优参数值。\n一类被称为 Acquisition 函数的目标函数有助于平衡均值和方差。回想一下，高斯过程模型的预测方差主要由它们与现有数据的距离决定。新候选点的预测均值和方差之间的权衡通常从探索和利用的角度来看待：\n\n探索会使选择偏向于观测到的候选模型较少（如果有的话）的区域。这往往会给方差较高的候选模型赋予更大的权重，并侧重于发现新的结果。\n利用主要依靠均值预测来找到最佳（均值）值。它侧重于已有的结果。\n\n为了说明这一点，让我们来看一个简单的例子，其中有一个参数，其值在[0, 1]之间，性能指标是 \\(R^2\\)。真实函数如 Figure 2 所示，同时还有五个具有现有结果的候选值（以点表示）。\n\n\n\n\n\n\n\nFigure 2: Hypothetical true performance profile over an arbitrary tuning parameter, with five estimated points\n\n\n\n\n对于这些数据，Figure 3 展示了高斯过程（GP）模型的拟合情况。阴影区域表示均值±1标准误差。两条竖线标示了两个候选点，后续会对其进行更详细的研究。阴影置信区域展示了平方指数方差函数；该区域在数据点之间会变得非常大，而在现有数据点处则收敛到零。\n\n\n\n\n\n\n\nFigure 3: Estimated performance profile generated by the Gaussian process model. The shaded region shows one-standard-error bounds.\n\n\n\n\n这种非线性趋势穿过每个观测点，但该模型并非完美无缺。在真实最优设置附近没有观测点，而在这个区域，拟合效果本可以好得多。尽管如此，高斯过程（GP）模型仍能有效地为我们指明正确方向。\n从纯粹的利用角度来看，最佳选择应该是选取具有最佳平均预测值的参数值。在这里，这个值是0.106，刚好在现有最佳观测点0.09的右侧。\n作为一种鼓励探索的方式，一种简单（但不常使用）的方法是找到与最大置信区间相关的调优参数。例如，通过为 \\(R^2\\) 置信界限使用单一标准差，下一个要采样的点将是0.236。这稍微更深入到没有观测结果的区域。增加上界中使用的标准差数量会将选择进一步推向空白区域。\n最常用的“Acquisition”函数之一是期望改进（expected improvement）。改进的概念需要当前最佳结果的数值（这与置信区间方法不同）。由于高斯过程（GP）可以用一个分布来描述新的候选点，我们可以利用改进发生的概率，对分布中显示出改进的部分进行加权。\n例如，考虑两个候选参数值0.10和0.25（如 Figure 3 中的竖线所示）。利用拟合的GP模型，它们的预测 \\(R^2\\) 分布如 Figure 4 所示，同时还有一条当前最佳结果的参考线。\n\n\n\n\n\n\n\nFigure 4: Predicted performance distributions for two sampled tuning parameter values\n\n\n\n\n仅考虑 \\(R^2\\) 预测均值时，0.10的参数值是更好的选择（参见 Table 3 ）。平均而言，0.25这个调优参数建议被预测为比当前最佳值更差。然而，由于它具有更高的方差，其整体概率分布中高于当前最佳值的区域更大。因此，它具有更大的期望改进：\n\n\n\nTable 3: Expected improvement for the two candidate tuning parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictions\n\n\n\nParameter Value\nMean\nStd Dev\nExpected Improvement\n\n\n\n\n0.10\n0.8679\n0.0004317\n0.000190\n\n\n0.25\n0.8671\n0.0039301\n0.001216\n\n\n\n\n\n\n\n\n当在整个调优参数范围内计算预期改进时，如 Figure 5 所示，建议的采样点更接近0.25而非0.10。\n\n\n\n\n\n\n\nFigure 5: The estimated performance profile generated by the Gaussian process model (top panel) and the expected improvement (bottom panel). The vertical line indicates the point of maximum improvement\n\n\n\n\n已经提出并讨论了许多获取函数；在tidymodels中，期望改进是默认的。\nThe tune_bayes() function\n要通过贝叶斯优化实现迭代搜索，请使用tune_bayes()函数。其语法与tune_grid()非常相似，但有几个额外的参数：\n\niter是最大搜索迭代次数。\ninitial可以是整数、使用tune_grid()生成的对象，或者是某个竞赛函数。使用整数时，它指定了在第一个高斯过程模型之前采样的空间填充设计的大小。\nobjective是一个用于确定应使用哪种“Acquisition”函数的参数。tune包中包含可在此处传递的函数，例如exp_improve()或conf_bound()。\nparam_info指定了参数的范围以及所使用的任何转换。这些用于定义搜索空间，当默认参数对象不够用时，param_info会被用来覆盖默认值。\n\ncontrol参数使用control_bayes()的结果。其中一些有用的参数包括：\n\nno_improve是一个整数，如果在no_improve次迭代内没有发现更优的参数，它将停止搜索。\nuncertain也是一个整数（或Inf），如果在uncertain次迭代内没有改进，它将进行一次不确定性抽样。这会选择具有较大变异性的下一个候选对象，由于它不考虑均值预测，因此具有纯粹探索的效果。\nverbose是一个逻辑值，它会在搜索过程中打印日志信息。\n\n让我们使用第14.1节中的首个支持向量机（SVM）结果作为高斯过程模型的初始基础。回想一下，对于这个应用，我们希望最大化ROC曲线下的面积。我们的代码如下：\n\nctrl &lt;- control_bayes(verbose = TRUE)\n\nset.seed(1403)\nsvm_bo &lt;-\n  svm_wflow %&gt;%\n  tune_bayes(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 25,\n    control = ctrl\n  )\n#&gt; maximum number of iterations reached 4.047214e-05 4.046292e-05\n#&gt; maximum number of iterations reached 5.209914e-05 5.211268e-05\n#&gt; maximum number of iterations reached 6.032941e-05 6.035564e-05\n#&gt; maximum number of iterations reached 5.733733e-05 5.734185e-05\n#&gt; maximum number of iterations reached 6.465712e-05 6.467571e-05\n#&gt; maximum number of iterations reached 3.238272e-05 3.237372e-05\n#&gt; maximum number of iterations reached 3.48257e-05 3.481683e-05\n#&gt; maximum number of iterations reached 4.664036e-05 4.664312e-05\n#&gt; maximum number of iterations reached 2.853935e-05 2.853362e-05\n#&gt; maximum number of iterations reached 3.949777e-05 3.949933e-05\n#&gt; maximum number of iterations reached 0.0004813998 0.000481399\n#&gt; maximum number of iterations reached 0.0004770225 0.0004770218\n#&gt; maximum number of iterations reached 0.0004500721 0.0004500714\n#&gt; maximum number of iterations reached 0.0004592099 0.0004592092\n#&gt; maximum number of iterations reached 0.0004819724 0.0004819717\n#&gt; maximum number of iterations reached 0.0004855306 0.0004855298\n#&gt; maximum number of iterations reached 0.0004604447 0.0004604441\n#&gt; maximum number of iterations reached 0.0005029539 0.0005029531\n#&gt; maximum number of iterations reached 0.0004475102 0.0004475095\n#&gt; maximum number of iterations reached 0.0004798083 0.0004798075\n#&gt; maximum number of iterations reached 0.0151271 0.01501914\n#&gt; maximum number of iterations reached 0.01495382 0.01484879\n#&gt; maximum number of iterations reached 0.0142988 0.01420819\n#&gt; maximum number of iterations reached 0.01459744 0.01450266\n#&gt; maximum number of iterations reached 0.01515703 0.01504754\n#&gt; maximum number of iterations reached 0.0151731 0.01506278\n#&gt; maximum number of iterations reached 0.0145107 0.01441776\n#&gt; maximum number of iterations reached 0.01570231 0.01557913\n#&gt; maximum number of iterations reached 0.01420909 0.01412161\n#&gt; maximum number of iterations reached 0.01490685 0.01480023\n\n搜索过程从ROC曲线下面积的初始最佳值 0.8659 开始。高斯过程模型利用这四个统计量来构建模型。大型候选集通过“期望改进”函数自动生成并评分。\n用于查询结果的函数与网格搜索中使用的函数相同（例如，collect_metrics()等）。例如：\n\nshow_best(svm_bo)\n#&gt; Warning in show_best(svm_bo): No value of `metric` was given; \"roc_auc\" will\n#&gt; be used.\n#&gt; # A tibble: 5 × 9\n#&gt;    cost rbf_sigma .metric .estimator  mean     n std_err .config .iter\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt; 1  31.8   0.00160 roc_auc binary     0.899    10 0.00785 iter24     24\n#&gt; 2  30.8   0.00191 roc_auc binary     0.899    10 0.00791 iter23     23\n#&gt; 3  31.4   0.00166 roc_auc binary     0.899    10 0.00784 iter22     22\n#&gt; 4  31.8   0.00153 roc_auc binary     0.899    10 0.00783 iter13     13\n#&gt; 5  30.8   0.00163 roc_auc binary     0.899    10 0.00782 iter15     15\n\nautoplot()函数有几种用于迭代搜索方法的选项。Figure 6 展示了通过使用autoplot(svm_bo, type = \"performance\")，结果在搜索过程中是如何变化的。\n\n\n\n\n\n\n\nFigure 6: The progress of the Bayesian optimization produced when the autoplot() method is used with type = \"performance\"\n\n\n\n\n另一种类型的图表使用type = \"parameters\"，它展示了参数值在迭代过程中的变化。\n下方的动画可视化了搜索结果。黑色的“××”值表示包含在svm_initial中的初始值。左上角的蓝色面板显示了ROC曲线下面积的预测均值。右上角的红色面板展示了ROC曲线下面积的预测变异性（方差），而底部的图表则可视化了预期改进。在每个面板中，颜色越深表示数值的吸引力越低（例如，均值小、变异性大以及改进小）。\n在搜索的最初几次迭代中，虽然预测的平均表面非常不准确，但它确实有助于将过程引导至性能良好的区域。换句话说，高斯过程模型虽然存在错误，但事实证明它非常有用。在最初的十次迭代内，搜索就在最优位置附近进行采样。\n虽然最佳的调优参数组合位于参数空间的边界上，但贝叶斯优化往往会选择边界另一侧的新点。尽管我们可以调整探索与利用的比例，但搜索往往会在早期对边界点进行采样。\n如果搜索以初始网格为种子，那么填充空间设计可能会是比常规设计更好的选择。它会对参数空间中更多独特的值进行采样，并能在早期迭代中改进对标准差的预测。\n最后，如果用户中断了tune_bayes()的计算，该函数会返回当前结果（而不是产生错误）。",
    "crumbs": [
      "14 Iterative Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/14 Iterative Search.html#simulated-annealing",
    "href": "Books/Tidy Modeling with R/14 Iterative Search.html#simulated-annealing",
    "title": "14 Iterative Search",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n模拟退火（Simulated annealing，SA）（Kirkpatrick，Gelatt 和 Vecchi，1983；Van Laarhoven 和 Aarts，1987）是一种受金属冷却过程启发的通用非线性搜索程序。它是一种全局搜索方法，能够有效遍历多种不同类型的搜索空间，包括不连续函数。与大多数基于梯度的优化程序不同，模拟退火可以重新评估之前的解决方案。\nSimulated annealing search process\n使用模拟退火的过程始于一个初始值，并在参数空间中进行受控的随机游走。每个新的候选参数值都是前一个值的微小扰动，这使得新点保持在局部邻域内。\n对候选点进行重采样以获取其相应的性能值。如果该性能值比之前参数的结果更好，则将其作为新的最优解并继续该过程。如果结果比之前的值更差，搜索程序仍可能使用该参数来确定后续步骤。这取决于两个因素。首先，随着性能变差，接受不良结果的可能性会降低。换句话说，与性能大幅下降的结果相比，略有变差的结果被接受的概率更高。另一个因素是搜索迭代的次数。随着搜索的进行，模拟退火希望接受更少的次优值。基于这两个因素，不良结果的接受概率可以形式化表示为：\n\\[\n\\operatorname{Pr}[\\text{accept suboptimal parameters at iteration } i] = \\exp(c\\times D_i \\times i)\n\\]\n其中i是迭代次数，\\(c\\) 是用户指定的常数，\\(D_i\\) 是新旧值之间的百分比差异（负值意味着结果更差）。对于较差的结果，我们会计算接受概率，并将其与一个随机均匀数进行比较。如果随机数大于该概率值，搜索会舍弃当前参数，下一次迭代会在之前值的邻域内生成候选值。否则，下一次迭代会基于当前（次优）值形成下一组参数。\n模拟退火的接受概率允许搜索朝着错误的方向进行，至少在短期内是这样，但从长远来看，有可能找到参数空间中一个好得多的区域。\n接受概率会受到怎样的影响？Figure 7 中的热图展示了接受概率如何随着迭代次数、性能以及用户指定的系数而变化。\n\n\n\n\n\n\n\nFigure 7: Heatmap of the simulated annealing acceptance probabilities for different coefficient values\n\n\n\n\n用户可以调整系数，以找到适合自己需求的概率分布。在finetune::control_sim_anneal()中，这个cooling_coef参数的默认值为0.02。减小该系数会使搜索过程对不佳结果更加宽容。\n这一过程会持续设定好的迭代次数，但如果在预先确定的迭代次数内没有出现全局最优结果，就会停止。不过，设置一个重启阈值会非常有帮助。如果出现一连串的失败，该功能会重新采用上一次的全局最优参数设置并重新开始。\n主要的重要细节是定义如何在迭代之间扰动调优参数。文献中对此有多种方法。我们采用了Bohachevsky, Johnson, and Stein（1986）提出的一种名为广义模拟退火（generalized simulated annealing）的方法。对于连续的调优参数，我们定义一个小半径来指定局部“邻域”。例如，假设有两个调优参数，每个参数的取值范围都在0到1之间。模拟退火过程会在周围的半径范围内生成随机值，并随机选择一个作为当前的候选值。\n在我们的实现中，邻域是通过根据参数对象的范围将当前候选值缩放到0到1之间来确定的，因此0.05到0.15之间的半径值似乎是合理的。对于这些值，搜索从参数空间的一侧到另一侧最快大约需要10次迭代。半径的大小控制着搜索探索参数空间的速度。在我们的实现中，会指定一个半径范围，因此不同大小的“局部”定义了新的候选值。\n为了说明这一点，我们将使用glmnet的两个主要调优参数：\n\n总正则化量（penalty）。该参数的默认范围是 \\(10^{-10}\\) 到 \\(10^{0}\\) 。通常对该参数进行以10为底的对数转换。\nLasso惩罚的比例（mixture）。它在0到1之间有界，无需转换。\n\n该过程从初始值penalty = 0.025和mixture = 0.050开始。使用一个在0.050到0.015之间随机波动的半径，对数据进行适当缩放，在初始点周围的半径范围内生成随机值，然后随机选择一个作为候选值。为便于说明，我们假设所有候选值都是更优的。利用这个新值，生成一组新的随机邻近值，从中选择一个，依此类推。Figure 8 展示了搜索过程向左上角推进的六个迭代步骤。\n\n\n\n\n\n\n\nFigure 8: How simulated annealing determines the local neighborhood for two numeric tuning parameters. The clouds of points show possible next values where one would be selected at random.\n\n\n\n\n请注意，在某些迭代过程中，沿半径生成的候选集不包括参数边界之外的点。此外，我们的实现会使下一个调优参数配置的选择倾向于远离与先前配置非常相似的新值。\n对于非数值参数，我们会为参数值的变化频率分配一个概率。\nThe tune_sim_anneal() function\n要通过模拟退火实现迭代搜索，请使用tune_sim_anneal()函数。该函数的语法与tune_bayes()几乎相同。它没有关于采集函数或不确定性抽样的选项。control_sim_anneal()函数包含一些用于定义局部邻域和冷却调度的细节：\n\nno_improve，对于模拟退火算法而言，是一个整数，若在no_improve次迭代内未发现全局最优结果或改进结果，该整数将使搜索停止。被接受的次优参数或被丢弃的参数均算作“无改进”。\nrestart是指在从之前的最佳结果重新开始之前，没有新的最佳结果出现的迭代次数。\nradius是一个取值在(0, 1)上的数值向量，它定义了初始点周围局部邻域的最小和最大半径。\nflip是一个概率值，它定义了改变分类参数或整数参数值的可能性。\ncooling_coef是 \\(\\exp(c\\times D_i \\times i)\\) 中的 \\(c\\) 系数，它调节接受概率在迭代过程中的下降速度。cooling_coef的值越大，接受次优参数设置的概率就越低。\n\n对于细胞分割数据，其语法与之前使用的函数非常一致：\n\nctrl_sa &lt;- finetune::control_sim_anneal(verbose = TRUE, no_improve = 10L)\n\nset.seed(1404)\nsvm_sa &lt;-\n  svm_wflow %&gt;%\n  finetune::tune_sim_anneal(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 50,\n    control = ctrl_sa\n  )\n\n模拟退火过程在4个不同的迭代中发现了新的全局最优解。最早的改进出现在第5次迭代，最终的最优解出现在第27次迭代。总体最佳结果出现在第27次迭代，此时ROC曲线下的平均面积为0.8985（初始最佳值为0.8659）。在迭代过程中，分别在第13、21、35和43次迭代时进行了4次重启，同时还有12个候选解被舍弃。（此处过程描述可能不准确，参考实际日志信息）\n与其他tune_*()函数一样，相应的autoplot()函数会对结果进行可视化评估。使用autoplot(svm_sa, type = \"performance\")可以显示迭代过程中的性能（ Figure 9 ），而autoplot(svm_sa, type = \"parameters\")则会绘制性能与特定调优参数值的对比图（ Figure 10 ）。\n\n\n\n\n\n\n\nFigure 9: Progress of the simulated annealing process shown when the autoplot() method is used with type = \"performance\"\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Performance versus tuning parameter values shown when the autoplot() method is used with type = \"parameters\"\n\n\n\n\n搜索路径的可视化有助于理解搜索过程在哪些地方表现良好，在哪些地方出现了偏差：\n与tune_bayes()类似，手动停止执行将会返回已完成的迭代。",
    "crumbs": [
      "14 Iterative Search"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/14 Iterative Search.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/14 Iterative Search.html#chapter-summary",
    "title": "14 Iterative Search",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n本章介绍了两种用于优化调优参数的迭代搜索方法。贝叶斯优化利用在现有重采样结果上训练的预测模型来推荐调优参数值，而模拟退火则在超参数空间中遍历以寻找合适的值。这两种方法无论是单独使用，还是作为初始网格搜索之后的后续方法来进一步微调性能，都能有效地找到合适的参数值。",
    "crumbs": [
      "14 Iterative Search"
    ]
  },
  {
    "objectID": "Books/The Elements of Statistical Learning/1 Introduction.html",
    "href": "Books/The Elements of Statistical Learning/1 Introduction.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html",
    "title": "15 Screening Many Models",
    "section": "",
    "text": "我们在第7章介绍了工作流集合，并在第11章展示了如何将其与重新采样的数据集结合使用。在本章中，我们将更详细地探讨这些多模型工作流集合，并介绍一个能够体现其实际价值的用例。\n对于那些包含尚未被充分理解的新数据集的项目，数据从业者可能需要筛选多种模型与预处理方法的组合。通常情况下，人们对哪种方法最适合处理全新数据集几乎一无所知，甚至完全缺乏先验知识。一个好的策略是，初期先花精力尝试多种建模方法，确定哪种效果最佳，然后再投入更多时间对少数几个模型进行微调和优化。\n工作流集提供了一个用户界面，用于创建和管理这一流程。我们还将演示如何利用第15.4节中讨论的竞赛方法，高效地评估这些模型。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#modeling-concrete-mixture-strength",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#modeling-concrete-mixture-strength",
    "title": "15 Screening Many Models",
    "section": "Modeling Concrete Mixture Strength",
    "text": "Modeling Concrete Mixture Strength\n为了演示如何筛选多个模型工作流，我们将以《Applied Predictive Modeling》（M. Kuhn 和 Johnson，2013）中的混凝土混合料数据为例。该书第10章展示了利用配料成分作为预测变量，预测混凝土混合料抗压强度的模型。书中评估了多种不同预测变量组合及预处理需求的模型。那么，工作流集合如何使这种大规模的模型测试过程变得更加简便呢？\n首先，让我们定义数据的分割和重采样方案。\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\ndata(concrete, package = \"modeldata\")\nglimpse(concrete)\n#&gt; Rows: 1,030\n#&gt; Columns: 9\n#&gt; $ cement               &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.…\n#&gt; $ blast_furnace_slag   &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95…\n#&gt; $ fly_ash              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ water                &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 2…\n#&gt; $ superplasticizer     &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n#&gt; $ coarse_aggregate     &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 93…\n#&gt; $ fine_aggregate       &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.…\n#&gt; $ age                  &lt;int&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90…\n#&gt; $ compressive_strength &lt;dbl&gt; 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.7…\n\ncompressive_strength列是结果列，age预测变量表示混凝土试样在测试时的龄期（以天为单位，因为混凝土会随时间增强），而其他预测变量，如cement和water，则分别以每立方米千克为单位，代表混凝土的组成成分。\n在本数据集的某些案例中，同一款混凝土配比曾被多次测试。我们不建议将这些重复配比作为独立的数据点纳入，因为它们可能同时分布于训练集和测试集中。如果这样做，可能会人为地夸大我们的性能评估结果。为解决这一问题，我们将采用每种混凝土配比的平均抗压强度进行建模：\n\nconcrete &lt;-\n  concrete %&gt;%\n  group_by(across(-compressive_strength)) %&gt;%\n  summarize(\n    compressive_strength = mean(compressive_strength),\n    .groups = \"drop\"\n  )\nnrow(concrete)\n#&gt; [1] 992\n\n让我们按照默认的3:1训练集与测试集比例分割数据，并对训练集进行5次重复的10折交叉验证重采样：\n\nset.seed(1501)\nconcrete_split &lt;- initial_split(concrete, strata = compressive_strength)\nconcrete_train &lt;- training(concrete_split)\nconcrete_test &lt;- testing(concrete_split)\n\nset.seed(1502)\nconcrete_folds &lt;-\n  vfold_cv(concrete_train, strata = compressive_strength, repeats = 5)\n\n一些模型（尤其是神经网络、KNN和支持向量机）需要经过中心化和标准化处理的预测变量，因此某些模型的工作流程将要求包含这些预处理步骤的recipe对象。而对于其他模型，采用传统的响应面设计模型扩展方法（即二次项及两两交互作用）则更为合适。为此，我们制定了两个recipe对象：\n\nnormalized_rec &lt;-\n  recipe(compressive_strength ~ ., data = concrete_train) %&gt;%\n  step_normalize(all_predictors())\n\npoly_recipe &lt;-\n  normalized_rec %&gt;%\n  step_poly(all_predictors()) %&gt;%\n  step_interact(~ all_predictors():all_predictors())\n\n对于模型，我们使用Parsnip加载项来创建一组模型规范：\n\nlibrary(rules)\nlibrary(baguette)\n\nlinear_reg_spec &lt;-\n  linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\nnnet_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine(\"nnet\", MaxNWts = 2600) %&gt;%\n  set_mode(\"regression\")\n\nmars_spec &lt;-\n  mars(prod_degree = tune()) %&gt;% #&lt;- use GCV to choose terms\n  set_engine(\"earth\") %&gt;%\n  set_mode(\"regression\")\n\nsvm_r_spec &lt;-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"regression\")\n\nsvm_p_spec &lt;-\n  svm_poly(cost = tune(), degree = tune()) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"regression\")\n\nknn_spec &lt;-\n  nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"regression\")\n\ncart_spec &lt;-\n  decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\nbag_cart_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine(\"rpart\", times = 50L) %&gt;%\n  set_mode(\"regression\")\n\nrf_spec &lt;-\n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_spec &lt;-\n  boost_tree(\n    tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(),\n    min_n = tune(), sample_size = tune(), trees = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\ncubist_spec &lt;-\n  cubist_rules(committees = tune(), neighbors = tune()) %&gt;%\n  set_engine(\"Cubist\")\n\nM. Kuhn 和 Johnson（2013）的分析指出，神经网络的隐藏层最多应包含27个单元。extract_parameter_set_dials()函数用于提取参数集，我们对其进行了修改，以确保参数范围正确：\n\nnnet_param &lt;-\n  nnet_spec %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(hidden_units = hidden_units(c(1, 27)))\n\n我们如何将这些模型与其对应的算法相匹配、进行调优，然后高效地评估其性能？一套工作流方案为此提供了答案。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#creating-the-workflow-set",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#creating-the-workflow-set",
    "title": "15 Screening Many Models",
    "section": "Creating the Workflow Set",
    "text": "Creating the Workflow Set\n工作流集将预处理程序和模型规范的命名列表组合成一个对象，其中包含多个工作流。预处理程序共有三种可能的类型：\n\n标准的R公式\n预处理前的recipe对象（用于估计或准备数据）\n类似dplyr风格的选择器，用于选择因变量和自变量\n\n作为第一个工作流集示例，让我们将仅对自变量进行标准化的配方与那些要求自变量处于相同单位的非线性模型结合起来：\n\nnormalized &lt;-\n  workflow_set(\n    preproc = list(normalized = normalized_rec),\n    models = list(\n      SVM_radial = svm_r_spec, SVM_poly = svm_p_spec,\n      KNN = knn_spec, neural_network = nnet_spec\n    )\n  )\nnormalized\n#&gt; # A workflow set/tibble: 4 × 4\n#&gt;   wflow_id                  info             option    result    \n#&gt;   &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n由于只有一个预处理器，此函数会使用该值创建一组工作流。如果预处理器包含多个条目，该函数将生成所有预处理器与模型的组合。\nwflow_id列会自动创建，但可通过调用mutate()进行修改。info列包含一个tibble，其中存储了一些标识符和工作流对象。该工作流可被提取：\n\nnormalized %&gt;% extract_workflow(id = \"normalized_KNN\")\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: nearest_neighbor()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_normalize()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; K-Nearest Neighbor Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   neighbors = tune()\n#&gt;   weight_func = tune()\n#&gt;   dist_power = tune()\n#&gt; \n#&gt; Computational engine: kknn\n\noption列是我们在评估工作流时用于指定任何参数的占位符。当使用来自tune或finetune包中的函数对工作流进行调优（或重采样）时，将采用此参数。例如，要添加神经网络参数对象：\n\nnormalized &lt;-\n  normalized %&gt;%\n  option_add(param_info = nnet_param, id = \"normalized_neural_network\")\nnormalized\n#&gt; # A workflow set/tibble: 4 × 4\n#&gt;   wflow_id                  info             option    result    \n#&gt;   &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\nresult列是调优或重采样函数输出的占位符。\n对于其他非线性模型，我们来创建另一套工作流，其中使用dplyr选择器来指定响应变量和预测变量：\n\nmodel_vars &lt;-\n  workflow_variables(\n    outcomes = compressive_strength,\n    predictors = everything()\n  )\n\nno_pre_proc &lt;-\n  workflow_set(\n    preproc = list(simple = model_vars),\n    models = list(\n      MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,\n      RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec\n    )\n  )\nno_pre_proc\n#&gt; # A workflow set/tibble: 6 × 4\n#&gt;   wflow_id           info             option    result    \n#&gt;   &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 simple_MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 simple_CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 simple_CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 simple_RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 5 simple_boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 6 simple_Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n最后，我们用适当的模型组装包含非线性项和交互作用的集合：\n\nwith_features &lt;-\n  workflow_set(\n    preproc = list(full_quad = poly_recipe),\n    models = list(linear_reg = linear_reg_spec, KNN = knn_spec)\n  )\n\n这些对象是带有额外“workflow_set”类别的tibble。行绑定不会影响集合的状态，其结果本身也是一个工作流集合：\n\nall_workflows &lt;-\n  bind_rows(no_pre_proc, normalized, with_features) %&gt;%\n  # Make the workflow ID's a little more simple:\n  mutate(wflow_id = gsub(\"(simple_)|(normalized_)\", \"\", wflow_id))\nall_workflows\n#&gt; # A workflow set/tibble: 12 × 4\n#&gt;   wflow_id    info             option    result    \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; # ℹ 6 more rows",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#tuning-and-evaluating-the-models",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#tuning-and-evaluating-the-models",
    "title": "15 Screening Many Models",
    "section": "Tuning and Evaluating the Models",
    "text": "Tuning and Evaluating the Models\n几乎所有all_workflows中的成员都包含调优参数。为了评估它们的性能，我们可以使用标准的调优或重采样函数（例如，tune_grid()等）。workflow_map()函数会将同一函数应用到集合中所有工作流；默认情况下，它使用tune_grid()。\n在这个示例中，网格搜索被应用于每个工作流，最多使用25个不同的参数候选值。在每次执行tune_grid()时，都有一组通用选项可供使用。例如，在以下代码中，我们将为每个工作流使用相同的重采样和控制对象，并指定网格大小为25。此外，workflow_map()函数还包含一个名为seed的额外参数，用于确保每次执行tune_grid()时都能消耗相同的随机数。\n\ngrid_ctrl &lt;-\n  control_grid(\n    save_pred = TRUE,\n    parallel_over = \"everything\",\n    save_workflow = TRUE\n  )\n\ngrid_results &lt;-\n  all_workflows %&gt;%\n  workflow_map(\n    seed = 1503,\n    resamples = concrete_folds,\n    grid = 25,\n    control = grid_ctrl\n  )\n\n结果显示，option列和result列已更新：\n\ngrid_results\n#&gt; # A workflow set/tibble: 12 × 4\n#&gt;   wflow_id    info             option    result   \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n#&gt; 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n#&gt; 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n#&gt; 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n#&gt; 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n#&gt; 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n#&gt; # ℹ 6 more rows\n\n选项列现在包含了我们在workflow_map()调用中使用的所有选项。这使得我们的结果具有可重复性。在result列中，“tune[+]”和“rsmp[+]”标记表示该对象未出现任何问题；而像“tune[x]”这样的值，则表明由于某种原因，所有模型均未能通过测试。\n有一些便捷函数可用于检查结果，例如grid_results。rank_results()函数会根据某种性能指标对模型进行排序。默认情况下，它使用指标集合中的第一个指标（本例中为RMSE）。让我们使用filter()仅查看RMSE：\n\ngrid_results %&gt;%\n  rank_results() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(model, .config, rmse = mean, rank)\n#&gt; # A tibble: 252 × 4\n#&gt;   model        .config                rmse  rank\n#&gt;   &lt;chr&gt;        &lt;chr&gt;                 &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 cubist_rules Preprocessor1_Model24  4.71     1\n#&gt; 2 cubist_rules Preprocessor1_Model21  4.72     2\n#&gt; 3 cubist_rules Preprocessor1_Model10  4.76     3\n#&gt; 4 cubist_rules Preprocessor1_Model23  4.78     4\n#&gt; 5 cubist_rules Preprocessor1_Model16  4.78     5\n#&gt; 6 cubist_rules Preprocessor1_Model19  4.78     6\n#&gt; # ℹ 246 more rows\n\n默认情况下，该函数会对所有候选集进行排序；因此，同一个模型可能会在输出中多次出现。此外，还可通过名为“select_best”的选项，按模型的最佳调参组合对其进行排序。\nautoplot()方法用于绘制排名；它还有一个select_best参数。 Figure 1 中的图表展示了每个模型的最佳结果，生成方式如下：\n\nautoplot(\n  grid_results,\n  rank_metric = \"rmse\", # &lt;- how to order models\n  metric = \"rmse\", # &lt;- which metric to visualize\n  select_best = TRUE # &lt;- one point per workflow\n) +\n  geom_text(aes(y = mean - 1 / 2, label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(3.5, 9.5)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 1: Estimated RMSE (and approximate confidence intervals) for the best model configuration in each workflow.\n\n\n\n\n如果您想查看特定模型的调参结果，例如 Figure 2 ，id参数可从wflow_id列中选取一个值，以指定要绘制的模型：\n\nautoplot(grid_results, id = \"Cubist\", metric = \"rmse\")\n\n\n\n\n\n\nFigure 2: The autoplot() results for the Cubist model contained in the workflow set.\n\n\n\n\n此外，还有collect_predictions()和collect_metrics()的方法。\n我们使用混凝土混合料数据进行的示例模型筛选共拟合了12,600个模型。通过2核心并行操作，整个估算过程耗时约2小时。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#efficiently-screening-models",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#efficiently-screening-models",
    "title": "15 Screening Many Models",
    "section": "Efficiently Screening Models",
    "text": "Efficiently Screening Models\n一种高效筛选大量模型的有效方法，是使用第13.5.5节中描述的竞速法。通过设置工作流集，我们可以利用workflow_map()函数来实施这一竞速策略。请注意，当我们以管道方式将工作流集导入后，所使用的参数正是应用于这些工作流的函数；在此情形下，我们可以选择“tune_race_anova”作为目标函数。此外，我们还需传递一个适当的控制对象；否则，选项将与前一节代码中的设置相同。\n\nlibrary(finetune)\n\nrace_ctrl &lt;-\n  control_race(\n    save_pred = TRUE,\n    parallel_over = \"everything\",\n    save_workflow = TRUE\n  )\n\nrace_results &lt;-\n  all_workflows %&gt;%\n  workflow_map(\n    \"tune_race_anova\",\n    seed = 1503,\n    resamples = concrete_folds,\n    grid = 25,\n    control = race_ctrl\n  )\n\n新的对象看起来非常相似，尽管result列中的元素显示值为“race[+]”，这表明它是一种不同类型的对象：\n\nrace_results\n#&gt; # A workflow set/tibble: 12 × 4\n#&gt;   wflow_id    info             option    result   \n#&gt;   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n#&gt; 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n#&gt; 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n#&gt; 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n#&gt; 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n#&gt; 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n#&gt; # ℹ 6 more rows\n\n同样的有用功能也适用于该对象，以查询结果；事实上，Figure 3 中所示的基本autoplot()方法生成的趋势与 Figure 1 相似。这是通过以下方式实现的：\n\nautoplot(\n  race_results,\n  rank_metric = \"rmse\",\n  metric = \"rmse\",\n  select_best = TRUE\n) +\n  geom_text(aes(y = mean - 1 / 2, label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(3.0, 9.5)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 3: Estimated RMSE (and approximate confidence intervals) for the best model configuration in each workflow in the racing results.\n\n\n\n\n总体而言，该竞速方法共估算出1050个模型，占完整网格中12600个模型总数的8.33%。因此，该竞速方法的速度提升了4.8倍。\n我们是否得到了相似的结果？对于这两个对象，我们对结果进行排序，然后将它们合并，并在 Figure 4 中相互对比绘制。\n\nmatched_results &lt;-\n  rank_results(race_results, select_best = TRUE) %&gt;%\n  select(wflow_id, .metric, race = mean, config_race = .config) %&gt;%\n  inner_join(\n    rank_results(grid_results, select_best = TRUE) %&gt;%\n      select(wflow_id, .metric,\n        complete = mean,\n        config_complete = .config, model\n      ),\n    by = c(\"wflow_id\", \".metric\"),\n  ) %&gt;%\n  filter(.metric == \"rmse\")\n\nlibrary(ggrepel)\n\nmatched_results %&gt;%\n  ggplot(aes(x = complete, y = race)) +\n  geom_abline(lty = 3) +\n  geom_point() +\n  geom_text_repel(aes(label = model)) +\n  coord_obs_pred() +\n  labs(x = \"Complete Grid RMSE\", y = \"Racing RMSE\")\n\n\n\n\n\n\nFigure 4: Estimated RMSE for the full grid and racing results.\n\n\n\n\n尽管在仅41.67%的模型中，Racing方法选择的候选参数与完整网格完全一致，但这些由Racing方法选出的模型性能指标却几乎相同。其中，RMSE值的相关性高达0.968，而秩相关性更是达到0.951。这表明，在单个模型内部，存在多种调参组合，它们的预测结果几乎完全一致。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#finalizing-a-model",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#finalizing-a-model",
    "title": "15 Screening Many Models",
    "section": "Finalizing a Model",
    "text": "Finalizing a Model\n正如我们在前几章中所展示的，选择最终模型并将其拟合到训练集上的过程非常简单。首先，我们需要选定一种工作流程来确定最终模型。由于提升树模型表现良好，我们将从中挑选出来，用数值上最优的参数设置进行更新，然后将其拟合到训练集上：\n\nbest_results &lt;-\n  race_results %&gt;%\n  extract_workflow_set_result(\"boosting\") %&gt;%\n  select_best(metric = \"rmse\")\nbest_results\n#&gt; # A tibble: 1 × 7\n#&gt;   trees min_n tree_depth learn_rate loss_reduction sample_size\n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1  1635     6          5     0.0142   0.0000000500       0.919\n#&gt; # ℹ 1 more variable: .config &lt;chr&gt;\n\nboosting_test_results &lt;-\n  race_results %&gt;%\n  extract_workflow(\"boosting\") %&gt;%\n  finalize_workflow(best_results) %&gt;%\n  last_fit(split = concrete_split)\n\n我们可以查看测试集的指标结果，并在 Figure 5 中可视化预测结果。\n\ncollect_metrics(boosting_test_results)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimator .estimate .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 rmse    standard       3.45  pre0_mod0_post0\n#&gt; 2 rsq     standard       0.952 pre0_mod0_post0\n\n\nboosting_test_results %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(x = compressive_strength, y = .pred)) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred() +\n  labs(x = \"observed\", y = \"predicted\")\n\n\n\n\n\n\nFigure 5: Observed versus predicted values for the test set.\n\n\n\n\n我们在这里看到，这些混凝土混合料的实测抗压强度与预测值之间高度吻合。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/15 Screening Many Models.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/15 Screening Many Models.html#chapter-summary",
    "title": "15 Screening Many Models",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n数据从业者常常需要针对手头的任务考虑大量可能的建模方法，尤其是在面对新数据集，或对哪种建模策略最有效尚缺乏了解的情况下。本章介绍了如何利用工作流集合，在这种情境下同时探索多种模型或特征工程策略。与逐一拟合所有候选模型相比，竞速法能够更高效地对模型进行排序。",
    "crumbs": [
      "15 Screening Many Models"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html",
    "title": "16 Dimensionality Reduction",
    "section": "",
    "text": "降维技术将数据集从高维空间转换为低维空间，当你觉得变量“过多”时，这可能是一个不错的选择。通常，过多的变量（尤其是预测变量）会带来问题，因为随着维度的增加，数据变得难以理解或可视化。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#what-problems-can-dimensionality-reduction-solve",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#what-problems-can-dimensionality-reduction-solve",
    "title": "16 Dimensionality Reduction",
    "section": "What Problems Can Dimensionality Reduction Solve?",
    "text": "What Problems Can Dimensionality Reduction Solve?\n降维技术既可用于特征工程，也可用于探索性数据分析。例如，在高维度的生物学实验中，任何建模之前，首要任务之一便是确定数据中是否存在不希望出现的趋势（比如批次等与研究问题无关的影响）。当数据维度高达数十万时，调试数据往往颇具挑战性，而降维则能有效帮助进行探索性数据分析。\n预测变量过多的另一个潜在后果，可能是对模型造成损害。最简单的例子就是普通线性回归法：在这种方法中，预测变量的数量应少于用于拟合模型的数据点数量。此外，多重共线性也是一个问题——当不同预测变量之间存在高度相关性时，这会负面地影响用于估计模型的数学运算。如果预测变量的数量多到极致，现实中真正起作用的潜在效应却极有可能远不及此数。换句话说，某些预测变量可能实际上是在测量相同的潜在效应，从而导致它们彼此高度相关。而许多降维技术恰恰擅长处理这种情况——事实上，大多数降维方法只有在预测变量间存在可被有效利用的相关关系时，才能发挥出色效果。\n在启动一个新的建模项目时，降低数据的维度或许能帮助你初步了解建模问题的难度。\n主成分分析（PCA）是减少数据集中列数最直接的方法之一，因为它基于线性方法且属于无监督学习（即不考虑目标数据）。对于高维分类问题，初步绘制出主要的PCA成分图后，可能会清晰地看到各类别之间的分离。如果确实如此，那么可以较为稳妥地认为，一个线性分类器或许能取得不错的效果。然而，情况并非总是如此——即使未观察到明显分离，也并不意味着问题无法解决。\n本章讨论的降维方法通常并非特征选择方法。例如，PCA等方法是用一组较少的新特征来表示原始预测变量，而计算这些新特征时仍需用到所有原始预测变量。不过，稀疏方法是个例外，它们能够在构建新特征时彻底消除某些预测变量的影响。\n本章有两个目标：\n\n展示如何利用recipe创建一组精简的特征，以捕捉原始预测变量集的主要特性。\n说明recipe如何单独使用（而非像第8.2节中那样嵌入工作流对象中）。\n\n后者在测试或调试recipe对象时非常有用。然而，如第8.2节所述，将recipe对象用于建模的最佳方式是通过工作流对象来实现。\n除了tidymodels包，本章还使用了以下包：baguette、beans、bestNormalize、corrplot、discrim、embed、ggforce、klaR、learntidymodels、mixOmics以及uwot。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#a-picture-is-worth-a-thousand-beans",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#a-picture-is-worth-a-thousand-beans",
    "title": "16 Dimensionality Reduction",
    "section": "A Picture Is Worth a Thousand… Beans",
    "text": "A Picture Is Worth a Thousand… Beans\n让我们通过一个示例数据集，结合具体recipe对象，逐步讲解如何使用降维技术。Koklu 和 Ozkan（2020）发表了一组关于干豆视觉特征的数据集，并详细介绍了从图像中识别干豆品种的方法。尽管与许多现实世界中的建模问题相比，这些数据的维度并不算高，但它却提供了一个绝佳的实践案例，帮助我们演示如何有效减少特征数量。摘自他们的论文：\n\n\n\n\n\n\nThe primary objective of this study is to provide a method for obtaining uniform seed varieties from crop production, which is in the form of population, so the seeds are not certified as a sole variety. Thus, a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.\n\n\n\n每张图像中都包含多种豆类。确定哪些像素对应于特定豆类的过程被称为图像分割。这些像素可被进一步分析，以提取每种豆类的特征，例如颜色和形态学（即形状）。随后，这些特征将被用于建立模型，以预测豆类的品种——因为不同品种的豆类外观各异。训练数据来源于一组经过人工标注的图像，利用这些数据构建出一个预测模型，能够准确区分卡利、霍罗兹、德玛森、塞克尔、孟买、巴尔布尼亚和西拉这七种豆类品种。开发出高效模型后，制造商便能更精确地评估一批豆类的均匀性。\n有许多方法可用于量化物体的形状（Mingqiang、Kidiyo 和 Joseph，2008）。其中许多方法与感兴趣物体的边界或区域相关。特征示例包括：\n\n该区域（或面积）可通过对物体中的像素数量，或围绕物体的凸包大小进行估算。\n我们可以用边界上的像素数量以及包围框的面积来测量周长（包围框是指包含物体的最小矩形）。\n长轴量化了连接物体最外侧部分的最长直线；短轴则与长轴垂直。\n我们可以通过比较物体的面积与具有相同周长的圆的面积之比，来衡量物体的紧凑程度。例如，“•”和“×”这两个符号的紧凑性就大不相同。\n此外，还有多种衡量物体是否细长或椭圆程度的指标。例如，偏心率是长轴与短轴之比；此外，还有与之相关的圆形度和凸度估算方法。\n\n请注意 Figure 1 中不同形状的偏心率。圆形和方形等形状的偏心率较低，而长条形则具有较高值。此外，该度量指标不受物体旋转的影响。\n\n\n\n\n\nFigure 1: Some example shapes and their eccentricity statistics\n\n\n这些图像特征中许多具有高度相关性；面积较大的物体往往也拥有较长的周长。此外，通常存在多种方法来量化同一基本特性（例如尺寸）。\n在豆类数据中，计算了16个形态特征：面积、周长、长轴长度、短轴长度、长宽比、偏心率、凸面积、等效直径、延伸度、坚实度、圆度、紧凑度、形状因子1、形状因子2、形状因子3和形状因子4。其中，后四个特征已在Symons和Fulcher（1988）中进行了详细描述。\n我们可以从加载数据开始：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ntidymodels_prefer()\nlibrary(beans)\n\n在评估降维技术时，保持良好的数据规范至关重要，尤其计划在模型中使用这些技术时。\n在我们的分析中，我们首先使用initial_split()保留一个测试集。剩余的数据则被划分为训练集和验证集：\n\nset.seed(1601)\nbean_split &lt;- initial_validation_split(beans, strata = class, prop = c(0.75, 0.125))\n#&gt; Warning: Too little data to stratify.\n#&gt; • Resampling will be unstratified.\nbean_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;10206/1702/1703/13611&gt;\n\n# Return data frames:\nbean_train &lt;- training(bean_split)\nbean_test &lt;- testing(bean_split)\nbean_validation &lt;- validation(bean_split)\n\n\nset.seed(1602)\n# Return an 'rset' object to use with the tune functions:\nbean_val &lt;- validation_set(bean_split)\nbean_val$splits[[1]]\n#&gt; &lt;Training/Validation/Total&gt;\n#&gt; &lt;10206/1702/11908&gt;\n\n为了直观评估不同方法的性能，我们可以在训练集（n = 10,206）颗豆子上对这些方法进行估计，并利用验证集（n = 1,702）展示结果。\n在开始任何降维之前，我们可以花些时间探索一下我们的数据。由于我们知道这些形状特征中许多可能测量的是相似的概念，让我们通过以下代码来看看 Figure 2 中数据的相关结构。\n\nlibrary(corrplot)\n#&gt; corrplot 0.95 loaded\ntmwr_cols &lt;- colorRampPalette(c(\"#91CBD765\", \"#CA225E\"))\nbean_train %&gt;%\n  select(-class) %&gt;%\n  cor() %&gt;%\n  corrplot(col = tmwr_cols(200), tl.col = \"black\", method = \"ellipse\")\n\n\n\n\n\n\nFigure 2: Correlation matrix of the predictors with variables ordered via clustering\n\n\n\n\n这些预测因子中有很多高度相关，例如面积与周长，或形状因子2和3。尽管我们在此不花时间详细分析，但同样重要的是要考察这种相关结构是否在不同类别结果间显著变化。这有助于构建更优质的模型。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#a-starter-recipe",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#a-starter-recipe",
    "title": "16 Dimensionality Reduction",
    "section": "A Starter Recipe",
    "text": "A Starter Recipe\n是时候在更小的范围内查看豆类数据了。我们可以从一个基本的预处理流程入手，以便在进行任何降维步骤之前对数据进行初步处理。其中一些预测变量是比率，因此很可能呈现偏态分布。而这种分布可能会严重干扰方差计算（例如主成分分析中所用的方差计算）。为此，bestNormalize 包提供了一个步骤，能够强制使预测变量的分布呈对称状态。我们将利用这一方法来有效缓解偏态分布带来的问题：\n\nlibrary(bestNormalize)\nbean_rec &lt;-\n  # Use the training data from the bean_val split object\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n记住，调用recipe()函数时，步骤函数step_*()不会以任何方式被估算或执行。\n这个recipe对象将增加用于降维分析的额外步骤。在进行这些步骤之前，我们先来了解一下如何在工作流程之外使用recipe对象。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#recipes-in-the-wild",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#recipes-in-the-wild",
    "title": "16 Dimensionality Reduction",
    "section": "Recipes in the Wild",
    "text": "Recipes in the Wild\n正如第8.2节所述，包含recipe对象的工作流使用fit()来估计配方和模型，然后使用predict()处理数据并生成模型预测。在recipes包中也有类似的函数，可用于实现相同的目的：\n\nprep(recipe, training)将recipe对象适配到训练集。\nbake(recipe, new_data)应用recipe对象到new_data。\n\nFigure 3 对此进行了总结。让我们更详细地探讨这些功能中的每一项。\n\n\n\n\n\nFigure 3: Summary of recipe-related functions\n\n\nPreparing a recipe\n让我们使用训练集数据来估计bean_rec，并执行prep(bean_rec)：\n\nbean_rec_trained &lt;- prep(bean_rec)\nbean_rec_trained\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 16\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 10206 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Zero variance filter removed: &lt;none&gt; | Trained\n#&gt; • orderNorm transformation on: area perimeter, ... | Trained\n#&gt; • Centering and scaling for: area perimeter, ... | Trained\n\n请注意，输出中显示步骤已训练完成，且选择器不再是一般性的（即不再使用all_numeric_predictors()），而是直接列出了实际选定的列。此外，prep(bean_rec) 不再需要training参数。你可以向该参数传递任何数据，但若省略，则将使用最初调用recipe()时提供的原始数据data——在本例中，即训练集数据。\nprep()的一个重要参数是retain。当retain = TRUE（默认值）时，训练集的预处理版本会被保留在recipe对象中。这一数据集已按照recipe对象中列出的所有步骤完成预处理。由于prep()在执行过程中需要逐步应用这些步骤，因此保留该版本的训练集将大有裨益——这样，若后续仍需使用该数据集，便可避免重复计算。不过，如果训练集本身规模庞大，将如此大量的数据存储于内存中可能会带来问题。此时，建议设置retain = FALSE，以规避这一风险。\n当向此预估recipe对象中添加新步骤后，重新应用prep()将仅估算未训练的步骤。这在我们尝试不同特征提取方法时将非常有用。如果在使用recipe对象遇到错误时，可以结合prep()的verbose选项进行故障排除：\n\nbean_rec_trained %&gt;%\n  step_dummy(cornbread) %&gt;% # &lt;- not a real predictor\n  prep(verbose = TRUE)\n#&gt; oper 1 step zv [pre-trained]\n#&gt; oper 2 step orderNorm [pre-trained]\n#&gt; oper 3 step normalize [pre-trained]\n#&gt; oper 4 step dummy [training]\n#&gt; Error in `step_dummy()`:\n#&gt; Caused by error in `prep()`:\n#&gt; ! Can't select columns that don't exist.\n#&gt; ✖ Column `cornbread` doesn't exist.\n\n另一个有助于你了解分析中发生情况的选项是log_changes：\n\nshow_variables &lt;-\n  bean_rec %&gt;%\n  prep(log_changes = TRUE)\n#&gt; step_zv (zv_RLYwH): same number of columns\n#&gt; \n#&gt; step_orderNorm (orderNorm_Jx8oD): same number of columns\n#&gt; \n#&gt; step_normalize (normalize_GU75D): same number of columns\n\nBaking the recipe\n使用bake()方法处理recipe对象，就像在模型中使用predict()方法一样；即，从训练集估算出的操作会被应用到任何数据上，比如测试数据或预测时的新数据。例如，可以对验证集样本进行处理：\n\nbean_val_processed &lt;- bake(bean_rec_trained, new_data = bean_validation)\n\nFigure 4 展示了配方制备前后面积预测器的直方图。\n\nlibrary(patchwork)\np1 &lt;-\n  bean_validation %&gt;%\n  ggplot(aes(x = area)) +\n  geom_histogram(bins = 30, color = \"white\", fill = \"blue\", alpha = 1 / 3) +\n  ggtitle(\"Original validation set data\")\n\np2 &lt;-\n  bean_val_processed %&gt;%\n  ggplot(aes(x = area)) +\n  geom_histogram(bins = 30, color = \"white\", fill = \"red\", alpha = 1 / 3) +\n  ggtitle(\"Processed validation set data\")\n\np1 + p2\n\n\n\n\n\n\nFigure 4: The area predictor before and after preprocessing\n\n\n\n\n这里有两个值得注意的bake()重要方面。\n首先，如前所述，使用prep(recipe, retain = TRUE)会保留配方中已处理的训练集版本。这使得用户能够调用bake(recipe, new_data = NULL)，从而直接返回该数据集，而无需进一步计算。例如：\n\nbake(bean_rec_trained, new_data = NULL) %&gt;% nrow()\n#&gt; [1] 10206\nbean_train %&gt;% nrow()\n#&gt; [1] 10206\n\n如果训练集规模并不庞大，使用这个保留值可以节省大量计算时间。\n其次，可以在调用中使用额外的选择器，以指定要返回哪些列。默认选择器是everything()，但也可使用更具体的指令。\n在下一节中，我们将使用prep()和bake()来演示其中的一些选项。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#feature-extraction-techniques",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#feature-extraction-techniques",
    "title": "16 Dimensionality Reduction",
    "section": "Feature Extraction Techniques",
    "text": "Feature Extraction Techniques\n由于在tidymodels中，recipe对象是进行降维的首选方法，因此我们来编写一个函数，用于估计变换，并通过ggforce包绘制出结果数据的散点图矩阵：\n\nlibrary(ggforce)\nplot_validation_results &lt;- function(recipe, dat = bean_validation) {\n  recipe %&gt;%\n    # Estimate any additional steps\n    prep() %&gt;%\n    # Process the data (the validation set by default)\n    bake(new_data = dat) %&gt;%\n    # Create the scatterplot matrix\n    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-class), layer.diag = 2) +\n    scale_color_brewer(palette = \"Dark2\") +\n    scale_fill_brewer(palette = \"Dark2\")\n}\n\n在本章中，我们将多次使用此函数。\n这里探讨了一系列特征提取方法。关于这些方法的概述，可参见 M. Kuhn 和 Johnson（2020）第6.3.1节及其参考文献。UMAP方法则在McInnes、Healy 和 Melville（2020）中有所介绍。\nPrincipal component analysis\n我们已经在本书中多次提到过PCA，现在是时候深入探讨了。PCA是一种无监督方法，它通过预测变量的线性组合来定义新特征。这些特征力求尽可能地捕捉原始数据中的变异信息。我们将在原始流程中加入step_pca()函数，并利用该函数在验证集上可视化结果，如 Figure 5 所示：\n\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"Principal Component Analysis\")\n\n\n\n\n\n\nFigure 5: Principal component scores for the bean validation set, colored by class\n\n\n\n\n我们发现，前两个主成分PC1和PC2，尤其是当它们结合使用时，能够有效地区分或分离各个类别。这或许使我们预期，整体上对这些豆类进行分类的问题并不会特别棘手。\n请记住，PCA 是无监督的。对于这些数据，结果表明，能够解释预测变量中最大变异性的 PCA 成分，恰好也能用于预测类别。那么，究竟是哪些特征在推动模型表现呢？learnedtidymodels 包中提供了有助于可视化每个成分顶部特征的函数，见 Figure 6 。为此，我们需要预先准备好的recipe对象；PCA 步骤已添加到以下代码中，并同时调用了prep()函数：\n\nlibrary(learntidymodels)\n#&gt; Loading required package: tidyverse\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ lubridate 1.9.4     ✔ tibble    3.2.1\n#&gt; ✔ readr     2.1.5     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ readr::col_factor() masks scales::col_factor()\n#&gt; ✖ purrr::discard()    masks scales::discard()\n#&gt; ✖ dplyr::filter()     masks stats::filter()\n#&gt; ✖ stringr::fixed()    masks recipes::fixed()\n#&gt; ✖ dplyr::lag()        masks stats::lag()\n#&gt; ✖ readr::spec()       masks yardstick::spec()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  prep() %&gt;%\n  plot_top_loadings(component_number &lt;= 4, n = 5) +\n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Principal Component Analysis\")\n\n\n\n\n\n\nFigure 6: Predictor loadings for the PCA transformation\n\n\n\n\n最主要的加载项大多与先前相关性图左上角所示的一组相关预测因子有关：周长、面积、主轴长度和凸包面积。这些指标均与豆子的大小密切相关。此外，根据 Symons 和 Fulcher（1988）提出的“shape factor 2”，即面积除以主轴长度的立方，同样与豆子大小相关。而伸长率的度量似乎主导了第二主成分。\nPartial least squares\n我们在第13.5.1节中介绍的PLS，是PCA的一种有监督版本。它试图找到既能最大程度地反映预测变量的变异，又能最大化这些成分与结果之间关系的主成分。Figure 7 展示了这一经过略微修改的PCA代码的运行结果：\n\nbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"Partial Least Squares\")\n\n\n\n\n\n\nFigure 7: PLS component scores for the bean validation set, colored by class\n\n\n\n\nFigure 7 中绘制的前两个PLS成分与前两个PCA成分几乎完全相同！我们得出这一结论，是因为这些PCA成分在区分不同种类的豆子时表现得极为出色。而其余的成分则有所不同。Figure 8 则直观展示了各成分的载荷，即每种成分的首要特征。\n\nbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  prep() %&gt;%\n  plot_top_loadings(component_number &lt;= 4, n = 5, type = \"pls\") +\n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Partial Least Squares\")\n\n\n\n\n\n\nFigure 8: Predictor loadings for the PLS transformation\n\n\n\n\n坚实度（即豆子的密度）与圆润度共同决定了第三个PLS成分。其中，坚实度可能捕捉到了与豆子表面“凹凸不平”相关的特征，因为它能够精确测量豆粒边界的不规则程度。\nIndependent component analysis\nICA与PCA略有不同，因为它寻找的是彼此间尽可能统计独立的成分（而非仅仅不相关）。可以说，ICA旨在最大化ICA成分的“非高斯性”，或者说是分离信息，而非像PCA那样通过压缩信息来实现降维。现在，让我们使用step_ica()函数生成 Figure 9 ：\n\nbean_rec_trained %&gt;%\n  step_ica(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"Independent Component Analysis\")\n\n\n\n\n\n\nFigure 9: ICA component scores for the bean validation set, colored by class\n\n\n\n\n观察这一图谱，使用ICA时，前几个主成分中各类别之间似乎没有明显分离。这些独立（或尽可能独立）的成分并未有效区分不同类型的豆子。\nUniform manifold approximation and projection\nUMAP 与流行的非线性降维方法 t-SNE 类似。在原始的高维空间中，UMAP 采用基于距离的最近邻方法，寻找数据中那些点之间更可能存在关联的局部区域。这些数据点之间的关系被保存为一个有向图模型，其中大部分点之间并不相连。\n随后，UMAP 将图中的点映射到降维后的空间中。为此，该算法采用了一种优化过程，利用交叉熵将数据点映射到更少数量的特征上，从而实现对原始图的精确近似。\n为了创建映射，embed 包中包含了一个用于该方法的步骤函数，如 Figure 10 所示。\n\nlibrary(embed)\nbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP\")\n\n\n\n\n\n\nFigure 10: UMAP component scores for the bean validation set, colored by class\n\n\n\n\n尽管类间空间明显存在，但每个簇内可能包含多种不同的类别。\n还有一种受监督的UMAP版本：\n\nbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP (supervised)\")\n\n\n\n\n\n\nFigure 11: Supervised UMAP component scores for the bean validation set, colored by class\n\n\n\n\nFigure 11 中展示的监督方法在数据建模方面前景可观。\nUMAP 是一种强大的降维方法，但它对调参非常敏感（例如，邻域数量等参数）。因此，建议尝试调整几个参数，以评估结果在这些数据上的稳健性。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#modeling",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#modeling",
    "title": "16 Dimensionality Reduction",
    "section": "Modeling",
    "text": "Modeling\nPLS 和 UMAP 方法都值得结合不同模型进行深入研究。让我们借助这些降维技术（以及完全不进行任何变换的情况），探索多种不同的模型：单层神经网络、Bagged 树、柔性判别分析（FDA）、朴素贝叶斯，以及正则化判别分析（RDA）。\n现在我们已重新进入“建模模式”，将创建一系列模型规范，然后使用工作流集在以下代码中调优这些模型。请注意，模型参数是与配方参数协同调整的（例如，降维后的维度大小、UMAP 参数）。\n\nlibrary(baguette)\nlibrary(discrim)\n#&gt; Warning: package 'discrim' was built under R version 4.4.3\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\nbagging_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nfda_spec &lt;-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %&gt;%\n  set_engine(\"earth\")\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine(\"klaR\")\n\nbayes_spec &lt;-\n  naive_Bayes() %&gt;%\n  set_engine(\"klaR\")\n\n我们还需要用于尝试的降维方法的配方。让我们从基础recipe对象bean_rec开始，然后逐步添加不同的降维步骤：\n\nbean_rec &lt;-\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npls_rec &lt;-\n  bean_rec %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = tune())\n\numap_rec &lt;-\n  bean_rec %&gt;%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n\n再次，workflowsets 包将预处理器与模型进行交叉组合。同时，控制control选项parallel_over被设置为启用并行处理，以便能够同时对多种调参组合展开优化。workflow_map()函数则针对 10 种参数组合，应用网格搜索法来优化模型及预处理参数（如适用）。最后，在验证集上估算多分类 ROC 曲线下的面积。\n\nctrl &lt;- control_grid(parallel_over = \"everything\")\nbean_res &lt;-\n  workflow_set(\n    preproc = list(basic = class ~ ., pls = pls_rec, umap = umap_rec),\n    models = list(\n      bayes = bayes_spec, fda = fda_spec,\n      rda = rda_spec, bag = bagging_spec,\n      mlp = mlp_spec\n    )\n  ) %&gt;%\n  workflow_map(\n    verbose = TRUE,\n    seed = 1603,\n    resamples = bean_val,\n    grid = 10,\n    metrics = metric_set(roc_auc),\n    control = ctrl\n  )\n\n我们可以通过模型在验证集上估计的ROC曲线下的面积对其进行排序，如 Figure 12 ：\n\nrankings &lt;-\n  rank_results(bean_res, select_best = TRUE) %&gt;%\n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1]))\n\ntidymodels_prefer()\nfilter(rankings, rank &lt;= 5) %&gt;% dplyr::select(rank, mean, model, method)\n#&gt; # A tibble: 5 × 4\n#&gt;    rank  mean model               method\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt; \n#&gt; 1     1 0.997 mlp                 pls   \n#&gt; 2     2 0.996 discrim_regularized pls   \n#&gt; 3     3 0.995 discrim_flexible    basic \n#&gt; 4     4 0.995 mlp                 basic \n#&gt; 5     5 0.995 naive_Bayes         pls\n\n\n\n\n\n\n\n\nFigure 12: Area under the ROC curve from the validation set\n\n\n\n\n从这些结果可以看出，大多数模型的表现都非常出色；这里几乎没有糟糕的选择。为了演示，我们将使用结合了PLS特征的RDA模型作为最终模型。接下来，我们将以数值上最优的参数确定工作流程，先将其拟合到训练集，再用测试集进行评估：\n\nrda_res &lt;-\n  bean_res %&gt;%\n  extract_workflow(\"pls_rda\") %&gt;%\n  finalize_workflow(\n    bean_res %&gt;%\n      extract_workflow_set_result(\"pls_rda\") %&gt;%\n      select_best(metric = \"roc_auc\")\n  ) %&gt;%\n  last_fit(split = bean_split, metrics = metric_set(roc_auc))\n\nrda_wflow_fit &lt;- extract_workflow(rda_res)\n\n我们在测试集上该指标（多分类ROC AUC）的结果是什么？\n\ncollect_metrics(rda_res)\n#&gt; # A tibble: 1 × 4\n#&gt;   .metric .estimator .estimate .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 roc_auc hand_till      0.995 pre0_mod0_post0\n\n很不错！我们将在下一章中使用这个模型，来展示变量重要性分析方法。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/16 Dimensionality Reduction.html#chapter-summary",
    "title": "16 Dimensionality Reduction",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n降维方法既可用于探索性数据分析，也可用于建模。recipes和embed包中包含了多种不同方法的步骤，而工作流集则有助于根据数据集的特点选择合适的方法。本章还探讨了如何单独使用recipes对象，无论是用于调试recipe对象中的问题，还是直接应用于探索性数据分析与数据可视化。",
    "crumbs": [
      "16 Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html",
    "title": "17 Encoding Categorical Data",
    "section": "",
    "text": "在R中进行统计建模时，分类或名称数据的首选表示方法是因子（factor），这是一种只能取有限个不同值的变量；在内部，因子以整数向量的形式存储，并附带一组文本标签。在第8.4.1节中，我们介绍了特征工程方法，用于将定性或名称数据编码或转换为更适合大多数模型算法的表示形式。我们讨论了如何将一个分类变量（例如Ames住房数据中的Bldg_Type，其水平包括OneFam、TwoFmCon、Duplex、Twnhs和TwnhsE）转换为一组虚拟变量或指示变量，如 Table 1 所示。\n许多模型实现都需要将分类数据转换为数值表示形式。附录A列出了适用于不同模型的推荐预处理技术表；请注意，表中许多模型均要求对所有预测变量进行数值编码。\n然而，对于一些实际的数据集，直接使用虚拟变量并不适用。这种情况通常是因为类别数量过多，或者在预测时出现了新类别。本章将探讨更复杂的分类预测变量编码方法，以解决这些问题。这些方法可通过 tidymodels 的 recipe 步骤，在embed和textrecipes包中获得。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#is-an-encoding-necessary",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#is-an-encoding-necessary",
    "title": "17 Encoding Categorical Data",
    "section": "Is an Encoding Necessary?",
    "text": "Is an Encoding Necessary?\n少数模型，例如基于树或规则的模型，能够原生处理分类数据，无需对这类特征进行编码或转换。例如，基于树的模型可直接将Bldg_Type这样的变量划分为若干个因子水平组——比如，将OneFam单独归为一组，而Duplex和Twnhs则合并为另一组。此外，朴素贝叶斯模型也是另一种典型例子：其模型结构本身就能自然地应对分类变量，具体而言，会在每个水平内分别计算概率分布，如针对数据集中所有不同类型的Bldg_Type。\n这些能够原生处理分类特征的模型，同样也能应对数值型、连续型特征，因此对这类变量的转换或编码变得可有可无。但这是否会在某种程度上帮助提升模型性能或缩短训练时间呢？通常情况下，并非如此——正如 M. Kuhn 和 Johnson（2020年）书中第5.7节所指出的，使用未经转换的因子变量与针对相同特征转换为虚拟变量后进行对比时，基准数据集的表现显示：采用虚拟编码方式不仅未显著改善模型性能，反而常常需要更长的训练时间。\n我们建议，在模型允许的情况下，优先使用未经过转换的分类变量；需要注意的是，对于这类模型，更复杂的编码方式通常并不会带来更好的性能。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#encoding-ordinal-predictors",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#encoding-ordinal-predictors",
    "title": "17 Encoding Categorical Data",
    "section": "Encoding Ordinal Predictors",
    "text": "Encoding Ordinal Predictors\n有时，定性列可以进行排序，例如“低”、“中”和“高”。在 base R 中，默认的编码策略是创建新的数值列，这些列是数据的多项式展开。对于具有五个有序值的列——如 Table 2 所示的例子——因子列会被替换为线性、二次、三次和四次项的列：\n\n\nTable 2: Polynominal expansions for encoding an ordered variable.\n\n\n\n\n\n\n\n\n\n\nRaw Data\nLinear\nQuadratic\nCubic\nQuartic\n\n\n\nnone\n-0.63\n0.53\n-0.32\n0.12\n\n\na little\n-0.32\n-0.27\n0.63\n-0.48\n\n\nsome\n0.00\n-0.53\n0.00\n0.72\n\n\na bunch\n0.32\n-0.27\n-0.63\n-0.48\n\n\ncopious amounts\n0.63\n0.53\n0.32\n0.12\n\n\n\n\n\n\n虽然这种做法并非毫无道理，但人们通常并不觉得它有用。例如，用一个11次多项式来编码一年中各个月份的顺序型因子，可能并不是最有效的方法。相反，不妨尝试使用与有序因子相关的recipe步骤，比如step_unorder()，将其转换为普通因子；或者使用step_ordinalscore()，将特定的数值映射到每个因子水平。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#using-the-outcome-for-encoding-predictors",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#using-the-outcome-for-encoding-predictors",
    "title": "17 Encoding Categorical Data",
    "section": "Using the Outcome for Encoding Predictors",
    "text": "Using the Outcome for Encoding Predictors\n除了虚拟变量或指示变量之外，还有多种更复杂的编码方式可供选择。其中一种方法称为效应编码（effect encoding）或似然编码（likelihood encoding），它用单个数值列取代了原有的分类变量，该数值列用于衡量这些数据的效应（Micci-Barreca 2001；Zumel 和 Mount 2019）。例如，在Ames住宅数据中，针对街区这一预测变量，我们可以计算每个街区的平均或中位数销售价格（如 Figure 1 所示），并用这些均值替换原始数据值：\n\nlibrary(tidymodels)\n#&gt; ── Attaching packages ─────────────────────────────────── tidymodels 1.4.1 ──\n#&gt; ✔ broom        1.0.9     ✔ recipes      1.3.1\n#&gt; ✔ dials        1.4.2     ✔ rsample      1.3.1\n#&gt; ✔ dplyr        1.1.4     ✔ tailor       0.1.0\n#&gt; ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n#&gt; ✔ infer        1.0.9     ✔ tune         2.0.0\n#&gt; ✔ modeldata    1.5.1     ✔ workflows    1.3.0\n#&gt; ✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n#&gt; ✔ purrr        1.1.0     ✔ yardstick    1.3.2\n#&gt; ── Conflicts ────────────────────────────────────── tidymodels_conflicts() ──\n#&gt; ✖ purrr::discard() masks scales::discard()\n#&gt; ✖ dplyr::filter()  masks stats::filter()\n#&gt; ✖ dplyr::lag()     masks stats::lag()\n#&gt; ✖ recipes::step()  masks stats::step()\ndata(ames)\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\n\names_train %&gt;%\n  group_by(Neighborhood) %&gt;%\n  summarize(\n    mean = mean(Sale_Price),\n    std_err = sd(Sale_Price) / sqrt(length(Sale_Price))\n  ) %&gt;%\n  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +\n  labs(y = NULL, x = \"Price (mean, log scale)\")\n\n\n\n\n\n\nFigure 1: Mean home price for neighborhoods in the Ames training set, which can be used as an effect encoding for this categorical variable\n\n\n\n\n当你的分类变量具有大量水平时，这种效应编码方法效果尤为出色。在tidymodels中，embed包提供了多种用于不同效应编码的recipe步骤函数，例如step_lencode_glm()、step_lencode_mixed()和step_lencode_bayes()。这些步骤利用广义线性模型来估计分类预测变量中每个水平对结果的影响。当你使用像step_lencode_glm()这样的recipe步骤时，需先指定要编码的变量，再通过vars()指明结果变量：\n\nlibrary(embed)\n\names_glm &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_glm\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Linear embedding for factors via GLM for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude Longitude\n\n正如第16.4节所详述，我们可以使用训练数据对recipe进行prep()处理，以调整或估计预处理转换所需的参数。随后，我们可利用tidy()方法整理这一已准备好的recipe，查看最终结果：\n\nglm_estimates &lt;-\n  prep(ames_glm) %&gt;%\n  tidy(number = 2)\n\nglm_estimates\n#&gt; # A tibble: 29 × 4\n#&gt;   level              value terms        id               \n#&gt;   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n#&gt; 1 North_Ames          5.15 Neighborhood lencode_glm_yj20u\n#&gt; 2 College_Creek       5.29 Neighborhood lencode_glm_yj20u\n#&gt; 3 Old_Town            5.07 Neighborhood lencode_glm_yj20u\n#&gt; 4 Edwards             5.09 Neighborhood lencode_glm_yj20u\n#&gt; 5 Somerset            5.35 Neighborhood lencode_glm_yj20u\n#&gt; 6 Northridge_Heights  5.49 Neighborhood lencode_glm_yj20u\n#&gt; # ℹ 23 more rows\n\n当我们使用通过这种方法创建的全新编码的Neighborhood数值变量时，会用GLM模型中对Sale_Price的估计值来替换原始水平（例如North_Ames）。\n像这种效应编码方法，也能无缝应对数据中遇到新因子水平的情况。当我们在缺乏特定小区信息时，该值value正是我们从GLM模型预测出的价格：\n\nglm_estimates %&gt;%\n  filter(level == \"..new\")\n#&gt; # A tibble: 1 × 4\n#&gt;   level value terms        id               \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n#&gt; 1 ..new  5.23 Neighborhood lencode_glm_yj20u\n\n效应编码功能强大，但应谨慎使用。这些效应应在数据拆分后，基于训练集进行计算。这种有监督的预处理步骤需严格采用重采样方法，以避免过拟合（参见第10章）。\n当你为分类变量创建效应编码时，实际上是在你的实际模型内部叠加了一个小型模型。效应编码存在过拟合的风险，这正是第7章中所阐述的：特征工程必须被视为模型流程的一部分；同时，特征工程也应与模型参数一起，在重采样过程中一并进行估计。\nEffect encodings with partial pooling\n使用step_lencode_glm()创建效应编码时，会针对每个因子水平（本例中为“街区”）分别估计效应。然而，有些街区的房屋数量较多，而另一些则只有少数几栋。因此，在“地标”街区的单套训练集房屋价格测量中，不确定性远高于北Ames街区的354套训练集房屋。我们可以通过部分分层法对这些估计值进行调整，使样本量较小的水平更接近整体均值。具体而言，各水平的效应将通过混合或分层广义线性模型一次性建模：\n\names_mixed &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_mixed\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Linear embedding for factors via mixed effects for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude Longitude\n\n让我们对这个recipe对象进行prep()和tidy()处理，看看效果：\n\nmixed_estimates &lt;-\n  prep(ames_mixed) %&gt;%\n  tidy(number = 2)\n\nmixed_estimates\n#&gt; # A tibble: 29 × 4\n#&gt;   level              value terms        id                 \n#&gt;   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n#&gt; 1 North_Ames          5.15 Neighborhood lencode_mixed_AmBz1\n#&gt; 2 College_Creek       5.29 Neighborhood lencode_mixed_AmBz1\n#&gt; 3 Old_Town            5.07 Neighborhood lencode_mixed_AmBz1\n#&gt; 4 Edwards             5.10 Neighborhood lencode_mixed_AmBz1\n#&gt; 5 Somerset            5.35 Neighborhood lencode_mixed_AmBz1\n#&gt; 6 Northridge_Heights  5.49 Neighborhood lencode_mixed_AmBz1\n#&gt; # ℹ 23 more rows\n\n随后，新等级的编码值与GLM几乎相同：\n\nmixed_estimates %&gt;%\n  filter(level == \"..new\")\n#&gt; # A tibble: 1 × 4\n#&gt;   level value terms        id                 \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n#&gt; 1 ..new  5.23 Neighborhood lencode_mixed_AmBz1\n\n你可以使用完全贝叶斯层次模型来以相同的方式对效应进行建模，方法是调用step_lencode_bayes()。\n让我们在 Figure 2 中通过可视化方式比较部分聚合与无聚合的效果：\n\nglm_estimates %&gt;%\n  rename(`no pooling` = value) %&gt;%\n  left_join(\n    mixed_estimates %&gt;%\n      rename(`partial pooling` = value),\n    by = \"level\"\n  ) %&gt;%\n  left_join(\n    ames_train %&gt;%\n      count(Neighborhood) %&gt;%\n      mutate(level = as.character(Neighborhood))\n  ) %&gt;%\n  ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  geom_point(alpha = 0.7) +\n  coord_fixed()\n#&gt; Joining with `by = join_by(level)`\n#&gt; Warning: Removed 1 row containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\n\n\n\nFigure 2: Comparing the effect encodings for neighborhood estimated without pooling to those with partial pooling\n\n\n\n\n请注意，如 Figure 2 所示，当我们比较整体聚合与完全不聚合时，大多数关于邻里效应的估计值基本一致。然而，那些房屋数量最少的社区，其效应估计值已被拉向（向上或向下）平均效应水平。这是因为采用整体聚合方法时，我们对这些社区房价的证据相对较少，因而会将效应估计值向均值靠拢。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#feature-hashing",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#feature-hashing",
    "title": "17 Encoding Categorical Data",
    "section": "Feature Hashing",
    "text": "Feature Hashing\n传统虚拟变量如第8.4.1节所述，要求必须已知所有可能的类别，才能构建出完整的数值特征集。而特征哈希方法（Weinberger等，2009）同样会创建虚拟变量，但仅根据类别值将其分配到预定义的虚拟变量池中。让我们再次查看Ames数据集中的Neighborhood字段，并使用rlang::hash()函数来进一步理解：\n\nlibrary(rlang)\n#&gt; \n#&gt; Attaching package: 'rlang'\n#&gt; The following objects are masked from 'package:purrr':\n#&gt; \n#&gt;     %@%, flatten, flatten_chr, flatten_dbl, flatten_int,\n#&gt;     flatten_lgl, flatten_raw, invoke, splice\n\names_hashed &lt;-\n  ames_train %&gt;%\n  mutate(Hash = map_chr(Neighborhood, hash))\n\names_hashed %&gt;%\n  select(Neighborhood, Hash)\n#&gt; # A tibble: 2,342 × 2\n#&gt;   Neighborhood    Hash                            \n#&gt;   &lt;fct&gt;           &lt;chr&gt;                           \n#&gt; 1 North_Ames      076543f71313e522efe157944169d919\n#&gt; 2 North_Ames      076543f71313e522efe157944169d919\n#&gt; 3 Briardale       b598bec306983e3e68a3118952df8cf0\n#&gt; 4 Briardale       b598bec306983e3e68a3118952df8cf0\n#&gt; 5 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n#&gt; 6 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n#&gt; # ℹ 2,336 more rows\n\n如果我们把“Briardale”输入这个哈希函数，每次都会得到相同的输出。在这种情况下，这些街区被称为“keys”，而输出结果则称为“hashes”。哈希函数将大小可变的输入映射为固定大小的输出。哈希函数广泛应用于密码学和数据库领域。\nrlang::hash()函数生成一个128位的哈希值，这意味着共有2^128种可能的哈希值。这对于某些应用来说非常理想，但并不适用于高基数变量（即具有大量水平的变量）的特征哈希。在特征哈希中，可能的哈希数量是一个超参数，由模型开发者通过计算整数哈希值的模来设定。例如，我们可以通过 Hash %% 16 获得16种可能的哈希值：\n\names_hashed %&gt;%\n  ## first make a smaller hash for integers that R can handle\n  mutate(\n    Hash = strtoi(substr(Hash, 26, 32), base = 16L),\n    ## now take the modulo\n    Hash = Hash %% 16\n  ) %&gt;%\n  select(Neighborhood, Hash)\n#&gt; # A tibble: 2,342 × 2\n#&gt;   Neighborhood     Hash\n#&gt;   &lt;fct&gt;           &lt;dbl&gt;\n#&gt; 1 North_Ames          9\n#&gt; 2 North_Ames          9\n#&gt; 3 Briardale           0\n#&gt; 4 Briardale           0\n#&gt; 5 Northpark_Villa     4\n#&gt; 6 Northpark_Villa     4\n#&gt; # ℹ 2,336 more rows\n\n现在，我们不再使用原始数据中的28个街区，也不再处理数量极其庞大的原始哈希值，而是仅保留了16个哈希值。这种方法速度非常快，且内存效率高，尤其在可能类别数量众多时，不失为一种不错的策略。\n特征哈希法不仅适用于文本数据，也适用于高基数的类别型数据。有关使用文本预测变量的案例研究演示，请参阅Hvitfeldt和Silge（2021）第6.7节。\n我们可以使用来自textrecipes包的recipe步骤来实现特征哈希：\n\nlibrary(textrecipes)\names_hash &lt;-\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +\n    Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;%\n  step_dummy_hash(Neighborhood, signed = FALSE, num_terms = 16L) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;%\n  step_ns(Latitude, Longitude, deg_free = 20)\n#&gt; 1 package (text2vec) is needed for this step but is not installed.\n#&gt; To install run: `install.packages(\"text2vec\")`\n\names_hash\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Feature hashing with: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude Longitude\n\n特征哈希法速度快、效率高，但也存在一些缺点。例如，不同的类别值常常会被映射到同一个哈希值上，这被称为冲突或别名现象。那么，在Ames的各个社区中，这种情况究竟有多频繁呢？Table 3 展示了每个哈希值对应的社区数量分布情况。\n\n\nTable 3: The number of hash features at each number of neighborhoods.\n\n\n\n\n\n\n\nNumber of neighborhoods within a hash feature\nNumber of occurrences\n\n\n\n0\n1\n\n\n1\n7\n\n\n2\n4\n\n\n3\n3\n\n\n4\n1\n\n\n\n\n\n\n映射到每个哈希值的街区数量介于零到四之间。所有大于一的哈希值均属于哈希冲突的实例。\n使用特征哈希时，有哪些需要考虑的事项？\n\n特征哈希无法直接解释，因为哈希函数不可逆。我们无法从哈希值推断出输入的类别级别，也无法判断是否发生了冲突。\n哈希值的数量是这种预处理技术的一个调优参数，您应尝试多个数值，以确定哪种设置最适合您的特定建模方法。哈希值数量较少时会导致更多冲突，但若数量过高，可能并不会比您原本高基数变量的表现更好。\n特征哈希能够在预测时处理新的类别级别，因为它不依赖于预先确定的虚拟变量。\n你可以通过设置signed = TRUE，来减少有符号哈希中的哈希冲突。这样，哈希值将从原来的 1 扩展为根据哈希符号决定的 +1 或 -1。\n\n很可能会有一些哈希列包含全零值，正如我们在本例中所看到的。我们建议使用step_zv()函数实施零方差过滤，以剔除这些列。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#more-encoding-options",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#more-encoding-options",
    "title": "17 Encoding Categorical Data",
    "section": "More Encoding Options",
    "text": "More Encoding Options\n还有更多选项可用于将因子转换为数值表示。\n我们可以构建一套完整的实体嵌入（entity embeddings，Guo 和 Berkhahn，2016），将具有多个类别的分类变量转换为一组低维向量。这种方法尤其适用于类别数量众多的名称变量——其类别数远超我们在Ames市社区示例中所使用的数量。实体嵌入的概念源自用于从文本数据中创建词嵌入的方法。更多关于词嵌入的内容，请参阅Hvitfeldt和Silge（2021）第5章。\n可以通过使用embed包中的step_embed()函数，借助TensorFlow神经网络来学习分类变量的嵌入表示。我们可以仅使用目标变量，也可选择性地结合一组额外的预测因子。与特征哈希类似，需创建的新编码列数量是特征工程的一个超参数。此外，我们还需决定神经网络的结构（隐藏单元的数量）以及如何训练该神经网络（训练多少个epoch，以及在评估指标时应使用多少数据进行验证）。\n另一种可用于处理二元结果的选项是，根据类别水平与二元结果的关联程度对其进行转换。这种基于证据权重（WoE）的转换方法（Good，1985）利用“贝叶斯因子”的对数（即后验比值与先验比值之比），并建立一个字典，将每个类别水平映射到相应的WoE值。WoE编码可通过embed中的step_woe()函数来确定。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#chapter-summary",
    "href": "Books/Tidy Modeling with R/17 Encoding Categorical Data.html#chapter-summary",
    "title": "17 Encoding Categorical Data",
    "section": "Chapter Summary",
    "text": "Chapter Summary\n在本章中，你学习了如何使用预处理方案对分类预测变量进行编码。将分类变量转换为数值表示的最直接方法是根据其水平创建虚拟变量，但当变量的基数较高（水平数量过多）或在预测时可能遇到新值（出现新水平）时，这种方法的效果并不理想。在这种情况下，一种选择是采用效应编码，这是一种基于监督的学习编码方法，它会利用目标变量的信息。效应编码既可以单独学习，也可以通过类别聚合的方式进行学习。此外，还有一种方法是使用哈希函数，将类别映射到一组更小的新虚拟变量上。特征哈希法速度快、内存占用低。其他可选方法还包括：通过神经网络学习得到的实体嵌入，以及证据权重转换法。\n大多数模型算法都需要对分类变量进行某种形式的转换或编码。而少数模型，包括基于树和规则的模型，则能直接处理分类变量，无需此类编码。",
    "crumbs": [
      "17 Encoding Categorical Data"
    ]
  },
  {
    "objectID": "Blog/Nextclade.html",
    "href": "Blog/Nextclade.html",
    "title": "Nextclade",
    "section": "",
    "text": "官方手册\n主要分两种方式使用："
  },
  {
    "objectID": "Blog/Nextclade.html#nextstrain-与-nextclade",
    "href": "Blog/Nextclade.html#nextstrain-与-nextclade",
    "title": "Nextclade",
    "section": "Nextstrain 与 Nextclade",
    "text": "Nextstrain 与 Nextclade\nNextstrain 项目提供了一套对数据进行处理，分析，可视化的开源软件工具，Nextclade 的分析基于这些工具，例如：nextclade 使用的输入数据集“dataset”是由 Augur 工具生成，输出结果的可视化由 Auspice 工具支持。\nAugur 和 Auspice\nAugur 与 Auspice 的关系如下：\n\n\n\n\n\ngraph LR\n  A[/equences.fasta/] --&gt; B(filter)\n  C[/metadata.tsv/] --&gt; B\n  subgraph Augur\n  B --&gt; D(align)\n  D --&gt; E(tree)\n  E --&gt; F(refine)\n  F --&gt; G(export)\n  end\n  G --&gt; H[(Dataset)]\n  H --&gt; I[Auspice]\n\n  classDef input fill:#d3d3d3,stroke:black\n  classDef Tool fill:#a6cee3,stroke:#1f78b4\n  classDef tool fill:white,stroke:#1f78b4\n\n  class A,C input\n  class B,D,E,F,G tool\n  class Augur,I Tool\n\n\n\n\n\n\nnextclade 工作流程\n\n将查询序列与参考序列进行比对\n检测突变\n根据突变进行基于“参考树”的分支\n将查询序列整合到“参考树”中"
  },
  {
    "objectID": "Blog/Nextclade.html#nextclade-cli",
    "href": "Blog/Nextclade.html#nextclade-cli",
    "title": "Nextclade",
    "section": "Nextclade CLI",
    "text": "Nextclade CLI\n安装\n有四种安装方式，暂时先使用docker镜像，进行初步研究。\n\nStandalone download\nWith Docker\nWith Conda\nBuild from source\n\ndocker\n下载方式：\ndocker pull nextstrain/nextclade:latest\ndocker run -it --rm nextstrain/nextclade:latest nextclade --help\n使用示例：\ndocker run -it --rm \\\n  --volume=\"$(pwd):/data/\" \\\n  --user=\"$(id -u):$(id -g)\" \\\n  \"nextstrain/nextclade\" \\\n  nextclade run \\\n      --dataset-name=\"sars-cov-2\" \\\n      --output-dir=\"/data/output/\" \\\n      \"/data/my_sequences.fasta\"\n\ndocker run -it --name nextclade -v /home/hgj/D/NextClade/Test:/root/data nextstrain/nextclade\nCLI 使用方法\nnextclade [OPTIONS] &lt;COMMAND&gt;\n主要的command有：\n\n\nrun：运行 Nextclade\n\ndataset：获取数据集信息\n\nschema：生成json格式模板\n\n详细参数见，nextclade -h 或 nextclade &lt;command&gt; -h，及官方参考refrence。\n测试示例：\nnextclade run --input-dataset data/sars-cov-2 --output-all=output/ data/sars-cov-2/sequences.fasta\n输入--input-dataset与输出--output-all可以更细致地进行自定义。\nnextclade run \\\n   --verbose \\\n   --include-reference \\\n   --in-order \\\n   # 输入数据集控制\n   --input-dataset=data/sars-cov-2 \\\n   --input-ref=data/sars-cov-2/reference.fasta \\\n   --input-annotation=data/sars-cov-2/genome_annotation.gff3 \\\n   --cds-selection=E,M,N,ORF1a,ORF1b,ORF3a,ORF6,ORF7a,ORF7b,ORF8,ORF9b,S \\\n   --input-tree=data/sars-cov-2/tree.json \\\n   --input-pathogen-json=data/sars-cov-2/pathogen.json \\\n   # 输出数据控制\n   --output-fasta=output/nextclade.aligned.fasta.gz \\\n   --output-json=output/nextclade.json \\\n   --output-ndjson=output/nextclade.ndjson \\\n   --output-csv=output/nextclade.csv \\\n   --output-tsv=output/nextclade.tsv \\\n   --output-tree=output/nextclade.auspice.json \\\n   --output-tree-nwk=output/nextclade.tree.nwk \\\n   --output-translations=output/nextclade_CDS_{cds}.translation.fasta.zst \\\n   # 多种输入文件\n   data/sars-cov-2/sequences.fasta \\\n   my_sequences1.fasta.gz \\\n   my_sequences2.fasta.xz\n输入序列格式：fasta, gz, xz, bz2, zst，可以同时进行多个fasta文件分析。"
  },
  {
    "objectID": "Blog/Nextclade.html#nextclade-输入文件",
    "href": "Blog/Nextclade.html#nextclade-输入文件",
    "title": "Nextclade",
    "section": "Nextclade 输入文件",
    "text": "Nextclade 输入文件\n由两部分组成：\n\n查询序列，可以是多个文件，也可以是单个文件多条序列。\n由参考序列，参考树，注释文件等组成的“dataset”，可以是路径，也可以是压缩文件。\n\n查询序列\nfasta 格式，支持除U外的IUPAC characters，必须有“id”且以&gt;开头。\ndataset\n数据集“dataset”是nextclade运行时，针对输入序列所需要的一些底层文件。它至少要包含两个文件：\n\n\nreference.fasta：参考序列\n\npathogen.json：datasets 配置文件，类似身份证\n\n其他分析用文件：\n\n\ngenome_annotation.gff3：参考序列的注释文件，GFF3格式\n\ntree.json：参考树，Auspice JSON v2格式，可以使用 https://auspice.us/ 进行可视化\n\n及说明文件：\n\nREADME.md\nCHANGELOG.md\n\n测试数据：\n\nsequences.fasta\n\nnextstrain/nextclade_data是官方和社区维护的数据集。自己也可以创建数据集——dataset 自制手册 。\ndatasets 的 name 和 version\nname 由四级路径名称组成，例如nextstrain/sars-cov-2/wuhan-hu-1/orfs表示：由 Nextstrain 维护 的 SARS-CoV-2 ，使用 Wuhan-Hu-1/2019 (MN908947) 毒株作为参考序列，并且这个序列带有 ORFs（开放阅读框），而不是其他成熟蛋白注释信息。\nversion 和软件的版本概念一致，用来确保结果的可重复性；它是一个数据释放时间戳YYYY-MM-DDTHH:MM:SSZ。Nextclade Web 和 CLI 都默认使用最新的版本；Web 使用URL来选择版本，CLI可以直接选择。\n要保证结果能够复现，需要：\n\n保证 Nextclade CLI 版本一致\n保证 datasets 版本一致\n\n查看可用的datasets：nextclade dataset list --only-names\nnextstrain/sars-cov-2/wuhan-hu-1/orfs\nnextstrain/sars-cov-2/wuhan-hu-1/proteins\nnextstrain/sars-cov-2/BA.2.86\nnextstrain/flu/h1n1pdm/ha/CY121680\nnextstrain/flu/h1n1pdm/ha/MW626062\nnextstrain/flu/h1n1pdm/na/MW626056\nnextstrain/flu/h3n2/ha/CY163680\nnextstrain/flu/h3n2/ha/EPI1857216\nnextstrain/flu/h3n2/na/EPI1857215\nnextstrain/flu/vic/ha/KX058884\nnextstrain/flu/vic/na/CY073894\nnextstrain/flu/yam/ha/JN993010\nnextstrain/rsv/a/EPI_ISL_412866\nnextstrain/rsv/b/EPI_ISL_1653999\nnextstrain/mpox/all-clades\nnextstrain/mpox/clade-i\nnextstrain/mpox/clade-iib\nnextstrain/mpox/lineage-b.1\nnextstrain/orthoebolavirus/ebov\nnextstrain/orthoebolavirus/sudv\nnextstrain/flu/h3n2/pb1\nnextstrain/flu/h3n2/np\nnextstrain/flu/h1n1pdm/pa\nnextstrain/flu/h3n2/ns\nnextstrain/flu/h1n1pdm/mp\nnextstrain/flu/h1n1pdm/np\nnextstrain/flu/h1n1pdm/ns\nnextstrain/flu/h3n2/mp\nnextstrain/flu/h3n2/pa\nnextstrain/flu/h1n1pdm/pb2\nnextstrain/flu/h1n1pdm/pb1\nnextstrain/flu/h3n2/pb2\nnextstrain/measles/genome/WHO-2012\nnextstrain/measles/N450/WHO-2012\nnextstrain/dengue/all\nnextstrain/yellow-fever/prM-E\nnextstrain/hmpv/all-clades/NC_039199\nnextstrain/flu/vic/pa\nnextstrain/flu/vic/pb1\nnextstrain/flu/vic/np\nnextstrain/flu/vic/mp\nnextstrain/flu/vic/pb2\nnextstrain/flu/vic/ns\nnextstrain/rubella/E1\nnextstrain/herpes/vzv/NC_001348\nnextstrain/mumps/sh\nnextstrain/mumps/genome\nnextstrain/rubella/genome\nenpen/enterovirus/ev-d68\ncommunity/isuvdl/mazeller/prrsv1/orf5/yimim2025\ncommunity/isuvdl/mazeller/prrsv2/orf5/yimim2023\ncommunity/neherlab/hiv-1/hxb2\ncommunity/moncla-lab/iav-h5/ha/2.3.4.4\ncommunity/moncla-lab/iav-h5/ha/all-clades\ncommunity/moncla-lab/iav-h5/ha/2.3.2.1\ncommunity/v-gen-lab/dengue/denv1\ncommunity/v-gen-lab/dengue/denv2\ncommunity/v-gen-lab/dengue/denv3\ncommunity/v-gen-lab/dengue/denv4\ncommunity/genspectrum/marburg/HK1980/all-lineages\ncommunity/pathoplexus/cchfv/L\ncommunity/pathoplexus/cchfv/S\ncommunity/pathoplexus/cchfv/M\ncommunity/v-gen-lab/chikV/genotypes\n下载datasets：\nnextclade dataset get \\\n   --name 'nextstrain/sars-cov-2/wuhan-hu-1' \\\n   --tag '2021-06-25T00:00:00Z' \\\n   --output-dir 'data/sars-cov-2'\nReference sequence\n--input-ref/-r\n当参考序列是一个注释完善、广泛使用的高质量基因组（例如来自 RefSeq）时，能获得最佳结果，该基因组最好是完整且明确的（涵盖整个基因组，没有模糊的核苷酸）。\n参考序列最好不要有“gap”-\nGenome annotation\n--input-annotation/-m\nGFF3 格式，但有额外要求：\n\ngene 名唯一\nCDS 名唯一\nprotein 名唯一\n\n##gff-version 3\n##sequence-region   . 266 29533\n.   .   gene    26245   26472   .   +   .   gene_name=E\n.   .   gene    26523   27191   .   +   .   gene_name=M\n.   .   gene    28274   29533   .   +   .   gene_name=N\n.   .   gene    266 13468   .   +   .   gene_name=ORF1a\n.   .   gene    13468   21555   .   +   .   gene_name=ORF1b\n.   .   gene    25393   26220   .   +   .   gene_name=ORF3a\n.   .   gene    27202   27387   .   +   .   gene_name=ORF6\n.   .   gene    27394   27759   .   +   .   gene_name=ORF7a\n.   .   gene    27756   27887   .   +   .   gene_name=ORF7b\n.   .   gene    27894   28259   .   +   .   gene_name=ORF8\n.   .   gene    28284   28577   .   +   .   gene_name=ORF9b\n.   .   gene    21563   25384   .   +   .   gene_name=S\n注释文件与后续的密码子比对，CDS翻译分析有关，如果不提供该文件，则不会生成肽段序列文件和检测氨基酸突变。\nReference tree\n--input-tree/-a\n格式： Auspice JSON v2\n由 augur export 生产，使用 Auspice 进行可视化\nPathogen configuration\n--input-pathogen-json/-R\n使用 nextclade schema write --for input-pathogen-json 生成模板。\n要求 schemaVersion 格式，其中 files 字段是必须的，\n{\n  \"files\": {\n    \"reference\": \"reference.fasta\",\n    \"pathogenJson\": \"pathogen.json\",\n    \"genomeAnnotation\": \"genome_annotation.gff3\",\n    \"treeJson\": \"tree.json\",\n    \"examples\": \"sequences.fasta\",\n    \"readme\": \"README.md\",\n    \"changelog\": \"CHANGELOG.md\"\n  }\n}\n可选属性：\n\n\nattributes：name,refrence name等信息\n\nqc：有该字段时执行QC检查，具体参数见 Algorithm: Quality control。\n\ncompatibility：数据集要求的 nextclade 最低版本\n\ndefaultCds：\n\ncdsOrderPreference：\n\ngeneralParams：\n\nalignmentParams：\n\ntreeBuilderParams：\n\nphenotypeData：\n\naaMotifs：\n\nmutLabels："
  },
  {
    "objectID": "Blog/Nextclade.html#nextclade-输出文件",
    "href": "Blog/Nextclade.html#nextclade-输出文件",
    "title": "Nextclade",
    "section": "Nextclade 输出文件",
    "text": "Nextclade 输出文件\n以 nextclade run --input-dataset data/sars-cov-2 --output-all=output/ data/sars-cov-2/sequences.fasta 的结果为例，--output-all参数表示输出所有结果（包括不同格式），output/表示输出路径。输出结果可以分为五大部分：\n\nNucleotide alignment\nTranslations\nAnalysis results\nPhylogenetic tree\nGenome annotation of query sequences\n\nnextclade.aligned.fasta\n\nnextclade.cds_translation.ORF3a.fasta\nnextclade.cds_translation.ORF6.fasta\nnextclade.cds_translation.E.fasta\nnextclade.cds_translation.ORF7a.fasta\nnextclade.cds_translation.M.fasta\nnextclade.cds_translation.ORF7b.fasta\nnextclade.cds_translation.N.fasta\nnextclade.cds_translation.ORF8.fasta\nnextclade.cds_translation.ORF1a.fasta\nnextclade.cds_translation.ORF9b.fasta\nnextclade.cds_translation.ORF1b.fasta\nnextclade.cds_translation.S.fasta\n\nnextclade.tsv\nnextclade.csv\nnextclade.json\nnextclade.ndjson\n\nnextclade.auspice.json\nnextclade.nwk\n\nnextclade.gff\nnextclade.tbl\nNucleotide alignment\nnextclade.aligned.fasta\n--output-fasta/-o &lt;FILENAME&gt;\n由 sequence alignment 步骤生产，输出结果是 相较于参考序列的 fasta 格式文件，任何相较于参考序列的插入片段都被移除，这些片段可以在后面的 tabular 和 json 文件中找到。\nTranslations\n模板字符串：nextclade.cds_translation.{cds}.fasta\n--output-translations/-P &lt;TEMPLATE_STRING&gt;\n由 translation and peptide alignment 步骤生成，每个 CDS/gene 一个结果，格式同上。\nAnalysis results\n突变检测、进化分支分配、质量控制和 PCR 引物变化的结果有：表格格式（TSV、CSV）或 JSON 格式（经典 JSON 或 NDJSON）。\ntabular\nnextclade.tsv or nextclade.csv\n--output-tsv/-t or --output-csv/-c\n结果是“1-base”，位置采用参考序列坐标（插入序列会被移除），坐标范围是闭区间的。\n“seqName”列往往不唯一，需要通过“index”将输入与输出进行关联。\n存在有关clade的额外列，例如示例结果中的“Nextclade_pango”，它们来自于参考树中的meta.extensions.clade_node_attrs。\n推荐使用tsv格式，csv格式由于历史原因存在分隔符错乱的情况。\nJSON\nnextclade.json or nextclade.ndjson\n--output-json/-J or --output-ndjson/-N\nJSON 结果会比tabular格式的结果多。NDJSON适合大型数据分析。\n结果是“0-base”，位置采用参考序列坐标（插入序列会被移除），坐标范围左闭右开。\nPhylogenetic tree\nnextclade.auspice.json or nextclade.nwk\n--output-tree/-T or --output-tree-nwk\n由 phylogenetic placement 步骤生成 “Auspice JSON v2” 或 “Newick”格式\nGenome annotation of query sequences\n此项结果仍在实验中！！！\nnextclade.gff or nextclade.tbl\n--output-annotation-gff or --output-annotation-tbl\n注释结果来源于输入的GFF文件，但是注释的坐标参考查询序列。\n两种格式：Genbank的TBL格式，GFF3格式\nGFF文件中有seq_index信息，用来绑定查询序列，因为查询序列的“name”信息不唯一。\n添加--retry-reverse-complement会有额外的is_reverse_complement列。\n如果输入的GFF文件中CDS没有对应的gene，nextclade会在结果中创建虚拟的gene；同样的，也会创建虚拟的CDS。\nErrors and warnings\n格外要注意在tabular或json格式的结果中有“警告”或“错误”的序列，它们在其他结果中不存在。"
  },
  {
    "objectID": "Blog/Nextclade.html#algorithm",
    "href": "Blog/Nextclade.html#algorithm",
    "title": "Nextclade",
    "section": "Algorithm",
    "text": "Algorithm\nnextclade 内部进行了并行化处理。\nSequence alignment\n采用的比对策略：“a banded local alignment algorithm with affine gap-cost”\n可以使用 --alignment-preset 中的预制参数，也可以在 pathogen.json 中设置，或在 run 中设置，优先级依次提高。\n正链失败会尝试负链，不建议小于100个核苷酸的序列进行分析，如果查询序列与参考序列差异过大可能也会失败。\n序列比对通常会存在模糊匹配结果，如果提供了基因组注释，Nextclade会在密码子的起始处（由上述模式中的|字符分隔）使用较低的缺口开放罚分，从而在可能的情况下将缺口锁定在阅读框内。同样，在存在歧义的情况下，Nextclade会优先将缺口放置在基因外部。\nTranslation\n当dataset中提供GFF3注释文件时运行。注释文件中的CDS可以通过--cds-selection/-g来选择。\n将查询序列的CDS翻译后与参考序列的进行比对，比对算法同上。\nPhylogenetic placement\n计算查询序列相较于参考序列的突变与其他树节点的距离，找到距离最近的节点。距离相近的节点的优先级可以在参考树中进行设置。\n\\[\nD = M_{ref} + M_{query} - 2 M_{agree} - M_{disagree} - M_{unknown}\n\\]\n在查看最近距离节点的过程中，不会将以放置的查询序列纳入计算。只有在后续进行贪婪型树的重构时才会将查询序列纳入计算。\n将查询序列与树最近节点分开的突变是私有突变，二者共有的突变是共有突变。QC质控会检查私有突变是否是由测序或组装错误引起，同时检查这些突变在基因组上的位置。\nnextclade 在设计时，考虑到了普通PC上的浏览器性能，所以树的准确性依赖参考树的多样性（dataset/tree.json）。\n输入的系统发育树的根必须与输入的参考（根）序列相对应。如果参考序列与根节点的序列不同，则必须将两者之间的差异作为根节点的祖先突变添加进去。当遇到树中的多样性与参考序列之间存在不一致时，Nextclade 会报错。\nClade assignment\nnextclade 在参考树中使用 nextstrain 定义的“clade”和 PANGO 定义的 “lineage” 两种谱系分类方式。\n分配时，基于 Phylogenetic placement 步骤中的结果；只能分配树中已有的 clade/lineage，无法识别参考树里没有的，它只会就近分配。\nMutation calling\nnextclade 会统计三种突变类型：替换，缺失，插入；同时统计N、非ATCGN等信息。氨基酸突变检测需要提供注释文件。\n对于查询序列相较于树节点的私有突变，nextclade会进一步分析，将其分为：\n\n回复突变：极大可能是测序时导致\n标记突变：可能是测序时导致\n未标突变：不太可能是测试时导致\n\n三种突变在分支过程中的权重不同。\nQuality Control (QC)\n在 dataset 中的 pathogen.json 中可以设置质控标准，也即每种病毒的质控标准不同。\n质控的检测指标有：\n\nMissing data (N)\nMised sites (M)\nPrivate mutations (P)\nMutation clusters (C)\nStop codons (S)\nFrame shifts (F)\n\nQC 评分是多种指标的综合结果:\n\\[\nS = \\sum_{i} \\frac{S_i^2}{100}\n\\]\nQC质控结果只是参考，并不代表真实的质量情况。\nDetection of PCR primer changes\n当 dataset 中的 pathogen.json 中配置了PCR引物表时，触发运行。"
  }
]